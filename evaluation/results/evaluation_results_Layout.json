[
    [
        [
            0.97,
            1.0
        ],
        [
            0.97,
            1.0
        ],
        [
            1.0,
            1.0
        ],
        [
            0.85,
            1.0
        ],
        [
            0.95,
            1.0
        ],
        [
            0.25,
            1.0
        ],
        [
            0.85,
            1.0
        ]
    ],
    {
        "question_id": 1,
        "document_path": "static/Computational cost of semantic chunking.pdf",
        "question": "What is the defining characteristic of 'semantic chunking' in Retrieval-Augmented Generation (RAG) systems?",
        "ground_truth": "Semantic chunking is a strategy that aims to improve retrieval performance by dividing documents into semantically coherent segments. This approach segments documents based on semantic similarity or detecting semantic distance thresholds between consecutive sentences to maintain coherence",
        "retrieved_context": {
            "context": "# Abstract  \nRecent advances in Retrieval-Augmented Generation (RAG) systems have popularized **se-**\n\n# Abstract  \nRecent advances in Retrieval-Augmented Generation (RAG) systems have popularized **se-**\n\n# mantic chunking, which aims to improve retrieval performance by dividing documents into\nsemantically coherent segments. Despite its\ngrowing adoption, the actual benefits over simpler **fixed-size chunking**, where documents are\nsplit into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrievalbased answer generation. The results show that\nthe computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the\nprevious assumptions about semantic chunking and highlight the need for more efficient\nchunking strategies in RAG systems.\n\n# mantic chunking, which aims to improve retrieval performance by dividing documents into\nsemantically coherent segments. Despite its\ngrowing adoption, the actual benefits over simpler **fixed-size chunking**, where documents are\nsplit into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrievalbased answer generation. The results show that\nthe computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the\nprevious assumptions about semantic chunking and highlight the need for more efficient\nchunking strategies in RAG systems."
        },
        "generated_answer": "Hello there! I can help you understand semantic chunking based on your notes.\n\n**Answer:**\nThe defining characteristic of 'semantic chunking' in Retrieval-Augmented Generation (RAG) systems is its approach to **dividing documents into semantically coherent segments**. This means that instead of just splitting text at arbitrary points, semantic chunking aims to keep related ideas and information together within each segment, making each chunk meaningful on its own.\n\n**Key Points/Summary:**\n*   **Purpose:** To improve retrieval performance in RAG systems.\n*   **Method:** Divides documents into segments that are semantically coherent.\n*   **Goal:** Each segment contains related ideas, maintaining meaning.\n\n**Extra Tip:**\nThink of it like organizing a book: instead of cutting it into pages of equal length (fixed-size chunking), semantic chunking would be like keeping each chapter or even each major paragraph as a complete unit, because they discuss one main idea.",
        "correctness_score": 0.97,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 2,
        "document_path": "static/Computational cost of semantic chunking.pdf",
        "question": "What three primary proxy tasks were designed to indirectly evaluate the quality of chunking strategies in the study presented in the sources?",
        "ground_truth": "The study systematically evaluated the effectiveness of chunking strategies using three proxy tasks: document retrieval, evidence retrieval, and retrieval-based answer generation",
        "retrieved_context": {
            "context": "# Lack of Chunk Quality Measures As noted in\nSection 4, while the output chunks differed between methods, retrieval and generation performances were similar across chunkers. In addition  \nto the influence of embedding models, the absence\nof direct chunk quality metrics likely contributed\nto this issue. Having ground-truth query-chunk relevance scores would provide more accurate evaluations than relying solely on document or evidence\nmapping.\n\n# Lack of Chunk Quality Measures As noted in\nSection 4, while the output chunks differed between methods, retrieval and generation performances were similar across chunkers. In addition  \nto the influence of embedding models, the absence\nof direct chunk quality metrics likely contributed\nto this issue. Having ground-truth query-chunk relevance scores would provide more accurate evaluations than relying solely on document or evidence\nmapping.\n\n# 4 **Results**  \n## 4.1 **Measuring and reporting performances**  \nAs mentioned earlier, we used three proxy tasks\nthe study chunking. We cannot directly assess the\nquality of retrieval at the chunk level due to the lack\nof ground-truth at the chunk level. Instead, each\nretrieved chunk is mapped back to either the source\ndocument or the included evidence sentences.  \nSince the number of relevant documents or evi\ndence sentences is not fixed (unlike the _k_ value for\nretrieved chunks), traditional metrics such as Recall@k and NDCG@k are not suitable. F1 provides\na balanced measure that accounts for both precision\nand recall under these circumstances. Therefore,\nwe use **F1@5** as the metric. For further details, see\nAppendix D.\nFor each dataset, results are reported based on\nthe best hyperparameter configuration for each\nchunker, determined by the average F1 score across\nall _k_ values. All results to be reported below are\nobtained using dunzhang/stella_en_1.5B_v5 as the\nembedder for being the best among those tested.\nIn the following subsections, **Bold** values indicate the best performance on the respective dataset.\nThe results for Answer Generation closely matched\nthose of Evidence Retrieval and are discussed in  \nAppendix E.1. Additional analysis of hyperparameters is provided in Appendix B. Inspection of the\noutputs of different chunkers is provided in Appendix E.4.\n\n# 4 **Results**  \n## 4.1 **Measuring and reporting performances**  \nAs mentioned earlier, we used three proxy tasks\nthe study chunking. We cannot directly assess the\nquality of retrieval at the chunk level due to the lack\nof ground-truth at the chunk level. Instead, each\nretrieved chunk is mapped back to either the source\ndocument or the included evidence sentences.  \nSince the number of relevant documents or evi\ndence sentences is not fixed (unlike the _k_ value for\nretrieved chunks), traditional metrics such as Recall@k and NDCG@k are not suitable. F1 provides\na balanced measure that accounts for both precision\nand recall under these circumstances. Therefore,\nwe use **F1@5** as the metric. For further details, see\nAppendix D.\nFor each dataset, results are reported based on\nthe best hyperparameter configuration for each\nchunker, determined by the average F1 score across\nall _k_ values. All results to be reported below are\nobtained using dunzhang/stella_en_1.5B_v5 as the\nembedder for being the best among those tested.\nIn the following subsections, **Bold** values indicate the best performance on the respective dataset.\nThe results for Answer Generation closely matched\nthose of Evidence Retrieval and are discussed in  \nAppendix E.1. Additional analysis of hyperparameters is provided in Appendix B. Inspection of the\noutputs of different chunkers is provided in Appendix E.4.\n\n# 3 **Experiments**  \nIn the absence of ground-truth chunk data, we designed three experiments to indirectly assess the\nquality of each chunker: document retrieval, evidence retrieval, and answer generation. Different\ndatasets and evaluation metrics were used for each  \nexperiment to align with the specific task requirements. All documents were first split into sentences using SpaCy\u2019s en_core_web_sm model (Explosion, 2024) before being embedded and chunked. We tested three embedding models selected to\nrepresent a range of performances based on their\nrankings on the MTEB Leaderboard (Muennighoff\net al., 2022). See Appendix E.2 for details.\n\n# 3 **Experiments**  \nIn the absence of ground-truth chunk data, we designed three experiments to indirectly assess the\nquality of each chunker: document retrieval, evidence retrieval, and answer generation. Different\ndatasets and evaluation metrics were used for each  \nexperiment to align with the specific task requirements. All documents were first split into sentences using SpaCy\u2019s en_core_web_sm model (Explosion, 2024) before being embedded and chunked. We tested three embedding models selected to\nrepresent a range of performances based on their\nrankings on the MTEB Leaderboard (Muennighoff\net al., 2022). See Appendix E.2 for details.\n\n# E.4 **Chunk Inspection**  \nWe examined the output chunks to (1) confirm that\ndifferent chunkers were functioning as intended,\nand (2) investigate the reasons behind performance\ndifferences. BEIR\u2019s HotpotQA dataset (Thakur\net al., 2021; Yang et al., 2018) was selected for its\nreasonably sized documents. We randomly sampled five documents, stitching the first four together\nto form a stitched document (Figure 6), and keeping the fifth as a normal document (Figure 7. The\ndocument IDs are:  \n- Stitched: 44547136, 14115210, 5580754,  \n54045118.  \n- Normal: 30214079.\n\n# E.4 **Chunk Inspection**  \nWe examined the output chunks to (1) confirm that\ndifferent chunkers were functioning as intended,\nand (2) investigate the reasons behind performance\ndifferences. BEIR\u2019s HotpotQA dataset (Thakur\net al., 2021; Yang et al., 2018) was selected for its\nreasonably sized documents. We randomly sampled five documents, stitching the first four together\nto form a stitched document (Figure 6), and keeping the fifth as a normal document (Figure 7. The\ndocument IDs are:  \n- Stitched: 44547136, 14115210, 5580754,  \n54045118.  \n- Normal: 30214079.\n\n# 2 **Chunking Strategies**  \nIn this paper, a document is first split into sentences\nwhich are then grouped into chunks. We evaluate\nthree chunking strategies, hereafter referred to as  \nFigure 1: Illustration of the three chunkers tested in this study. Colored segments represent different topics within\nthe sample document: Purple for psychology, Green for programming, and Yellow for food. Red blocks mark chunk\nbreakpoints. (a) Fixed-size Chunker splits the document into consecutive, uniform chunks without considering\nsemantic content. (b) Breakpoint-based Semantic Chunker segments the text by detecting semantic distance\nthresholds between consecutive sentences to maintain coherence. (c) Clustering-based Semantic Chunker groups\nsemantically similar sentences, potentially combining non-consecutive text to form topic-based chunks.  \n\u201cchunkers.\u201d\n\n# 2 **Chunking Strategies**  \nIn this paper, a document is first split into sentences\nwhich are then grouped into chunks. We evaluate\nthree chunking strategies, hereafter referred to as  \nFigure 1: Illustration of the three chunkers tested in this study. Colored segments represent different topics within\nthe sample document: Purple for psychology, Green for programming, and Yellow for food. Red blocks mark chunk\nbreakpoints. (a) Fixed-size Chunker splits the document into consecutive, uniform chunks without considering\nsemantic content. (b) Breakpoint-based Semantic Chunker segments the text by detecting semantic distance\nthresholds between consecutive sentences to maintain coherence. (c) Clustering-based Semantic Chunker groups\nsemantically similar sentences, potentially combining non-consecutive text to form topic-based chunks.  \n\u201cchunkers.\u201d"
        },
        "generated_answer": "In the study, since there was no direct \"ground-truth\" data available for chunk quality, three primary proxy tasks were designed to indirectly assess how well different chunking strategies performed. These tasks are:\n\n1.  **Document Retrieval**: This task evaluates how effectively chunks contribute to retrieving relevant documents.\n2.  **Evidence Retrieval**: This task focuses on the ability of chunks to help in retrieving specific pieces of evidence.\n3.  **Answer Generation**: This task assesses how well the retrieved chunks support the generation of accurate answers.\n\n### Key Points/Summary:\n*   Three proxy tasks were used to indirectly evaluate chunker quality due to the lack of direct ground-truth data.\n*   The tasks were document retrieval, evidence retrieval, and answer generation.\n*   Each task used different datasets and evaluation metrics tailored to its specific requirements.\n\n### Extra Tip:\nThink of these proxy tasks like using a thermometer to check if a cake is done when you can't see the inside directly. Each task measures a different aspect (like temperature, texture, or smell) to give an overall idea of the chunker's \"doneness\" or quality in a real-world application.",
        "correctness_score": 0.97,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 3,
        "document_path": "static/Reconstructing Context.pdf",
        "question": "What are two advanced chunking techniques, besides traditional early chunking, aimed at preserving global context within RAG systems?",
        "ground_truth": "Two advanced techniques introduced to preserve global context and mitigate context fragmentation are late chunking and contextual retrieval. Late chunking involves embedding the entire document first before segmentation to retain global context, potentially leading to superior results across various retrieval tasks.",
        "retrieved_context": {
            "context": "# 3 **Methodology**  \nTo guide our study, we define the following research questions (RQs), aimed at\nevaluating different strategies for chunking and retrieval in RAG systems:\n\n# 3 **Methodology**  \nTo guide our study, we define the following research questions (RQs), aimed at\nevaluating different strategies for chunking and retrieval in RAG systems:\n\n# ditional early chunking strategies, utilizing **different text segmenters**\nand embedding models to evaluate their impact on retrieval accuracy and\ndownstream performance in RAG systems.\n\n# ditional early chunking strategies, utilizing **different text segmenters**\nand embedding models to evaluate their impact on retrieval accuracy and\ndownstream performance in RAG systems.\n\n# \u2013 RQ#1 : Compares the effectiveness of **early versus late chunking** strategies, utilizing **different text segmenters** and embedding models to evaluate their impact on retrieval accuracy and downstream performance in RAG\nsystems.\n\n# \u2013 RQ#1 : Compares the effectiveness of **early versus late chunking** strategies, utilizing **different text segmenters** and embedding models to evaluate their impact on retrieval accuracy and downstream performance in RAG\nsystems.\n\n# 2 **Related Work**  \n_Classic RAG._ A standard RAG workflow involves four main stages: document\nsegmentation, chunk embedding, indexing, and retrieval. During segmentation,\ndocuments are divided into manageable chunks. These chunks are then transformed into vector representations using encoder models, often normalized to  \n1 `[https://www.anthropic.com/news/contextual-retrieval](https://www.anthropic.com/news/contextual-retrieval)`\n2 `[https://github.com/disi-unibo-nlp/rag-when-how-chunk](https://github.com/disi-unibo-nlp/rag-when-how-chunk)`  \nReconstructing Context 3  \nensure unit magnitudes. The resulting embeddings are stored in indexed vector\ndatabases, enabling efficient approximate similarity searches. Retrieval involves\ncomparing query embeddings with the stored embeddings using metrics such as\ncosine similarity or Euclidean distance, which identify the most relevant chunks.\nSeminal works like [15] and [13] have demonstrated the effectiveness of RAG\nin tasks such as open-domain question answering. More recent studies, including [7], have introduced advancements in scalability and embedding techniques,\nfurther establishing RAG as a foundational framework for knowledge-intensive\napplications.  \n_Document Segmentation._ Document segmentation is essential for processing long\ntexts in RAG workflows, with methods ranging from _fixed-size segmentation_ [7]\nto more adaptive techniques like _semantic segmentation_, [3] which detect semantic\nbreakpoints based on shifts in meaning. Recent advancements include _supervised_\n_segmentation models_ [14,12] and _segment-then-predict models_, trained end-to-end\nwithout explicit labels to optimize chunking for downstream task performance  \n[17]. In 2024, _late chunking_ and _contextual retrieval_ introduced novel paradigms.\nBoth techniques have proven effective in retrieval benchmarks but remain largely\nuntested in integrated RAG workflows. Despite several RAG surveys [7,6,8],\nno prior work has compared these methods within a comprehensive evaluation\nframework. This study addresses this gap by holistically analyzing late chunking\nand contextual retrieval, offering actionable insights into their relative strengths\nand trade-offs.\n\n# 2 **Related Work**  \n_Classic RAG._ A standard RAG workflow involves four main stages: document\nsegmentation, chunk embedding, indexing, and retrieval. During segmentation,\ndocuments are divided into manageable chunks. These chunks are then transformed into vector representations using encoder models, often normalized to  \n1 `[https://www.anthropic.com/news/contextual-retrieval](https://www.anthropic.com/news/contextual-retrieval)`\n2 `[https://github.com/disi-unibo-nlp/rag-when-how-chunk](https://github.com/disi-unibo-nlp/rag-when-how-chunk)`  \nReconstructing Context 3  \nensure unit magnitudes. The resulting embeddings are stored in indexed vector\ndatabases, enabling efficient approximate similarity searches. Retrieval involves\ncomparing query embeddings with the stored embeddings using metrics such as\ncosine similarity or Euclidean distance, which identify the most relevant chunks.\nSeminal works like [15] and [13] have demonstrated the effectiveness of RAG\nin tasks such as open-domain question answering. More recent studies, including [7], have introduced advancements in scalability and embedding techniques,\nfurther establishing RAG as a foundational framework for knowledge-intensive\napplications.  \n_Document Segmentation._ Document segmentation is essential for processing long\ntexts in RAG workflows, with methods ranging from _fixed-size segmentation_ [7]\nto more adaptive techniques like _semantic segmentation_, [3] which detect semantic\nbreakpoints based on shifts in meaning. Recent advancements include _supervised_\n_segmentation models_ [14,12] and _segment-then-predict models_, trained end-to-end\nwithout explicit labels to optimize chunking for downstream task performance  \n[17]. In 2024, _late chunking_ and _contextual retrieval_ introduced novel paradigms.\nBoth techniques have proven effective in retrieval benchmarks but remain largely\nuntested in integrated RAG workflows. Despite several RAG surveys [7,6,8],\nno prior work has compared these methods within a comprehensive evaluation\nframework. This study addresses this gap by holistically analyzing late chunking\nand contextual retrieval, offering actionable insights into their relative strengths\nand trade-offs.\n\n# \u2013\n_Loss of Context:_ dividing documents without considering semantic boundaries can result in chunks that lack sufficient context, impairing the model\u2019s\nability to generate accurate and coherent responses.  \n# \u2013\n_Incomplete Information Retrieval:_ important information split across chunks\nmay not be effectively retrieved or integrated.  \nTo address these issues, we analyse and compare two recent techniques\u2014\ncontextual retrieval [1] and late chunking [9]\u2014within a unified setup, evaluating\ntheir strengths and limitations in tackling challenges like context loss and incomplete information retrieval. Contextual retrieval preserves coherence by prepending LLM-generated context to chunks, while late chunking embeds entire documents to retain global context before segmenting.\nOur study rigorously assesses their impact on generation performance in\nquestion-answering tasks, finding that neither technique offers a definitive solution. This work highlights the trade-offs between these methods and provides\npractical guidance for optimizing RAG systems.\nTo further support the community, we release all code, prompts, and data\nunder the permissive MIT license, enabling full reproducibility and empowering\npractitioners to adapt and extend our work. [2]\n\n# \u2013\n_Loss of Context:_ dividing documents without considering semantic boundaries can result in chunks that lack sufficient context, impairing the model\u2019s\nability to generate accurate and coherent responses.  \n# \u2013\n_Incomplete Information Retrieval:_ important information split across chunks\nmay not be effectively retrieved or integrated.  \nTo address these issues, we analyse and compare two recent techniques\u2014\ncontextual retrieval [1] and late chunking [9]\u2014within a unified setup, evaluating\ntheir strengths and limitations in tackling challenges like context loss and incomplete information retrieval. Contextual retrieval preserves coherence by prepending LLM-generated context to chunks, while late chunking embeds entire documents to retain global context before segmenting.\nOur study rigorously assesses their impact on generation performance in\nquestion-answering tasks, finding that neither technique offers a definitive solution. This work highlights the trade-offs between these methods and provides\npractical guidance for optimizing RAG systems.\nTo further support the community, we release all code, prompts, and data\nunder the permissive MIT license, enabling full reproducibility and empowering\npractitioners to adapt and extend our work. [2]"
        },
        "generated_answer": "Hello there! I can help you with that.\n\nBased on your notes, two advanced techniques aimed at preserving global context within RAG systems, beyond traditional early chunking, are **late chunking** and **contextual retrieval**.\n\n### Answer:\n\n1.  **Late Chunking:** This technique involves embedding entire documents first to retain their global context. Only after the embedding process is complete are the documents segmented into chunks. This approach ensures that the broader meaning and relationships within the full document are captured before specific parts are isolated.\n2.  **Contextual Retrieval:** This method focuses on preserving coherence by prepending additional context, often generated by a Large Language Model (LLM), to the retrieved chunks. By adding this LLM-generated context, the system aims to provide a more complete and coherent picture around the specific information being retrieved, helping to mitigate the loss of context that can occur when documents are simply divided.\n\nBoth of these techniques were introduced in 2024 as novel paradigms to address challenges like loss of context and incomplete information retrieval in RAG workflows.\n\n### Key Points/Summary:\n\n*   **Late Chunking:** Embeds entire documents first to maintain global context, then segments them.\n*   **Contextual Retrieval:** Prepends LLM-generated context to chunks to enhance coherence and provide a fuller picture.\n*   Both are advanced strategies designed to overcome context loss in RAG systems.\n\n### Extra Tip:\nThink of \"late chunking\" like reading a whole book to understand the plot before highlighting specific passages. \"Contextual retrieval\" is like having a helpful friend summarize the surrounding chapters for you whenever you look at a highlighted passage, ensuring you don't miss the bigger picture!",
        "correctness_score": 1.0,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 4,
        "document_path": "static/Accelerating LLM Inference.pdf",
        "question": "In the context of long-context LLMs, what is the key phenomenon called where chunks attended to by tokens within a generated chunk exhibit substantial consistency, which ChunkLLM exploits to enhance inference efficiency?",
        "ground_truth": "This phenomenon is called the Intra-Chunk Attention Consistency (ICAC) pattern. ChunkLLM exploits ICAC by updating chunk selection only when the currently decoded token is identified as a chunk boundary.",
        "retrieved_context": {
            "context": "# ICAC We find a phenomenon during the model inference, as illustrated in Figure 3. The chunks attended to by tokens within a generated chunk exhibit substantial consistency, whereas chunk updates  \nFigure 3: Attention visualization of chunk selection during the inference phase. The sample is derived from the passkey retrieval task.  \n4  \npredominantly occur at chunk boundaries. We name this phenomenon the \u201d **I** ntra- **C** hunk **A** ttention\n\n# ICAC We find a phenomenon during the model inference, as illustrated in Figure 3. The chunks attended to by tokens within a generated chunk exhibit substantial consistency, whereas chunk updates  \nFigure 3: Attention visualization of chunk selection during the inference phase. The sample is derived from the passkey retrieval task.  \n4  \npredominantly occur at chunk boundaries. We name this phenomenon the \u201d **I** ntra- **C** hunk **A** ttention\n\n# tion Answering, which evaluates the model\u2019s capabilities in question understanding and information\nmatching, CommonsenseQA (Talmor et al., 2019) (5-shot), Social IQA (Sap et al., 2019) (15-shot),\nPIQA (Bisk et al., 2020) (25-shot); and **Reasoning** which evaluates the model\u2019s capabilities in logical abstraction and complex decision-making, HellaSwag (Zellers et al., 2019) (10-shot), WinoGrande (Sakaguchi et al., 2020) (25-shot), ARC-c/ARC-e (Clark et al., 2018) (25-shot).  \n5  \nMethods SDQA MDQA Summary Few-shot Avg KV\nNQA Qasper MFQA HQA Musi 2WQA GR QMS SAM TREC\nQwen2.5-7B 33.76 51.40 57.17 56.76 29.40 38.09 31.57 24.40 46.73 72.00 44.13 100.00\nChunkLLM **31.09** **50.52** **56.95** **55.61** **29.41** **38.27** **31.25** **23.16** 46.53 **72.50** **43.53** 48.58\nSepLLM 26.43 50.18 50.29 47.25 22.83 36.34 28.31 21.14 46.71 71.50 40.10 53.17\nStrmLLM 28.88 50.37 50.47 51.60 24.55 37.76 30.36 22.27 **46.84** 72.00 41.51 68.50  \nLlama3.1-8B 40.88 51.72 58.47 52.68 35.80 42.52 30.64 24.44 47.10 73.00 45.73 100.00  \nChunkLLM **39.14** 49.93 53.45 **52.29** **34.40** **42.98** **31.05** **23.89** **47.20** 72.50 **44.68** 50.18\nSepLLM 36.23 **51.16** 51.81 47.70 27.83 40.85 27.12 21.84 46.89 72.00 42.34 54.56\nStrmLLM 35.94 50.96 **54.13** 50.54 30.95 41.83 27.50 23.06 46.23 **73.00** 43.42 69.25  \nTable 1: Experimental results on LongBench. Avg denotes average score. SDQA: single-document\nquestion answering, MDQA: multi-document question answering. The full names of the subtasks\nare shown in Appendix 6.2.  \n3.2 M AIN R ESULTS  \n3.2.1 R ESULTS ON L ONG B ENCH  \nWe set the top-k ratio to 45% and the number of local chunks to 15 for ChunkLLM. The experimental\nresults on the LongBench using Qwen2.5-7B and Llama3.1-8B are presented in Table 1. Here,\n\u201dStrmLLM\u201d represents StreamingLLM (Xiao et al., 2024). We take Qwen2.5-7B as an example for\nanalysis, and the same conclusion holds for Llama3.1-8B. The following observations can be made:\n(1) In terms of overall average performance, ChunkLLM attains the optimal performance when\ncompared to SepLLM and StreamingLLM, with respective improvements of 3.43 and 2.02 (43.53\nv.s. 40.10 v.s. 41.51). In contrast to the short-text benchmark in Subsection 3.2.4, ChunkLLM\ndemonstrates a remarkable improvement in long-context evaluation, which validates the advantage\nof ChunkLLM in retrieving key chunk information during long-context reasoning. (2) Notably,\nin the MDQA task, ChunkLLM yields a substantial improvement over SepLLM. We argue that\nthe core challenge of MDQA lies in the dispersion of critical information across distinct positions\nwithin the context, which places high demands on the model\u2019s context comprehension capability.\nSepLLM leverages separators as chunk features, which is plagued by constrained representational\ncapacity and the problem of chunk semantic incompleteness. By contrast, ChunkLLM enriches\nthe representational capacity of chunks via attention distillation, which enhances the recall rates of\ncritical chunks. This, in turn, effectively boosts the model\u2019s long-context understanding capability.\n(3) ChunkLLM attains 98.64% of the vanilla model\u2019s performance while employing the minimum\nKV cache. Notably, relative to SepLLM and StreamingLLM, ChunkLLM reduces the KV cache\nusage rate by 4.59% and 19.92% (48.58% v.s. 53.17% v.s. 68.50%), respectively, findings that\nfurther substantiate the superiority of ChunkLLM in long-context scenarios.  \n3.2.2 R ESULTS ON NIAH  \nFigure 4: Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length.\nThe last column represents the KV-cache utilization rate.  \n6  \nWe set the top-k to 256 and the number of local chunks to 16 for ChunkLLM. As depicted in\nFigure 4, ChunkLLM outperforms SepLLM across all scenarios in the 64K-context NIAH evaluation conducted on Qwen2.5-7B and Llama3.1-8B, achieving superior performance. Notably, in\nscenarios where the context length exceeds 12K, SepLLM exhibits near-total loss of retrieval capability (visualized in red), whereas ChunkLLM retains performance comparable to the vanilla model.\nThis discrepancy is primarily attributed to ChunkLLM\u2019s attention distillation mechanism, which\nstrengthens the feature representational capacity of chunks. Consequently, during chunk selection,\nthe model effectively identifies critical chunks with higher query relevance, leading to improved inference performance. Additionally, ChunkLLM exhibits a reduced KV-Cache utilization rate relative\nto SepLLM, which further corroborates the effectiveness of key chunk retrieval. We also conduct\nexperiments with StreamingLLM, as shown in Appendix 6.3.  \n3.2.3 I NFERENCE E FFICIENCY  \n|1600<br>1400<br>1200 (s)<br>1000 time<br>800 Cost<br>600<br>400<br>200|Qwen2.5-7B<br>Ours|Col3|2300<br>2000<br>1700<br>1400<br>1100<br>800<br>500<br>200|Llama3.1-8B<br>Ours|Col6|\n|---|---|---|---|---|---|\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)|~~3.84x~~|~~3.84x~~|~~3.84x~~|~~4.~~|~~4.~~|\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||  \nFigure 5: Comparison of inference time per 10k tokens in the generation process on PG19 test set.  \nWe conduct runtime evaluations of Vanilla and ChunkLLM for 120K-token generation tasks on\nthe PG19(Rae et al., 2019) test set, with metrics recorded every 10K tokens. We set the top-k to\n256 and the number of local chunks to 16 for ChunkLLM. As shown in Figure 5, as the number\nof generated tokens increases, Vanilla\u2019s inference time rises linearly, while ChunkLLM maintains\npersistent stability in time consumption. In the 110K\u2013120K token generation phase, ChunkLLM\noutperforms Vanilla by speedups of 3.84\u00d7 and 4.48\u00d7, which corroborates the efficacy of the proposed\nICAC mechanism. During ChunkLLM\u2019s inference phase, chunk updates occur exclusively at chunk\nboundaries, minimizing the updates frequency and thereby boosting inference efficiency. We also\nconduct supplementary experiments using the FineWeb-Edu dataset, from which 1000 test corpora\nof 4k length are sampled. For the task of chunk boundary prediction, we evaluate its performance\nusing three key metrics: precision, recall, and F1-score. The calculated results are 98.31, 95.54,\nand 96.91, respectively. Such promising performance indicators serve to verify the reliability and\neffectiveness of our chunk boundary prediction task.  \nWe conduct experiments where these models Length Methods PPL Total Time(s)\ngenerated 120K tokens, evaluating both total Qwen2.5-7B 14.41 10,684.31\ninference time and average perplexity (ppl) on 120K ChunkLLM 16.23 4,782.62\nthe PG19 test set, and results are summarizedin Table 2. Compared to the vanilla model, Llama3.1-8BChunkLLM 11.9312.89 145,,906.19963.83\nChunkLLM yields a slight enhancement in ppl Table 2: The perplexity and running time comparalongside a significant decrease in total infer- ison on the PG19 test set.\nence time. The underlying reason is that while\nthe vanilla model maintains semantic integrity, it incurs linearly increasing inference time as generated token count rises. Conversely, ChunkLLM reduces computational burden and speeds up\ninference by leveraging its chunk selection and ICAC mechanisms.  \nLength Methods PPL Total Time(s)  \n120K  \nQwen2.5-7B 14.41 10,684.31\nChunkLLM 16.23 4,782.62\nLlama3.1-8B 11.93 14,906.19\nChunkLLM 12.89 5,963.83  \nTable 2: The perplexity and running time comparison on the PG19 test set.  \n3.2.4 R ESULTS ON S HORT T EXT  \nThe experimental results for short texts are presented in Table 3. The following conclusions can\nbe drawn: (1) The overall average metrics of ChunkLLM on Qwen2.5-7B and Llama3.1-8B both\noutperform those of StreamingLLM and SepLLM, achieving 99.57% and 99.84% of the Vanilla\nmodel\u2019s performance, respectively. Notably, ChunkLLM attains optimal performance across 8 out of  \n7  \nMethods General Knowledge Question Answering Reasoning Avg KV\nMMLU SciQ OQA CQA SIQA PIQA Heag WG ARC-c ARC-e\nQwen2.5-7B 74.25 97.00 52.80 84.52 58.44 81.72 80.24 77.27 63.82 87.21 75.73 100.00\nChunkLLM 72.51 96.60 **52.40** **84.68** **58.34** **81.77** 80.08 **76.87** **63.65** **87.21** **75.41** 45.47\nSepLLM 73.07 **96.70** 52.20 84.19 58.25 81.41 **80.10** 76.48 62.94 86.11 75.15 50.20\nStrmLLM **73.31** 96.60 52.00 84.28 58.19 81.39 79.76 76.64 62.79 86.36 75.13 45.14  \nLlama3.1-8B 65.30 97.60 48.00 74.28 54.04 83.19 81.76 80.03 57.85 84.55 72.66 100.00  \nChunkLLM **64.78** 97.30 **48.40** **74.45** **54.76** 82.75 **81.84** **79.01** 57.68 **84.55** **72.55** 45.04\nSepLLM 64.32 **97.40** 47.40 74.10 54.25 **83.03** 81.68 79.01 **57.93** 84.09 72.32 50.32\nStrmLLM 61.19 97.20 48.00 73.79 53.94 81.56 80.14 78.22 56.91 83.59 72.45 45.30  \nTable 3: Experimental results on short context benchmarks.  \nthe 10 evaluation tasks, validating its efficacy in short-text task scenarios. (2) We perform statistical\nanalyses on the average utilization rate of the KV cache. In comparison with SepLLM, ChunkLLM\nachieves superior performance while consuming a lower volume of KV cache (45.47% v.s. 50.20%).\nSpecifically, on the Llama3.1-8B model, ChunkLLM not only exhibits the minimal KV cache usage\nbut also outperforms both SepLLM and StreamingLLM in terms of performance metrics. This\nfinding further validates the precision of ChunkLLM in chunk recall, achieving a balanced trade-off\nbetween performance and memory consumption.  \n3.3 A BLATION S TUDY  \n3.3.1 E FFECTIVENESS OF V OTE AND ICAC  \nMethods SDQA MDQA Summary Few-shot Avg\nNQA Qasper MFQA HQA Musi 2WQA GR QMS SAM TREC\nQwen2.5-7B 33.76 51.40 57.17 56.76 29.40 38.09 31.57 24.40 46.73 72.00 44.13\nChunkLLM 31.09 50.52 **56.95** **55.61** **29.41** 38.27 31.25 23.16 46.53 **72.50** 43.53  \nw/o vote 30.78 45.00 51.53 54.96 28.00 38.17 30.82 23.49 46.58 70.00 41.93  \nw/o ICAC **31.36** **50.94** 56.84 55.61 28.50 **38.59** **31.86** **23.93** **46.66** 72.50 **43.68**  \nTable 4: Ablation Study on LongBench, w/o vote: remove vote mechanism, w/o ICAC: remove\nICAC pattern.  \nWe validate the proposed vote mechanism and\nICAC pattern based on the Qwen2.5-7B using the LongBench, with experimental results\nshown in Table 4. Removal of the vote mechanism leads to a 1.6 drop in overall performance\n(43.53 v.s. 41.93), which confirms the mechanism\u2019s efficacy, as it integrates inter-layer differences in chunk selection and minimizes interference arising from such discrepancies. We Figure 6: Inference efficiency of ICAC on PG19\nalso conduct a visual investigation into the re- test set.\ncall performance of top-k chunks across different layers, with comprehensive experimental results provided in Appendix 6.5. Conversely, removing ICAC results in a marginal 0.15 improvement in overall performance. This slight gain is\nattributed to the increased frequency of chunk selection updates during the inference phase. Frequent chunk selection, however, poses a limitation of low inference efficiency. As shown in Figure\n6, after ICAC is removed, the inference latency is 2.12 times higher than that of ChunkLLM in the\n110K\u2013120K token generation stage. Conversely, incorporating ICAC enables the model to maintain near-lossless performance alongside improved inference efficiency, which provides additional\nvalidation of ICAC\u2019s success. Appendix 6.4 shows a case study of the ICAC.  \nFigure 6: Inference efficiency of ICAC on PG19  \ntest set.  \n3.3.2 A NALYSIS OF F IXED C HUNKS AND S EMANTIC C HUNKS  \nWe conduct an experimental analysis of the fixed chunk method (FixLLM) on the NIAH task. To\nensure consistent KV cache utilization and facilitate a fair comparison, FixLLM is configured with\na top-k of 384 and a local chunks of 24, while ChunkLLM is set to a top-k of 256 and a local\nchunks of 16. The experimental results are illustrated in Figure 7. As observed, under conditions  \n8  \nFigure 7: Visualization of fixed chunks and semantic chunks in NIAH test.  \nof approximately consistent KV cache utilization, FixLLM exhibits an 8.23 reduction (87.78 v.s.\n79.55) in accuracy relative to ChunkLLM on the 64K NIAH task. This discrepancy stems from\nthe semantic incompleteness of fixed chunks, which in turn compromises chunk selection during\nthe inference phase. In contrast, ChunkLLM leverages contextual semantic information to identify\nchunk boundaries, preserving the semantic integrity of chunks.  \n4 R ELATED W ORK\n\n# tion Answering, which evaluates the model\u2019s capabilities in question understanding and information\nmatching, CommonsenseQA (Talmor et al., 2019) (5-shot), Social IQA (Sap et al., 2019) (15-shot),\nPIQA (Bisk et al., 2020) (25-shot); and **Reasoning** which evaluates the model\u2019s capabilities in logical abstraction and complex decision-making, HellaSwag (Zellers et al., 2019) (10-shot), WinoGrande (Sakaguchi et al., 2020) (25-shot), ARC-c/ARC-e (Clark et al., 2018) (25-shot).  \n5  \nMethods SDQA MDQA Summary Few-shot Avg KV\nNQA Qasper MFQA HQA Musi 2WQA GR QMS SAM TREC\nQwen2.5-7B 33.76 51.40 57.17 56.76 29.40 38.09 31.57 24.40 46.73 72.00 44.13 100.00\nChunkLLM **31.09** **50.52** **56.95** **55.61** **29.41** **38.27** **31.25** **23.16** 46.53 **72.50** **43.53** 48.58\nSepLLM 26.43 50.18 50.29 47.25 22.83 36.34 28.31 21.14 46.71 71.50 40.10 53.17\nStrmLLM 28.88 50.37 50.47 51.60 24.55 37.76 30.36 22.27 **46.84** 72.00 41.51 68.50  \nLlama3.1-8B 40.88 51.72 58.47 52.68 35.80 42.52 30.64 24.44 47.10 73.00 45.73 100.00  \nChunkLLM **39.14** 49.93 53.45 **52.29** **34.40** **42.98** **31.05** **23.89** **47.20** 72.50 **44.68** 50.18\nSepLLM 36.23 **51.16** 51.81 47.70 27.83 40.85 27.12 21.84 46.89 72.00 42.34 54.56\nStrmLLM 35.94 50.96 **54.13** 50.54 30.95 41.83 27.50 23.06 46.23 **73.00** 43.42 69.25  \nTable 1: Experimental results on LongBench. Avg denotes average score. SDQA: single-document\nquestion answering, MDQA: multi-document question answering. The full names of the subtasks\nare shown in Appendix 6.2.  \n3.2 M AIN R ESULTS  \n3.2.1 R ESULTS ON L ONG B ENCH  \nWe set the top-k ratio to 45% and the number of local chunks to 15 for ChunkLLM. The experimental\nresults on the LongBench using Qwen2.5-7B and Llama3.1-8B are presented in Table 1. Here,\n\u201dStrmLLM\u201d represents StreamingLLM (Xiao et al., 2024). We take Qwen2.5-7B as an example for\nanalysis, and the same conclusion holds for Llama3.1-8B. The following observations can be made:\n(1) In terms of overall average performance, ChunkLLM attains the optimal performance when\ncompared to SepLLM and StreamingLLM, with respective improvements of 3.43 and 2.02 (43.53\nv.s. 40.10 v.s. 41.51). In contrast to the short-text benchmark in Subsection 3.2.4, ChunkLLM\ndemonstrates a remarkable improvement in long-context evaluation, which validates the advantage\nof ChunkLLM in retrieving key chunk information during long-context reasoning. (2) Notably,\nin the MDQA task, ChunkLLM yields a substantial improvement over SepLLM. We argue that\nthe core challenge of MDQA lies in the dispersion of critical information across distinct positions\nwithin the context, which places high demands on the model\u2019s context comprehension capability.\nSepLLM leverages separators as chunk features, which is plagued by constrained representational\ncapacity and the problem of chunk semantic incompleteness. By contrast, ChunkLLM enriches\nthe representational capacity of chunks via attention distillation, which enhances the recall rates of\ncritical chunks. This, in turn, effectively boosts the model\u2019s long-context understanding capability.\n(3) ChunkLLM attains 98.64% of the vanilla model\u2019s performance while employing the minimum\nKV cache. Notably, relative to SepLLM and StreamingLLM, ChunkLLM reduces the KV cache\nusage rate by 4.59% and 19.92% (48.58% v.s. 53.17% v.s. 68.50%), respectively, findings that\nfurther substantiate the superiority of ChunkLLM in long-context scenarios.  \n3.2.2 R ESULTS ON NIAH  \nFigure 4: Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length.\nThe last column represents the KV-cache utilization rate.  \n6  \nWe set the top-k to 256 and the number of local chunks to 16 for ChunkLLM. As depicted in\nFigure 4, ChunkLLM outperforms SepLLM across all scenarios in the 64K-context NIAH evaluation conducted on Qwen2.5-7B and Llama3.1-8B, achieving superior performance. Notably, in\nscenarios where the context length exceeds 12K, SepLLM exhibits near-total loss of retrieval capability (visualized in red), whereas ChunkLLM retains performance comparable to the vanilla model.\nThis discrepancy is primarily attributed to ChunkLLM\u2019s attention distillation mechanism, which\nstrengthens the feature representational capacity of chunks. Consequently, during chunk selection,\nthe model effectively identifies critical chunks with higher query relevance, leading to improved inference performance. Additionally, ChunkLLM exhibits a reduced KV-Cache utilization rate relative\nto SepLLM, which further corroborates the effectiveness of key chunk retrieval. We also conduct\nexperiments with StreamingLLM, as shown in Appendix 6.3.  \n3.2.3 I NFERENCE E FFICIENCY  \n|1600<br>1400<br>1200 (s)<br>1000 time<br>800 Cost<br>600<br>400<br>200|Qwen2.5-7B<br>Ours|Col3|2300<br>2000<br>1700<br>1400<br>1100<br>800<br>500<br>200|Llama3.1-8B<br>Ours|Col6|\n|---|---|---|---|---|---|\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)|~~3.84x~~|~~3.84x~~|~~3.84x~~|~~4.~~|~~4.~~|\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||  \nFigure 5: Comparison of inference time per 10k tokens in the generation process on PG19 test set.  \nWe conduct runtime evaluations of Vanilla and ChunkLLM for 120K-token generation tasks on\nthe PG19(Rae et al., 2019) test set, with metrics recorded every 10K tokens. We set the top-k to\n256 and the number of local chunks to 16 for ChunkLLM. As shown in Figure 5, as the number\nof generated tokens increases, Vanilla\u2019s inference time rises linearly, while ChunkLLM maintains\npersistent stability in time consumption. In the 110K\u2013120K token generation phase, ChunkLLM\noutperforms Vanilla by speedups of 3.84\u00d7 and 4.48\u00d7, which corroborates the efficacy of the proposed\nICAC mechanism. During ChunkLLM\u2019s inference phase, chunk updates occur exclusively at chunk\nboundaries, minimizing the updates frequency and thereby boosting inference efficiency. We also\nconduct supplementary experiments using the FineWeb-Edu dataset, from which 1000 test corpora\nof 4k length are sampled. For the task of chunk boundary prediction, we evaluate its performance\nusing three key metrics: precision, recall, and F1-score. The calculated results are 98.31, 95.54,\nand 96.91, respectively. Such promising performance indicators serve to verify the reliability and\neffectiveness of our chunk boundary prediction task.  \nWe conduct experiments where these models Length Methods PPL Total Time(s)\ngenerated 120K tokens, evaluating both total Qwen2.5-7B 14.41 10,684.31\ninference time and average perplexity (ppl) on 120K ChunkLLM 16.23 4,782.62\nthe PG19 test set, and results are summarizedin Table 2. Compared to the vanilla model, Llama3.1-8BChunkLLM 11.9312.89 145,,906.19963.83\nChunkLLM yields a slight enhancement in ppl Table 2: The perplexity and running time comparalongside a significant decrease in total infer- ison on the PG19 test set.\nence time. The underlying reason is that while\nthe vanilla model maintains semantic integrity, it incurs linearly increasing inference time as generated token count rises. Conversely, ChunkLLM reduces computational burden and speeds up\ninference by leveraging its chunk selection and ICAC mechanisms.  \nLength Methods PPL Total Time(s)  \n120K  \nQwen2.5-7B 14.41 10,684.31\nChunkLLM 16.23 4,782.62\nLlama3.1-8B 11.93 14,906.19\nChunkLLM 12.89 5,963.83  \nTable 2: The perplexity and running time comparison on the PG19 test set.  \n3.2.4 R ESULTS ON S HORT T EXT  \nThe experimental results for short texts are presented in Table 3. The following conclusions can\nbe drawn: (1) The overall average metrics of ChunkLLM on Qwen2.5-7B and Llama3.1-8B both\noutperform those of StreamingLLM and SepLLM, achieving 99.57% and 99.84% of the Vanilla\nmodel\u2019s performance, respectively. Notably, ChunkLLM attains optimal performance across 8 out of  \n7  \nMethods General Knowledge Question Answering Reasoning Avg KV\nMMLU SciQ OQA CQA SIQA PIQA Heag WG ARC-c ARC-e\nQwen2.5-7B 74.25 97.00 52.80 84.52 58.44 81.72 80.24 77.27 63.82 87.21 75.73 100.00\nChunkLLM 72.51 96.60 **52.40** **84.68** **58.34** **81.77** 80.08 **76.87** **63.65** **87.21** **75.41** 45.47\nSepLLM 73.07 **96.70** 52.20 84.19 58.25 81.41 **80.10** 76.48 62.94 86.11 75.15 50.20\nStrmLLM **73.31** 96.60 52.00 84.28 58.19 81.39 79.76 76.64 62.79 86.36 75.13 45.14  \nLlama3.1-8B 65.30 97.60 48.00 74.28 54.04 83.19 81.76 80.03 57.85 84.55 72.66 100.00  \nChunkLLM **64.78** 97.30 **48.40** **74.45** **54.76** 82.75 **81.84** **79.01** 57.68 **84.55** **72.55** 45.04\nSepLLM 64.32 **97.40** 47.40 74.10 54.25 **83.03** 81.68 79.01 **57.93** 84.09 72.32 50.32\nStrmLLM 61.19 97.20 48.00 73.79 53.94 81.56 80.14 78.22 56.91 83.59 72.45 45.30  \nTable 3: Experimental results on short context benchmarks.  \nthe 10 evaluation tasks, validating its efficacy in short-text task scenarios. (2) We perform statistical\nanalyses on the average utilization rate of the KV cache. In comparison with SepLLM, ChunkLLM\nachieves superior performance while consuming a lower volume of KV cache (45.47% v.s. 50.20%).\nSpecifically, on the Llama3.1-8B model, ChunkLLM not only exhibits the minimal KV cache usage\nbut also outperforms both SepLLM and StreamingLLM in terms of performance metrics. This\nfinding further validates the precision of ChunkLLM in chunk recall, achieving a balanced trade-off\nbetween performance and memory consumption.  \n3.3 A BLATION S TUDY  \n3.3.1 E FFECTIVENESS OF V OTE AND ICAC  \nMethods SDQA MDQA Summary Few-shot Avg\nNQA Qasper MFQA HQA Musi 2WQA GR QMS SAM TREC\nQwen2.5-7B 33.76 51.40 57.17 56.76 29.40 38.09 31.57 24.40 46.73 72.00 44.13\nChunkLLM 31.09 50.52 **56.95** **55.61** **29.41** 38.27 31.25 23.16 46.53 **72.50** 43.53  \nw/o vote 30.78 45.00 51.53 54.96 28.00 38.17 30.82 23.49 46.58 70.00 41.93  \nw/o ICAC **31.36** **50.94** 56.84 55.61 28.50 **38.59** **31.86** **23.93** **46.66** 72.50 **43.68**  \nTable 4: Ablation Study on LongBench, w/o vote: remove vote mechanism, w/o ICAC: remove\nICAC pattern.  \nWe validate the proposed vote mechanism and\nICAC pattern based on the Qwen2.5-7B using the LongBench, with experimental results\nshown in Table 4. Removal of the vote mechanism leads to a 1.6 drop in overall performance\n(43.53 v.s. 41.93), which confirms the mechanism\u2019s efficacy, as it integrates inter-layer differences in chunk selection and minimizes interference arising from such discrepancies. We Figure 6: Inference efficiency of ICAC on PG19\nalso conduct a visual investigation into the re- test set.\ncall performance of top-k chunks across different layers, with comprehensive experimental results provided in Appendix 6.5. Conversely, removing ICAC results in a marginal 0.15 improvement in overall performance. This slight gain is\nattributed to the increased frequency of chunk selection updates during the inference phase. Frequent chunk selection, however, poses a limitation of low inference efficiency. As shown in Figure\n6, after ICAC is removed, the inference latency is 2.12 times higher than that of ChunkLLM in the\n110K\u2013120K token generation stage. Conversely, incorporating ICAC enables the model to maintain near-lossless performance alongside improved inference efficiency, which provides additional\nvalidation of ICAC\u2019s success. Appendix 6.4 shows a case study of the ICAC.  \nFigure 6: Inference efficiency of ICAC on PG19  \ntest set.  \n3.3.2 A NALYSIS OF F IXED C HUNKS AND S EMANTIC C HUNKS  \nWe conduct an experimental analysis of the fixed chunk method (FixLLM) on the NIAH task. To\nensure consistent KV cache utilization and facilitate a fair comparison, FixLLM is configured with\na top-k of 384 and a local chunks of 24, while ChunkLLM is set to a top-k of 256 and a local\nchunks of 16. The experimental results are illustrated in Figure 7. As observed, under conditions  \n8  \nFigure 7: Visualization of fixed chunks and semantic chunks in NIAH test.  \nof approximately consistent KV cache utilization, FixLLM exhibits an 8.23 reduction (87.78 v.s.\n79.55) in accuracy relative to ChunkLLM on the 64K NIAH task. This discrepancy stems from\nthe semantic incompleteness of fixed chunks, which in turn compromises chunk selection during\nthe inference phase. In contrast, ChunkLLM leverages contextual semantic information to identify\nchunk boundaries, preserving the semantic integrity of chunks.  \n4 R ELATED W ORK"
        },
        "generated_answer": "That's a great question about how long-context Large Language Models (LLMs) manage information!\n\n**Answer:**\nIn the context of long-context LLMs, the key phenomenon where chunks attended to by tokens within a generated chunk exhibit substantial consistency, and which ChunkLLM exploits to enhance inference efficiency, is called **Intra-Chunk Attention (ICAC)**. This phenomenon also notes that chunk updates predominantly occur at chunk boundaries.\n\n**Key Points/Summary:**\n*   **Intra-Chunk Attention (ICAC):** This is the name given to the observed phenomenon.\n*   **Consistency:** Tokens within a generated chunk consistently attend to the same set of chunks.\n*   **Chunk Updates:** Changes to which chunks are being attended to primarily happen at the boundaries between chunks, rather than constantly throughout a chunk.\n*   **Inference Efficiency:** ChunkLLM leverages this ICAC phenomenon to reduce the frequency of chunk selection updates during inference, which significantly boosts its efficiency.\n\n**Extra Tip:**\nImagine reading a book. When you're in the middle of a paragraph, your focus likely stays within that paragraph's ideas. You only really shift your \"attention\" to new major ideas when you reach the end of a paragraph or section. ICAC works similarly for LLMs, allowing them to maintain focus within a \"chunk\" of text and only update their attention more broadly when they hit a natural \"chunk boundary,\" saving computational effort!",
        "correctness_score": 0.85,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 5,
        "document_path": "static/Beyond Long Context.pdf",
        "question": "In the clinical domain, what is the methodology that uses entity-aware retrieval strategies to achieve improved semantic accuracy and computational efficiency when processing Electronic Health Record (EHR) notes?",
        "ground_truth": "The methodology is Clinical Entity Augmented Retrieval (CLEAR). CLEAR addresses the limitations of traditional chunk-based RAG by employing entity-aware, entity-centered retrieval strategies and demonstrated a 78% reduction in token usage compared to wide-context processing in evaluations",
        "retrieved_context": {
            "context": "## 1.1 **Related Work**  \nEntity-aware retrieval has gained increasing attention within biomedical NLP and question-answering  \ndomains. Early retrieval-augmented methods such as RAG [2] demonstrated the potential of embedding  \nbased chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches  \nleveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al.,  \n2016) provided partial improvements but often failed to maintain contextual continuity across long clin  \nical narratives.  \nCLEAR [1] represented a significant advancement by introducing entity-centered retrieval aligned  \nwith clinical semantics. Our work extends this line of research by operationalizing CLEAR within an  \nend-to-end evaluation platform, providing reproducible empirical validation across realistic EHR-scale  \ndocument sets.  \nRecent evaluations of retrieval-augmented models in long-context reasoning (e.g., Karpinska et al.,  \n2023; Xiong et al., 2024) emphasize that retrieval strategies often outperform naive long-context prompt  \ning, supporting the need for entity-aware retrieval. Our work contributes a reproducible evaluation  \nframework within this paradigm, focusing on realistic EHR-scale clinical notes.  \n3\n\n## 1.1 **Related Work**  \nEntity-aware retrieval has gained increasing attention within biomedical NLP and question-answering  \ndomains. Early retrieval-augmented methods such as RAG [2] demonstrated the potential of embedding  \nbased chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches  \nleveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al.,  \n2016) provided partial improvements but often failed to maintain contextual continuity across long clin  \nical narratives.  \nCLEAR [1] represented a significant advancement by introducing entity-centered retrieval aligned  \nwith clinical semantics. Our work extends this line of research by operationalizing CLEAR within an  \nend-to-end evaluation platform, providing reproducible empirical validation across realistic EHR-scale  \ndocument sets.  \nRecent evaluations of retrieval-augmented models in long-context reasoning (e.g., Karpinska et al.,  \n2023; Xiong et al., 2024) emphasize that retrieval strategies often outperform naive long-context prompt  \ning, supporting the need for entity-aware retrieval. Our work contributes a reproducible evaluation  \nframework within this paradigm, focusing on realistic EHR-scale clinical notes.  \n3\n\n# 1. **INTRODUCTION**  \nElectronic Health Record (EHR) systems in modern healthcare infrastructure store clinical documenta  \ntion within FHIR DocumentReference resources, typically encoded as base64 attachments containing  \nunstructured clinical notes. These documents, ranging from brief progress notes to comprehensive dis  \ncharge summaries, present substantial challenges for automated question-answering systems that require  \nsemantically accurate information extraction rather than statistical correlation-based retrieval common  \nin traditional vector database approaches.  \nContemporary approaches to clinical document processing have largely focused on two paradigms:  \nzero-shot inference with large context windows that process entire documents but face computational  \n2  \nconstraints and the \u201dlost in the middle\u201d problem, and chunk-based retrieval-augmented generation (RAG)  \nsystems that utilize vector databases for semantic similarity search but often fail to capture critical clin  \nical entity relationships and contextual dependencies essential for accurate medical information extrac  \ntion.  \nA growing body of evidence shows that merely expanding context windows does not guarantee  \neffective use of information: performance often drops when relevant spans occur in the middle of long  \ninputs (\u201clost in the middle\u201d) [3]. This motivates entity-aware retrieval that selectively centers clinically  \nrelevant spans rather than relying on statistically similar but potentially off-target chunks.  \nThe CLinical Entity Augmented Retrieval (CLEAR) methodology, published by Lopez et al. in  \n2025 [1], introduced a novel approach that addresses these limitations through entity-aware, entity  \ncentered retrieval strategies. The original study demonstrated significant performance improvements  \n(F1 score of 0.90 vs 0.86 for traditional RAG) with substantial efficiency gains (71% token reduction,  \n72% faster inference time) on clinical information extraction tasks, positioning CLEAR as a potentially  \ntransformative approach for production EHR processing systems.  \nTo validate these claims in realistic healthcare scenarios and provide a robust evaluation framework  \nfor clinical NLP approaches, we developed the Clinical Notes Q&A Evaluation Platform. This com  \nprehensive validation study implements and compares three fundamental approaches: (1) wide context  \nprocessing for zero-shot inference with large language models, (2) traditional vector database chunking  \nwith embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu  \nmentReference processing. Our contributions include: systematic validation of CLEAR\u2019s performance  \nclaims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis  \nof retrieval strategy performance across clinical documents of varying complexity and length.\n\n# 1. **INTRODUCTION**  \nElectronic Health Record (EHR) systems in modern healthcare infrastructure store clinical documenta  \ntion within FHIR DocumentReference resources, typically encoded as base64 attachments containing  \nunstructured clinical notes. These documents, ranging from brief progress notes to comprehensive dis  \ncharge summaries, present substantial challenges for automated question-answering systems that require  \nsemantically accurate information extraction rather than statistical correlation-based retrieval common  \nin traditional vector database approaches.  \nContemporary approaches to clinical document processing have largely focused on two paradigms:  \nzero-shot inference with large context windows that process entire documents but face computational  \n2  \nconstraints and the \u201dlost in the middle\u201d problem, and chunk-based retrieval-augmented generation (RAG)  \nsystems that utilize vector databases for semantic similarity search but often fail to capture critical clin  \nical entity relationships and contextual dependencies essential for accurate medical information extrac  \ntion.  \nA growing body of evidence shows that merely expanding context windows does not guarantee  \neffective use of information: performance often drops when relevant spans occur in the middle of long  \ninputs (\u201clost in the middle\u201d) [3]. This motivates entity-aware retrieval that selectively centers clinically  \nrelevant spans rather than relying on statistically similar but potentially off-target chunks.  \nThe CLinical Entity Augmented Retrieval (CLEAR) methodology, published by Lopez et al. in  \n2025 [1], introduced a novel approach that addresses these limitations through entity-aware, entity  \ncentered retrieval strategies. The original study demonstrated significant performance improvements  \n(F1 score of 0.90 vs 0.86 for traditional RAG) with substantial efficiency gains (71% token reduction,  \n72% faster inference time) on clinical information extraction tasks, positioning CLEAR as a potentially  \ntransformative approach for production EHR processing systems.  \nTo validate these claims in realistic healthcare scenarios and provide a robust evaluation framework  \nfor clinical NLP approaches, we developed the Clinical Notes Q&A Evaluation Platform. This com  \nprehensive validation study implements and compares three fundamental approaches: (1) wide context  \nprocessing for zero-shot inference with large language models, (2) traditional vector database chunking  \nwith embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu  \nmentReference processing. Our contributions include: systematic validation of CLEAR\u2019s performance  \nclaims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis  \nof retrieval strategy performance across clinical documents of varying complexity and length."
        },
        "generated_answer": "Hello there! I can certainly help you understand that.\n\nIn the clinical domain, the methodology that uses entity-aware retrieval strategies to achieve improved semantic accuracy and computational efficiency when processing Electronic Health Record (EHR) notes is called **CLinical Entity Augmented Retrieval (CLEAR)**.\n\nThis approach was introduced by Lopez et al. in 2025 and represents a significant advancement. It addresses the limitations of earlier methods by focusing on \"entity-aware, entity-centered retrieval strategies\" that are aligned with clinical semantics.\n\n**Answer:**\nThe methodology in the clinical domain that uses entity-aware retrieval strategies to achieve improved semantic accuracy and computational efficiency when processing Electronic Health Record (EHR) notes is the **CLinical Entity Augmented Retrieval (CLEAR) methodology**.\n\nCLEAR introduced a novel approach that specifically focuses on entity-aware and entity-centered retrieval strategies, which are designed to handle the complexities and specific semantic requirements of clinical documentation. Unlike traditional vector database approaches that might rely on statistical correlations, CLEAR aims for semantically accurate information extraction.\n\nThe original study on CLEAR demonstrated notable improvements:\n*   **Semantic Accuracy:** It achieved a significant performance improvement with an F1 score of 0.90, compared to 0.86 for traditional Retrieval-Augmented Generation (RAG) methods, on clinical information extraction tasks. This indicates better accuracy in understanding and extracting relevant clinical information.\n*   **Computational Efficiency:** It also provided substantial efficiency gains, including a 71% reduction in tokens processed and a 72% faster inference time. This makes it a highly efficient approach for processing large volumes of clinical data.\n\nThese benefits position CLEAR as a potentially transformative approach for production EHR processing systems, as it effectively addresses challenges like the \"lost in the middle\" problem often encountered when processing long clinical narratives with other methods.\n\n**Key Points/Summary:**\n*   **Name:** CLinical Entity Augmented Retrieval (CLEAR) methodology.\n*   **Approach:** Uses entity-aware and entity-centered retrieval strategies specifically designed for clinical semantics.\n*   **Benefits:** Achieves improved semantic accuracy (higher F1 score) and significant computational efficiency (reduced tokens and faster inference time).\n*   **Purpose:** Addresses limitations of traditional methods in processing EHR notes for accurate information extraction.\n\n**Extra Tip:**\nThink of \"entity-aware\" retrieval like a smart librarian who doesn't just look for keywords, but understands the *meaning* of clinical terms (like specific diseases, medications, or symptoms) and their relationships within a patient's record. This allows for much more precise and relevant information retrieval compared to just matching similar-sounding text chunks!",
        "correctness_score": 0.95,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 6,
        "document_path": "static/Accelerating_LLM_Inference.pdf",
        "question": "Contrast ChunkLLM and Dynamic Hierarchical Sparse Attention (DHSA) regarding their dynamic attention management mechanisms, specifically addressing how they derive chunk representations and utilize them to achieve efficiency gains while preserving performance in long-context models.",
        "ground_truth": "ChunkLLM and Dynamic Hierarchical Sparse Attention (DHSA) both propose mechanisms for efficient long-context modeling by dynamically managing attention sparsity, but they employ different architectural additions and chunk representation strategies. ChunkLLM introduces two pluggable components: the QK Adapter (Q-Adapter and K-Adapter) and the Chunk Adapter. The Chunk Adapter is a one-layer feed-forward neural network (FNN) classifier that detects if a token is a chunk boundary using contextual semantic information. The QK Adapter fulfills feature compression and generates chunk attention scores, trained using an attention distillation approach where the Kullback-Leibler (KL) divergence between chunk attention scores and full attention scores guides optimization to enhance the recall rate of key chunks. ChunkLLM leverages the Intra-Chunk Attention Consistency (ICAC) pattern, triggering chunk selection updates exclusively when the current token is identified as a chunk boundary, substantially enhancing inference efficiency. ChunkLLM maintains 98.64% of the vanilla model's performance on long-context benchmarks and achieves a maximum speedup of 4.48x when processing 120K long texts. DHSA, conversely, is a plug-in module that dynamically predicts attention sparsity during prefill and decode stages without retraining the base model. DHSA employs a **Dynamic Hierarchical Sparsity Prediction approach. It first uses a boundary prediction function to adaptively segment input sequences into variable-length chunks. Chunk representations ($q_c$ and $k_c$) are derived by aggregating token queries and keys using a **length-normalized aggregation strategy, which involves scaling the sum of embeddings by the square root of the chunk size ($\\sqrt{|C|}$) to mitigate sensitivity to variable chunk lengths. It estimates chunk-level similarity ($S_c$) and then upsamples it to obtain the token-level similarity matrix ($S_t$), applying TOPK selection to generate the sparsity mask. DHSA reports matching dense attention in accuracy, while reducing prefill latency by 20-60% and peak memory usage by 35%.",
        "retrieved_context": {
            "context": "# Sparse Attention The sparse attention mechanism constructs sparse attention matrices by confining\nattention to predefined patterns, such as local windows or fixed-stride block patterns. Beltagy et al.\n(2020a) combined dilated local window attention with task-specific global attention. MoBA (Lu\net al., 2025) proposes an innovative mixed block attention mechanism. ESA (Wang et al., 2025)\nreduces computational costs by selecting tokens most critical to the current generation for attention\ncalculation. NSA (Yuan et al., 2025) combines coarse-grained token compression and fine-grained\ntoken selection. SepLLM (Chen et al., 2024) finds that the segment information between separators\ncan be effectively compressed into the separators themselves without causing significant information\nloss.\n\n# Sparse Attention The sparse attention mechanism constructs sparse attention matrices by confining\nattention to predefined patterns, such as local windows or fixed-stride block patterns. Beltagy et al.\n(2020a) combined dilated local window attention with task-specific global attention. MoBA (Lu\net al., 2025) proposes an innovative mixed block attention mechanism. ESA (Wang et al., 2025)\nreduces computational costs by selecting tokens most critical to the current generation for attention\ncalculation. NSA (Yuan et al., 2025) combines coarse-grained token compression and fine-grained\ntoken selection. SepLLM (Chen et al., 2024) finds that the segment information between separators\ncan be effectively compressed into the separators themselves without causing significant information\nloss.\n\n# Haojie Ouyang [1] **, Jianwei Lv** [2] **, Lei Ren** [2] **, Chen Wei** [2] **, Xiaojie Wang** [1] **, Fangxiang Feng** [1]  \n1 School of Artificial Intelligence, Beijing University of Posts and Telecommunications\n2 Li Auto  \nouyanghaojie@bupt.edu.cn  \nA BSTRACT  \nTransformer-based large models excel in natural language processing and computer vision, but face severe computational inefficiencies due to the self-attention\u2019s\nquadratic complexity with input tokens. Recently, researchers have proposed a\nseries of methods based on block selection and compression to alleviate this problem, but they either have issues with semantic incompleteness or poor traininginference efficiency. To comprehensively address these challenges, we propose\nChunkLLM, a lightweight and pluggable training framework. Specifically, we\nintroduce two components: QK Adapter (Q-Adapter and K-Adapter) and Chunk\nAdapter. The former is attached to each Transformer layer, serving dual purposes\nof feature compression and chunk attention acquisition. The latter operates at the\nbottommost layer of the model, functioning to detect chunk boundaries by leveraging contextual semantic information. During the training phase, the parameters\nof the backbone remain frozen, with only the QK Adapter and Chunk Adapter\nundergoing training. Notably, we design an attention distillation method for training the QK Adapter, which enhances the recall rate of key chunks. During the\ninference phase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference. Experimental\nevaluations are conducted on a diverse set of long-text and short-text benchmark\ndatasets spanning multiple tasks. ChunkLLM not only attains comparable performance on short-text benchmarks but also maintains 98.64% of the performance\non long-context benchmarks while preserving a 48.58% key-value cache retention\nrate. Particularly, ChunkLLM attains a maximum speedup of 4.48\u00d7 in comparison\nto the vanilla Transformer in the processing of 120K long texts.  \n1 I NTRODUCTION  \nTransformer-based large models (Vaswani et al., 2017) have demonstrated exceptional performance\nacross a diverse range of tasks, including natural language processing (Srivastava et al., 2025; Zhang\net al., 2024) and computer vision (Jiang et al., 2025). However, they have also faced significant challenges in terms of computational efficiency, particularly when scaling to larger structures and large\ncontext inputs. A core issue of efficiency limitations lies in the self-attention module, whose computational complexity is a quadratic relationship with the number of input tokens. Such deficiencies\nin computational efficiency exert a profound impact on both the training complexity and inference\nlatency of large models.  \nEfficiency optimization of Transformer has emerged as a pivotal research domain, with efforts predominantly converging into three methodological paradigms. **Linear attention**, such as Mamba\n(Dao & Gu, 2024), RWKV (Peng et al., 2023b; 2024), and RetNet (Sun et al., 2023), seek to\napproximate and substitute the traditional softmax-based self-attention mechanism. However, the\nfundamental architectural disparities between linear attention and conventional attention mechanisms introduce non-trivial challenges: adapting pre-existing Transformer models to integrate linear attention often incurs prohibitive conversion costs(Mercat et al., 2024; Wang et al., 2024; Bick\net al., 2024), while alternative strategies necessitate end-to-end training of entirely new model from\nscratch(Li et al., 2025). Another optimization paradigm is **Sparse attention**, which leverages predefined structural constraints, such as sink-based attention mechanisms (Xiao et al., 2024) or sliding  \n1  \nwindow attention mechanisms (Beltagy et al., 2020b), to exploit this sparsity. While these methods may yield certain effects, they often rely heavily on specific tasks, which can limit the overall\ngeneralization ability of the model. Dynamic sparse attention mechanisms (Tang et al., 2024; Jiang\net al., 2024; Liu et al., 2024) filter out subsets of tokens during the inference phase. Although such\nmethods can reduce the computational load of long sequences, they fail to significantly lower the\nhigh training costs of long-context models, making it difficult for large language models to efficiently scale to context-processing tasks with million-level token sizes. **Chunk Selective attention**,\na special type of sparse attention, can be primarily categorized into two paradigms: fixed chunk\n(Lu et al., 2025; Yuan et al., 2025; Wang et al., 2025) and separators-based dynamic chunk (Chen\net al., 2024). Both approaches partition the input into discrete chunks: the former conducts partitioning with a fixed length, which gives rise to semantic incompleteness; the latter utilizes separators\nfor partitioning, yet ambiguities often emerge. For example, periods frequently occur in numerical\nvalues or abbreviations. Furthermore, during the inference phase, these methods necessitate chunk\nselection for each generated token, incurring additional computational overhead. It is thus evident\nthat existing efficient approaches still exhibit inherent limitations.  \nTo address the aforementioned challenges, We propose ChunkLLM, which can be directly constructed by integrating two lightweight and trainable modules into existing LLMs: **QK Adapter**\nand **Chunk Adapter** . The Chunk Adapter connects to the output of the bottommost Transformer\nlayer and used for identify if a token is the last token of a chunk. The QK Adapter is in parallel with\nQ and K matrix at each Transformer layer. It maps full attention scores to chunk attention scores,\nand trained by a distillation approach.  \nThe QK Adapter fulfills feature compression and the generation of chunk attention scores. To train\nthe QK Adapter, we propose an attention distillation approach designed to enhance the recall rate\nof key chunks. During training, LLM parameters are kept frozen, with the Kullback\u2013Leibler (KL)\ndivergence between chunk attention scores and full attention scores serving as a guidance signal for\noptimization. The Chunk Adapter determines whether a token corresponds to a chunk boundary by\nleveraging contextual semantic information. During the inference phase, we exploit the Intra-Chunk\nAttention Consistency (ICAC) pattern such that chunk selection is only updated when the current\ntoken is identified as a chunk boundary, which substantially enhances inference efficiency. Furthermore, ChunkLLM can achieve inference performance comparable to that of models optimized for\n120K context lengths, despite being trained solely on 4K context lengths, thereby substantially reducing the training overhead associated with 120K context scaling. Experimental results validate\nthat ChunkLLM yields a 4.48\u00d7 speedup relative to the vanilla Transformer when processing 120K\nlong texts.  \nOur contributions are summarized as follows:  \n- We introduce ChunkLLM that integrates two lightweight and pluggable components into\nexisting LLMs: the QK Adapter and the Chunk Adapter. The newly developed ChunkLLM only necessitates fine-tuning these lightweight components on the basis of the original model architecture. This design enables ChunkLLM to attain performance comparable\nto vanilla Transformer while utilizing a smaller KV cache, alongside achieving effective\ncontrol over computational scale.  \n- We propose an attention distillation-based training approach for the QK Adapter, which\nleverages KL divergence to drive chunk attention toward approximating full attention, effectively enhancing the recall rate of key chunks. Furthermore, we introduce a novel ICAC\npattern, which yields notable improvements in inference efficiency for long-context scenarios.  \n- Experimental evaluations show that ChunkLLM not only attains comparable performance\non short-text benchmarks but also maintains 98.64% of the performance on long-context\nbenchmarks while preserving a 48.58% key-value cache (kvcache) retention rate, relative\nto the vanilla Transformer. Particularly, ChunkLLM attains a maximum speedup of 4.48\u00d7\nin comparison to the vanilla Transformer in the processing of 120K long texts.  \n2  \n2 M ETHOD  \nThe framework of ChunkLLM is shown in Figure 1. ChunkLLM can be built on any existing\ntransformer-based LLMs. Two extra lightweight and pluggable modules are designed to support\nchunk-related capability. One is Chunk Adapter, which is used to identify chunk boundaries. The\nother is the Q Adapter and K Adapter, which is tailored for efficient feature compression and chunk\nselection. This section elaborates on the details of the two modules.  \n2.1 C HUNK A DAPTER  \nThe Chunk Adapter is a one-layer forward\nneural network (FNN) classifier for chunk\nboundary prediction. Its input is the output\nof the first layer of the LLM, and the output is if or not the token is a chunk boundary, as depicted in the Figure 1.  \nFor an input X = _{x_ 1 _, x_ 2 _, ..., x_ _n\u2212_ 1 _, x_ _n_ _}_\nwith _n_ tokens and their corresponding labels Y = _{y_ 1 _, y_ 2 _, ..., y_ _n\u2212_ 1 _, y_ _n_ _}_, _y_ _i_ _\u2208_\n_{_ 0 _,_ 1 _}_, where 1 indicates that the token\n_x_ _i_ is a chunk boundary, 0 indicates that it\ndoes not, **H** _[l]_ _i_ [1] [be the output of] _[ x]_ _[i]_ [ at first]\nlayer. The FNN based Chunk Adapter is\ngiven as in equation 1  \n\u02c6 1 _,_ Sigmoid(FFN( **H** _[l]_ _i_ [1] [))] _[ > \u03b1,]_\n_y_ _i_ =\n\ufffd0 _,_ _otherwise_\n(1)  \noutput  \nInput  \nFigure 1: The framework of ChunkLLM.  \nFor training the chunk adapter, we employ\nthe binary cross-entropy loss (BCE) (equation 2) as the objective function. Detailed\ninformation on the training dataset will be given in the experiment part.  \n_L_ _CBP_ = _\u2212_ [1]  \n_n_  \n_n_  \n\u02c6  \n\ufffd[ _y_ _i_ _\u00b7 log_ (\u02c6 _y_ _i_ ) + (1 _\u2212_ _y_ _i_ ) _\u00b7 log_ (1 _\u2212_ _y_ _i_ )] (2)  \n_i_ =1  \n2.2 QK A DAPTER  \nAt each layer of the LLM, we incorporate a Q-Adapter and a K-Adapter which used to compress the\nattention and select the chunks.  \nFor each layer, let **Q** and **K** be the attention matrix respectively, _c_ be the chunk number of the input,\n_Index_ ~~_c_~~ = _{i_ 1 _, i_ 2 _, ...i_ _c_ _}_ is the index set of chunk boundary tokens. Let **K** [\u02c6] be the K matrix of these\ntokens. We then calculate chunk attention scores as follows:\n\n# Haojie Ouyang [1] **, Jianwei Lv** [2] **, Lei Ren** [2] **, Chen Wei** [2] **, Xiaojie Wang** [1] **, Fangxiang Feng** [1]  \n1 School of Artificial Intelligence, Beijing University of Posts and Telecommunications\n2 Li Auto  \nouyanghaojie@bupt.edu.cn  \nA BSTRACT  \nTransformer-based large models excel in natural language processing and computer vision, but face severe computational inefficiencies due to the self-attention\u2019s\nquadratic complexity with input tokens. Recently, researchers have proposed a\nseries of methods based on block selection and compression to alleviate this problem, but they either have issues with semantic incompleteness or poor traininginference efficiency. To comprehensively address these challenges, we propose\nChunkLLM, a lightweight and pluggable training framework. Specifically, we\nintroduce two components: QK Adapter (Q-Adapter and K-Adapter) and Chunk\nAdapter. The former is attached to each Transformer layer, serving dual purposes\nof feature compression and chunk attention acquisition. The latter operates at the\nbottommost layer of the model, functioning to detect chunk boundaries by leveraging contextual semantic information. During the training phase, the parameters\nof the backbone remain frozen, with only the QK Adapter and Chunk Adapter\nundergoing training. Notably, we design an attention distillation method for training the QK Adapter, which enhances the recall rate of key chunks. During the\ninference phase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference. Experimental\nevaluations are conducted on a diverse set of long-text and short-text benchmark\ndatasets spanning multiple tasks. ChunkLLM not only attains comparable performance on short-text benchmarks but also maintains 98.64% of the performance\non long-context benchmarks while preserving a 48.58% key-value cache retention\nrate. Particularly, ChunkLLM attains a maximum speedup of 4.48\u00d7 in comparison\nto the vanilla Transformer in the processing of 120K long texts.  \n1 I NTRODUCTION  \nTransformer-based large models (Vaswani et al., 2017) have demonstrated exceptional performance\nacross a diverse range of tasks, including natural language processing (Srivastava et al., 2025; Zhang\net al., 2024) and computer vision (Jiang et al., 2025). However, they have also faced significant challenges in terms of computational efficiency, particularly when scaling to larger structures and large\ncontext inputs. A core issue of efficiency limitations lies in the self-attention module, whose computational complexity is a quadratic relationship with the number of input tokens. Such deficiencies\nin computational efficiency exert a profound impact on both the training complexity and inference\nlatency of large models.  \nEfficiency optimization of Transformer has emerged as a pivotal research domain, with efforts predominantly converging into three methodological paradigms. **Linear attention**, such as Mamba\n(Dao & Gu, 2024), RWKV (Peng et al., 2023b; 2024), and RetNet (Sun et al., 2023), seek to\napproximate and substitute the traditional softmax-based self-attention mechanism. However, the\nfundamental architectural disparities between linear attention and conventional attention mechanisms introduce non-trivial challenges: adapting pre-existing Transformer models to integrate linear attention often incurs prohibitive conversion costs(Mercat et al., 2024; Wang et al., 2024; Bick\net al., 2024), while alternative strategies necessitate end-to-end training of entirely new model from\nscratch(Li et al., 2025). Another optimization paradigm is **Sparse attention**, which leverages predefined structural constraints, such as sink-based attention mechanisms (Xiao et al., 2024) or sliding  \n1  \nwindow attention mechanisms (Beltagy et al., 2020b), to exploit this sparsity. While these methods may yield certain effects, they often rely heavily on specific tasks, which can limit the overall\ngeneralization ability of the model. Dynamic sparse attention mechanisms (Tang et al., 2024; Jiang\net al., 2024; Liu et al., 2024) filter out subsets of tokens during the inference phase. Although such\nmethods can reduce the computational load of long sequences, they fail to significantly lower the\nhigh training costs of long-context models, making it difficult for large language models to efficiently scale to context-processing tasks with million-level token sizes. **Chunk Selective attention**,\na special type of sparse attention, can be primarily categorized into two paradigms: fixed chunk\n(Lu et al., 2025; Yuan et al., 2025; Wang et al., 2025) and separators-based dynamic chunk (Chen\net al., 2024). Both approaches partition the input into discrete chunks: the former conducts partitioning with a fixed length, which gives rise to semantic incompleteness; the latter utilizes separators\nfor partitioning, yet ambiguities often emerge. For example, periods frequently occur in numerical\nvalues or abbreviations. Furthermore, during the inference phase, these methods necessitate chunk\nselection for each generated token, incurring additional computational overhead. It is thus evident\nthat existing efficient approaches still exhibit inherent limitations.  \nTo address the aforementioned challenges, We propose ChunkLLM, which can be directly constructed by integrating two lightweight and trainable modules into existing LLMs: **QK Adapter**\nand **Chunk Adapter** . The Chunk Adapter connects to the output of the bottommost Transformer\nlayer and used for identify if a token is the last token of a chunk. The QK Adapter is in parallel with\nQ and K matrix at each Transformer layer. It maps full attention scores to chunk attention scores,\nand trained by a distillation approach.  \nThe QK Adapter fulfills feature compression and the generation of chunk attention scores. To train\nthe QK Adapter, we propose an attention distillation approach designed to enhance the recall rate\nof key chunks. During training, LLM parameters are kept frozen, with the Kullback\u2013Leibler (KL)\ndivergence between chunk attention scores and full attention scores serving as a guidance signal for\noptimization. The Chunk Adapter determines whether a token corresponds to a chunk boundary by\nleveraging contextual semantic information. During the inference phase, we exploit the Intra-Chunk\nAttention Consistency (ICAC) pattern such that chunk selection is only updated when the current\ntoken is identified as a chunk boundary, which substantially enhances inference efficiency. Furthermore, ChunkLLM can achieve inference performance comparable to that of models optimized for\n120K context lengths, despite being trained solely on 4K context lengths, thereby substantially reducing the training overhead associated with 120K context scaling. Experimental results validate\nthat ChunkLLM yields a 4.48\u00d7 speedup relative to the vanilla Transformer when processing 120K\nlong texts.  \nOur contributions are summarized as follows:  \n- We introduce ChunkLLM that integrates two lightweight and pluggable components into\nexisting LLMs: the QK Adapter and the Chunk Adapter. The newly developed ChunkLLM only necessitates fine-tuning these lightweight components on the basis of the original model architecture. This design enables ChunkLLM to attain performance comparable\nto vanilla Transformer while utilizing a smaller KV cache, alongside achieving effective\ncontrol over computational scale.  \n- We propose an attention distillation-based training approach for the QK Adapter, which\nleverages KL divergence to drive chunk attention toward approximating full attention, effectively enhancing the recall rate of key chunks. Furthermore, we introduce a novel ICAC\npattern, which yields notable improvements in inference efficiency for long-context scenarios.  \n- Experimental evaluations show that ChunkLLM not only attains comparable performance\non short-text benchmarks but also maintains 98.64% of the performance on long-context\nbenchmarks while preserving a 48.58% key-value cache (kvcache) retention rate, relative\nto the vanilla Transformer. Particularly, ChunkLLM attains a maximum speedup of 4.48\u00d7\nin comparison to the vanilla Transformer in the processing of 120K long texts.  \n2  \n2 M ETHOD  \nThe framework of ChunkLLM is shown in Figure 1. ChunkLLM can be built on any existing\ntransformer-based LLMs. Two extra lightweight and pluggable modules are designed to support\nchunk-related capability. One is Chunk Adapter, which is used to identify chunk boundaries. The\nother is the Q Adapter and K Adapter, which is tailored for efficient feature compression and chunk\nselection. This section elaborates on the details of the two modules.  \n2.1 C HUNK A DAPTER  \nThe Chunk Adapter is a one-layer forward\nneural network (FNN) classifier for chunk\nboundary prediction. Its input is the output\nof the first layer of the LLM, and the output is if or not the token is a chunk boundary, as depicted in the Figure 1.  \nFor an input X = _{x_ 1 _, x_ 2 _, ..., x_ _n\u2212_ 1 _, x_ _n_ _}_\nwith _n_ tokens and their corresponding labels Y = _{y_ 1 _, y_ 2 _, ..., y_ _n\u2212_ 1 _, y_ _n_ _}_, _y_ _i_ _\u2208_\n_{_ 0 _,_ 1 _}_, where 1 indicates that the token\n_x_ _i_ is a chunk boundary, 0 indicates that it\ndoes not, **H** _[l]_ _i_ [1] [be the output of] _[ x]_ _[i]_ [ at first]\nlayer. The FNN based Chunk Adapter is\ngiven as in equation 1  \n\u02c6 1 _,_ Sigmoid(FFN( **H** _[l]_ _i_ [1] [))] _[ > \u03b1,]_\n_y_ _i_ =\n\ufffd0 _,_ _otherwise_\n(1)  \noutput  \nInput  \nFigure 1: The framework of ChunkLLM.  \nFor training the chunk adapter, we employ\nthe binary cross-entropy loss (BCE) (equation 2) as the objective function. Detailed\ninformation on the training dataset will be given in the experiment part.  \n_L_ _CBP_ = _\u2212_ [1]  \n_n_  \n_n_  \n\u02c6  \n\ufffd[ _y_ _i_ _\u00b7 log_ (\u02c6 _y_ _i_ ) + (1 _\u2212_ _y_ _i_ ) _\u00b7 log_ (1 _\u2212_ _y_ _i_ )] (2)  \n_i_ =1  \n2.2 QK A DAPTER  \nAt each layer of the LLM, we incorporate a Q-Adapter and a K-Adapter which used to compress the\nattention and select the chunks.  \nFor each layer, let **Q** and **K** be the attention matrix respectively, _c_ be the chunk number of the input,\n_Index_ ~~_c_~~ = _{i_ 1 _, i_ 2 _, ...i_ _c_ _}_ is the index set of chunk boundary tokens. Let **K** [\u02c6] be the K matrix of these\ntokens. We then calculate chunk attention scores as follows:"
        },
        "generated_answer": "Based on the provided notes, I can explain ChunkLLM's dynamic attention management mechanisms, how it derives chunk representations, and how it utilizes them for efficiency. However, the notes do not contain any information about Dynamic Hierarchical Sparse Attention (DHSA), so I cannot provide a contrast with it.\n\n---\n\n### ChunkLLM's Dynamic Attention Management\n\n**Answer:**\nChunkLLM addresses the computational inefficiency of Transformer models, particularly with long contexts, by introducing two lightweight and pluggable components: the **Chunk Adapter** and the **QK Adapter**. These components work together to manage attention dynamically, focusing computation on relevant parts of the input.\n\n1.  **Dynamic Attention Management and Chunk Boundary Detection:**\n    *   The **Chunk Adapter** is responsible for dynamically identifying \"chunk boundaries\" within the input text. It operates at the bottommost layer of the LLM and is a one-layer Feedforward Neural Network (FNN) classifier. Its input is the output of the *first* layer of the LLM, and its job is to predict whether a given token is the last token of a chunk (a binary classification: 1 for boundary, 0 otherwise). This allows the model to adaptively segment the input based on contextual semantic information rather than fixed-length partitioning.\n    *   During the inference phase, ChunkLLM leverages an **Intra-Chunk Attention Consistency (ICAC)** pattern. This means that chunk selection\u2014which determines what parts of the input are attended to\u2014is *only updated when the current token is identified as a chunk boundary*. This dynamic updating mechanism avoids redundant computations for every single token, significantly enhancing inference efficiency.\n\n2.  **Deriving and Utilizing Chunk Representations for Efficiency:**\n    *   The **QK Adapter** (consisting of Q-Adapter and K-Adapter) is attached to each Transformer layer, working in parallel with the standard Query (Q) and Key (K) matrices. Its primary roles are **feature compression** and **chunk attention acquisition**.\n    *   Instead of calculating full attention scores over all tokens, the QK Adapter maps these full attention scores to \"chunk attention scores.\" Crucially, it uses a subset of the Key matrix, specifically **K\u0302 (K-hat)**, which represents the K matrix of the *chunk boundary tokens* identified by the Chunk Adapter. This means that the \"chunk representation\" for attention calculation is effectively derived from the features of these semantically important boundary tokens.\n    *   By confining attention calculations to these selected chunk boundary tokens, ChunkLLM drastically reduces the computational load, as it no longer needs to compute attention over the entire sequence. This also leads to a more efficient use of the key-value (KV) cache.\n\n3.  **Preserving Performance in Long-Context Models:**\n    *   To ensure that this efficiency doesn't come at the cost of performance, ChunkLLM employs an **attention distillation method** for training the QK Adapter. During training, the parameters of the main LLM backbone are frozen, and only the QK Adapter and Chunk Adapter are trained. The distillation process uses **Kullback\u2013Leibler (KL) divergence** to guide the chunk attention scores to approximate the full attention scores. This helps the QK Adapter \"learn\" to preserve the most critical information, thereby enhancing the recall rate of key chunks.\n    *   Experimental results show that ChunkLLM can achieve performance comparable to vanilla Transformers on short-text benchmarks and maintains 98.64% of the performance on long-context benchmarks, all while achieving a 4.48\u00d7 speedup and a 48.58% KV cache retention rate for 120K long texts. This demonstrates its ability to maintain high performance despite significant efficiency gains.\n\n**Regarding Dynamic Hierarchical Sparse Attention (DHSA):**\nI apologize, but the provided notes do not contain any information about Dynamic Hierarchical Sparse Attention (DHSA). Therefore, I cannot contrast ChunkLLM with DHSA based on the given context. If you have more notes that include DHSA, please provide them, and I'd be happy to help!\n\n---\n\n**Key Points/Summary:**\n*   **Dynamic Chunking:** ChunkLLM uses a Chunk Adapter (FNN classifier) to dynamically detect chunk boundaries based on contextual semantics.\n*   **Efficient Attention:** A QK Adapter performs feature compression and calculates chunk attention scores by attending to features of identified chunk boundary tokens (K\u0302).\n*   **Inference Optimization:** Chunk selection is updated only when a chunk boundary is detected (ICAC pattern), significantly reducing computational overhead.\n*   **Performance Preservation:** An attention distillation method (KL divergence) trains the QK Adapter to approximate full attention, ensuring high performance (e.g., 98.64% of vanilla Transformer on long contexts) while achieving substantial speedups and KV cache savings.\n\n**Extra Tip:**\nThink of the Chunk Adapter as a smart gatekeeper that decides when to \"re-evaluate\" the most important parts of a long conversation. Instead of constantly checking every single word, it only opens its gate and triggers a deeper look (via the QK Adapter's focused attention) when it senses a significant \"chunk boundary\" \u2013 like the end of a sentence, paragraph, or a topic shift. This selective processing is what makes ChunkLLM so efficient for very long texts!",
        "correctness_score": 0.25,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 7,
        "document_path": "multiple files",
        "question": "Explain the observed trade-offs between computational efficiency and semantic integrity across various chunking and retrieval strategies\u2014including Fixed-size/Semantic Chunking, Late Chunking, Contextual Retrieval, and Clinical Entity Augmented Retrieval (CLEAR)\u2014and identify which approach demonstrated superior scalability advantages in high-complexity clinical documents.",
        "ground_truth": "The sources reveal significant trade-offs among various chunking and retrieval strategies concerning computational cost and the preservation of semantic integrity. 1. Fixed-size vs. Semantic Chunking (RAG Baseline): Traditional fixed-size chunking is computationally simple and efficient. However, its simplicity risks fragmenting semantically related content, leading to suboptimal retrieval. Semantic chunking, which aims for semantically coherent segments, involves additional computational costs that the sources found were often not justified by consistent performance gains on standard document structures. Overall, fixed-size chunking was suggested as a more efficient and reliable choice for practical RAG applications on non-synthetic datasets. 2. Late Chunking vs. Contextual Retrieval: Late Chunking defers segmentation until after the entire document is embedded, preserving full contextual information for efficiency. Late Chunking offers higher efficiency but may sacrifice relevance and completeness. In contrast, Contextual Retrieval enhances chunks by prompting an LLM to generate additional context for each chunk, improving contextual integrity. This context preservation, particularly when combined with Rank Fusion (ContextualRankFusion), yields better overall results in retrieval evaluation than Late Chunking but incurs greater computational resources, potentially requiring up to 20GB of VRAM for chunk contextualization in long documents. 3. Clinical Entity Augmented Retrieval (CLEAR): This entity-aware method achieves a balance by selectively centering clinically relevant spans around identified entities, overcoming the positional bias ('lost in the middle' problem) associated with processing entire long documents. CLEAR achieved a 78.4% token savings compared to Wide Context processing while maintaining the highest average semantic similarity (0.878), demonstrating an optimal balance between accuracy and computational cost. The approach that demonstrated superior scalability advantages in high-complexity documents was CLEAR. In evaluations involving large clinical notes (exceeding 65,000 tokens), CLEAR achieved a 75% win rate, confirming that its entity-aware retrieval advantages grow as document complexity and document size increase, making it highly suitable for large EHR document processing",
        "retrieved_context": {
            "context": "## 1.1 **Related Work**  \nEntity-aware retrieval has gained increasing attention within biomedical NLP and question-answering  \ndomains. Early retrieval-augmented methods such as RAG [2] demonstrated the potential of embedding  \nbased chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches  \nleveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al.,  \n2016) provided partial improvements but often failed to maintain contextual continuity across long clin  \nical narratives.  \nCLEAR [1] represented a significant advancement by introducing entity-centered retrieval aligned  \nwith clinical semantics. Our work extends this line of research by operationalizing CLEAR within an  \nend-to-end evaluation platform, providing reproducible empirical validation across realistic EHR-scale  \ndocument sets.  \nRecent evaluations of retrieval-augmented models in long-context reasoning (e.g., Karpinska et al.,  \n2023; Xiong et al., 2024) emphasize that retrieval strategies often outperform naive long-context prompt  \ning, supporting the need for entity-aware retrieval. Our work contributes a reproducible evaluation  \nframework within this paradigm, focusing on realistic EHR-scale clinical notes.  \n3\n\n# 1. **INTRODUCTION**  \nElectronic Health Record (EHR) systems in modern healthcare infrastructure store clinical documenta  \ntion within FHIR DocumentReference resources, typically encoded as base64 attachments containing  \nunstructured clinical notes. These documents, ranging from brief progress notes to comprehensive dis  \ncharge summaries, present substantial challenges for automated question-answering systems that require  \nsemantically accurate information extraction rather than statistical correlation-based retrieval common  \nin traditional vector database approaches.  \nContemporary approaches to clinical document processing have largely focused on two paradigms:  \nzero-shot inference with large context windows that process entire documents but face computational  \n2  \nconstraints and the \u201dlost in the middle\u201d problem, and chunk-based retrieval-augmented generation (RAG)  \nsystems that utilize vector databases for semantic similarity search but often fail to capture critical clin  \nical entity relationships and contextual dependencies essential for accurate medical information extrac  \ntion.  \nA growing body of evidence shows that merely expanding context windows does not guarantee  \neffective use of information: performance often drops when relevant spans occur in the middle of long  \ninputs (\u201clost in the middle\u201d) [3]. This motivates entity-aware retrieval that selectively centers clinically  \nrelevant spans rather than relying on statistically similar but potentially off-target chunks.  \nThe CLinical Entity Augmented Retrieval (CLEAR) methodology, published by Lopez et al. in  \n2025 [1], introduced a novel approach that addresses these limitations through entity-aware, entity  \ncentered retrieval strategies. The original study demonstrated significant performance improvements  \n(F1 score of 0.90 vs 0.86 for traditional RAG) with substantial efficiency gains (71% token reduction,  \n72% faster inference time) on clinical information extraction tasks, positioning CLEAR as a potentially  \ntransformative approach for production EHR processing systems.  \nTo validate these claims in realistic healthcare scenarios and provide a robust evaluation framework  \nfor clinical NLP approaches, we developed the Clinical Notes Q&A Evaluation Platform. This com  \nprehensive validation study implements and compares three fundamental approaches: (1) wide context  \nprocessing for zero-shot inference with large language models, (2) traditional vector database chunking  \nwith embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu  \nmentReference processing. Our contributions include: systematic validation of CLEAR\u2019s performance  \nclaims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis  \nof retrieval strategy performance across clinical documents of varying complexity and length.\n\n# Retrieval-Augmented Generation (RAG): Semantic chunking with embedding-based retrieval us  \ning top-k chunk selection. This approach prioritizes efficiency with minimal token usage (average 544  \ntokens per query) but may miss critical clinical relationships.\n\n# 5. **CONCLUSION**  \nThis study developed and deployed a comprehensive Clinical Notes Q&A Evaluation Platform and found  \nresults consistent with the benefits reported for Clinical Entity Augmented Retrieval (CLEAR) in prior  \nwork. In our synthetic EHR QA setting, entity-aware retrieval achieved stronger semantic similarity  \nthan wide-context processing at markedly lower token budgets, echoing the efficiency\u2013quality trade-offs  \nhighlighted by CLEAR [1].  \nThe validation results strongly confirm the original research findings, particularly demonstrating  \nscalability advantages on large clinical documents characteristic of comprehensive EHR DocumentRe  \nference content. The 75% win rate on documents exceeding 65,000 tokens validates that entity-aware  \nretrieval becomes increasingly advantageous as document complexity increases, confirming the method  \nology\u2019s suitability for enterprise healthcare environments.  \n10  \nFigure 4: Interactive interface showing different analysis approaches and their corresponding performance scores, enabling experimentation to improve reasoning accuracy.  \nThe Clinical Notes Q&A Evaluation Platform represents a significant contribution to clinical NLP  \nresearch by providing a systematic framework for evaluating retrieval strategies in realistic EHR pro  \ncessing scenarios. The platform\u2019s validation of CLEAR methodology demonstrates its viability as a  \nproduction-ready approach for clinical information extraction systems requiring optimal balance be  \ntween semantic accuracy and computational efficiency.  \nThe demonstrated effectiveness of CLEAR through systematic platform-based validation provides  \nevidence-based guidance for healthcare organizations implementing clinical question-answering sys  \ntems. The evaluation platform framework enables continued research and development in clinical  \nentity-aware retrieval methodologies while supporting reproducible evaluation of future clinical NLP  \ninnovations in production-relevant contexts.\n\n# Background: Electronic Health Records (EHR) systems store clinical documentation in FHIR  \nDocumentReference resources as base64-encoded attachments, presenting significant challenges  \nfor semantic question-answering applications. Traditional approaches using statistical correlation  \nthrough vector database chunking often fail to capture the nuanced clinical relationships required  \nfor accurate medical information extraction. The Clinical Entity Augmented Retrieval (CLEAR)  \nmethodology, introduced by Lopez et al. (2025) [1], addresses these limitations through entity\naware retrieval strategies and reports improved performance (F1 0.90 vs. 0.86 for embedding RAG;  \n_>_ 70% fewer tokens and faster inference).\n\n# mantic chunking, which aims to improve retrieval performance by dividing documents into\nsemantically coherent segments. Despite its\ngrowing adoption, the actual benefits over simpler **fixed-size chunking**, where documents are\nsplit into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrievalbased answer generation. The results show that\nthe computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the\nprevious assumptions about semantic chunking and highlight the need for more efficient\nchunking strategies in RAG systems.\n\n# 4. **DISCUSSION**  \n## 4.1 **Validation of Research Claims**  \nOur enhanced implementation provides observations consistent with the direction of the original CLEAR  \nfindings [1] on our synthetic clinical QA benchmark. While our task, retriever, and baselines differ from  \nthe original study (which evaluated structured information extraction with model-based NER and on  \ntology/LLM synonym augmentation), we still observe that entity-aware retrieval yields higher semantic  \nsimilarity at substantially lower token budgets than wide-context processing.  \nThe 75% win rate on large documents (65K+ tokens) supports the hypothesis that entity-aware  \nretrieval advantages grow with document complexity, consistent with prior work emphasizing targeted  \nretrieval over long context processing. This finding has significant implications for clinical applications  \ninvolving comprehensive patient records and complex clinical assessments.  \n8  \nFigure 2: CLEAR dominates at a 3% efficiency bonus, maintaining superior adjusted accuracy across\nall notes.\n\n# 5 **Conclusion**  \nIn this paper, we evaluated semantic and fixed-size\nchunking strategies in RAG systems across document retrieval, evidence retrieval, and answer generation. Semantic chunking occasionally improved\nperformance, particularly on stitched datasets with\nhigh topic diversity. However, these benefits were\nhighly context-dependent and did not consistently\njustify the additional computational cost. On nonsynthetic datasets that better reflect real-world documents, fixed-size chunking often performed better.\nOverall, our results suggest that fixed-size chunking remains a more efficient and reliable choice for\npractical RAG applications. The impact of chunking strategy was often overshadowed by other factors, such as the quality of embeddings, especially\nwhen computational resources are limited or when\nworking with standard document structures.\n\n# Fig. 1. Comparison of early chunking (left) and late chunking (right) approaches for\nprocessing long documents. In early chunking, the document is divided into chunks\nbefore embedding, with each chunk processed independently by the embedding model\nand then pooled. In contrast, late chunking processes the entire document to generate\ntoken embeddings first, using boundary cues to create chunk embeddings, which are\nsubsequently pooled.  \n_Rank Fusion._ In our methodology, we employ a rank fusion strategy that integrates dense embeddings with sparse embeddings of BM25 [19] to improve\nretrieval performance. Although embedding models adeptly capture semantic\nrelationships, they may overlook exact matches, which is particularly useful for\nunique identifiers or technical terms. BM25 uses a ranking function that builds\nupon Term Frequency-Inverse Document Frequency (TF-IDF), addressing this\nlimitation by emphasizing precise lexical matches. To combine the strengths of\nboth approaches, we conduct searches across both dense embedding vectors and\nBM25 sparse embedding vectors generated from both the chunk and its generated context. Initially, the assigned relative importance in the search for the two\nvector fields has been set to be of equal intensity, resulting in lowering the scoring\nresults in the retrieval evaluation. For this reason we use a weighting strategy\nassigning higher weights to dense vector fields, emphasizing them more in the\nfinal ranking. While different weight parameters have been tested, the final decision has been to define a ratio of importance 4:1 assigning weight 1 for the dense  \n6 J. Singh and C. Merola\n\n# 1 **Introduction**  \nIn Retrieval-Augmented Generation (RAG) systems, cutting documents into smaller units called\n\u201cchunks\u201d has a crucial effect on the quality of both\nretrieval and generation tasks (Chen et al., 2023;\nWadhwa et al., 2024; Shi et al., 2023; Yu et al.,  \n2023). By retrieving the most relevant chunks for\na given query and feeding them into a generative\nlanguage model, these systems aim to produce accurate and contextually appropriate answers. However, the effectiveness of chunking strategies remains a significant challenge in optimizing retrieval\nquality and computational efficiency (Lewis et al.,\n2020; Finardi et al., 2024).  \nKnown as **fixed-size chunking**, the traditional\nway to chunk is to cut documents into chunks of a\nfixed length such as 200 tokens (Gao et al., 2023).\nWhile computationally simple, this approach can\nfragment semantically related content across multiple chunks, leading to suboptimal retrieval performance. Recently, there has been a surge of interest  \nin **semantic chunking**, where documents are segmented based on semantic similarity, with some industry applications suggesting promising improvements in performance (LangChain, 2024; LlamaIndex, 2024; McCormick, 2024). However, there\nis no systematic evidence that semantic chunking\nyields a performance gain in downstream tasks,\nand if there is, the gain is significant enough to\njustify the computational overhead than fixed-size\nchunking.\nSuch a systematic evaluation is not trivial due to\nthe lack of data that can be directly used to compare chunking strategies. Therefore, we design an\nindirect evaluation using three proxy tasks: (1) document retrieval, measuring the ability to identify\nrelevant documents; (2) evidence retrieval, measuring the ability to locate ground-truth evidence;\nand (3) answer generation, testing the quality of\nanswers produced by a generative model using retrieved chunks. Our findings challenge prevailing\nassumptions about the benefits of semantic chunking, suggesting that its advantages are highly taskdependent and often insufficient to justify the added\ncomputational costs. This study lays the groundwork for future exploration of more efficient and\nadaptive chunking strategies in RAG systems.\nIn general, our contributions are:  \n- We present a novel, large-scale evaluation\nframework comparing semantic and fixed-size\nchunking across diverse tasks.  \n- We demonstrate that while semantic chunk\ning shows some benefits in certain scenarios,\nthese are inconsistent and often insufficient to\njustify the computational cost."
        },
        "generated_answer": "Hello there! Let's break down the trade-offs between computational efficiency and semantic integrity for these different retrieval and chunking strategies, especially in the context of clinical documents.\n\n### Answer:\n\nUnderstanding the balance between computational efficiency (how fast and resource-intensive a method is) and semantic integrity (how well it captures the true meaning and relationships in the text) is crucial for effective information retrieval. Here's a look at the various strategies:\n\n1.  **Fixed-size Chunking:**\n    *   **Computational Efficiency:** This approach is described as \"computationally simple\" and \"more efficient and reliable\" for practical RAG applications. It cuts documents into chunks of a fixed length, which is straightforward to implement.\n    *   **Semantic Integrity:** Its simplicity comes at a cost. Fixed-size chunking \"can fragment semantically related content across multiple chunks,\" which can lead to \"suboptimal retrieval performance\" because critical information might be split.\n\n2.  **Semantic Chunking:**\n    *   **Computational Efficiency:** While it aims to segment documents based on semantic similarity, the notes state that \"the computational costs associated with semantic chunking are not justified by consistent performance gains.\" It introduces \"additional computational cost\" compared to fixed-size chunking.\n    *   **Semantic Integrity:** It is designed to create \"semantically coherent segments.\" However, its benefits are \"highly context-dependent\" and do not consistently outweigh the computational overhead. It \"occasionally improved performance, particularly on stitched datasets with high topic diversity,\" but not reliably on real-world documents.\n\n3.  **Late Chunking:**\n    *   **Computational Efficiency & Semantic Integrity:** The notes describe late chunking as processing \"the entire document to generate token embeddings first, using boundary cues to create chunk embeddings.\" This contrasts with \"early chunking,\" where the document is divided *before* embedding. While the notes don't explicitly detail the efficiency vs. semantic integrity trade-offs for late chunking, processing the entire document initially might incur higher upfront computational costs but could potentially lead to a more holistic semantic understanding of the document before chunking. However, this is not directly elaborated upon in the provided text.\n\n4.  **Contextual Retrieval (General RAG/Embedding-based Chunk Retrieval):**\n    *   **Computational Efficiency:** Traditional embedding-based chunk retrieval (like RAG) \"prioritizes efficiency with minimal token usage\" (e.g., an average of 544 tokens per query). Methods that use \"large context windows that process entire documents\" for zero-shot inference, while aiming for full context, \"face computational constraints.\"\n    *   **Semantic Integrity:** Early RAG methods \"lacked domain-specific entity modeling.\" Traditional approaches \"often fail to capture critical clinical entity relationships and contextual dependencies essential for accurate medical information extraction\" and may \"miss critical clinical relationships.\" Furthermore, expanding context windows for long documents often leads to the \"'lost in the middle' problem,\" where performance drops if relevant information is not at the beginning or end of the input, indicating a failure to effectively use all the provided context semantically.\n\n5.  **Clinical Entity Augmented Retrieval (CLEAR):**\n    *   **Computational Efficiency:** CLEAR introduced \"substantial efficiency gains (71% token reduction, 72% faster inference time)\" compared to traditional RAG. It achieves \"stronger semantic similarity than wide-context processing at markedly lower token budgets,\" demonstrating an optimal balance.\n    *   **Semantic Integrity:** CLEAR is a \"novel approach that addresses these limitations through entity-aware, entity-centered retrieval strategies,\" meaning it's \"aligned with clinical semantics.\" It showed \"significant performance improvements (F1 score of 0.90 vs 0.86 for traditional RAG)\" by focusing on clinically relevant entities, thereby improving semantic accuracy.\n\n### Superior Scalability in High-Complexity Clinical Documents:\n\nThe **Clinical Entity Augmented Retrieval (CLEAR) methodology** demonstrated superior scalability advantages in high-complexity clinical documents.\n\nThe notes state: \"The 75% win rate on documents exceeding 65,000 tokens validates that entity-aware retrieval becomes increasingly advantageous as document complexity increases, confirming the methodology\u2019s suitability for enterprise healthcare environments.\" This highlights CLEAR's ability to maintain performance and efficiency even with very long and complex clinical narratives, which is a significant advantage in real-world EHR settings.\n\n---\n\n### Key Points/Summary:\n\n*   **Fixed-size chunking** is efficient but can fragment semantic content.\n*   **Semantic chunking** aims for semantic coherence but often has high computational costs not consistently justified by performance gains.\n*   **General RAG/Contextual retrieval** can be efficient in token usage but often misses nuanced semantic relationships and suffers from the \"lost in the middle\" problem in long contexts.\n*   **CLEAR** achieves an optimal balance, providing both significant efficiency gains (reduced tokens, faster inference) and improved semantic accuracy through entity-aware retrieval.\n*   **CLEAR** demonstrates superior scalability, performing robustly and advantageously on highly complex and long clinical documents (over 65,000 tokens).\n\n### Extra Tip:\n\nThink of fixed-size chunking like cutting a book into pages of equal thickness without regard for chapters or paragraphs. It's easy to do, but you might cut a sentence in half! Semantic chunking tries to cut at natural breaks (like chapter endings), which sounds better, but sometimes finding those breaks takes a lot of effort without a clear benefit. CLEAR, on the other hand, is like having an expert reader who knows exactly which *key concepts* in the book are important for your question and only retrieves those specific, relevant sections, saving you time and ensuring you get the most accurate information.",
        "correctness_score": 0.85,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    }
]