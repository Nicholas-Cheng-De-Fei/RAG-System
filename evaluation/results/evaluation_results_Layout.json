[
    [
        [
            0.97,
            1.0
        ],
        [
            0.0,
            0.0
        ],
        [
            0.0,
            0.0
        ],
        [
            0.85,
            1.0
        ],
        [
            0.95,
            1.0
        ],
        [
            0.1,
            1.0
        ],
        [
            0.55,
            1.0
        ]
    ],
    {
        "question_id": 1,
        "document_path": "static/Computational cost of semantic chunking.pdf",
        "question": "What is the defining characteristic of 'semantic chunking' in Retrieval-Augmented Generation (RAG) systems?",
        "ground_truth": "Semantic chunking is a strategy that aims to improve retrieval performance by dividing documents into semantically coherent segments. This approach segments documents based on semantic similarity or detecting semantic distance thresholds between consecutive sentences to maintain coherence",
        "retrieved_context": {
            "context": "# Abstract  \nRecent advances in Retrieval-Augmented Generation (RAG) systems have popularized **se-**\n\n# Abstract  \nRecent advances in Retrieval-Augmented Generation (RAG) systems have popularized **se-**\n\n# mantic chunking, which aims to improve retrieval performance by dividing documents into\nsemantically coherent segments. Despite its\ngrowing adoption, the actual benefits over simpler **fixed-size chunking**, where documents are\nsplit into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrievalbased answer generation. The results show that\nthe computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the\nprevious assumptions about semantic chunking and highlight the need for more efficient\nchunking strategies in RAG systems.\n\n# mantic chunking, which aims to improve retrieval performance by dividing documents into\nsemantically coherent segments. Despite its\ngrowing adoption, the actual benefits over simpler **fixed-size chunking**, where documents are\nsplit into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrievalbased answer generation. The results show that\nthe computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the\nprevious assumptions about semantic chunking and highlight the need for more efficient\nchunking strategies in RAG systems."
        },
        "generated_answer": "Hello there! I can help you understand semantic chunking based on your notes.\n\n**Answer:**\nThe defining characteristic of 'semantic chunking' in Retrieval-Augmented Generation (RAG) systems is its approach to **dividing documents into semantically coherent segments**. This means that instead of just splitting text at arbitrary points, semantic chunking aims to keep related ideas and information together within each segment, making each chunk meaningful on its own.\n\n**Key Points/Summary:**\n*   **Purpose:** To improve retrieval performance in RAG systems.\n*   **Method:** Divides documents into segments that are semantically coherent.\n*   **Goal:** Each segment contains related ideas, maintaining meaning.\n\n**Extra Tip:**\nThink of it like organizing a book: instead of cutting it into pages of equal length (fixed-size chunking), semantic chunking would be like keeping each chapter or even each major paragraph as a complete unit, because they discuss one main idea.",
        "correctness_score": 0.97,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 2,
        "document_path": "static/Computational cost of semantic chunking.pdf",
        "question": "What three primary proxy tasks were designed to indirectly evaluate the quality of chunking strategies in the study presented in the sources?",
        "ground_truth": "The study systematically evaluated the effectiveness of chunking strategies using three proxy tasks: document retrieval, evidence retrieval, and retrieval-based answer generation",
        "retrieved_context": {
            "context": "# Lack of Chunk Quality Measures As noted in\nSection 4, while the output chunks differed between methods, retrieval and generation performances were similar across chunkers. In addition  \nto the influence of embedding models, the absence\nof direct chunk quality metrics likely contributed\nto this issue. Having ground-truth query-chunk relevance scores would provide more accurate evaluations than relying solely on document or evidence\nmapping.\n\n# Lack of Chunk Quality Measures As noted in\nSection 4, while the output chunks differed between methods, retrieval and generation performances were similar across chunkers. In addition  \nto the influence of embedding models, the absence\nof direct chunk quality metrics likely contributed\nto this issue. Having ground-truth query-chunk relevance scores would provide more accurate evaluations than relying solely on document or evidence\nmapping.\n\n# 4 **Results**  \n## 4.1 **Measuring and reporting performances**  \nAs mentioned earlier, we used three proxy tasks\nthe study chunking. We cannot directly assess the\nquality of retrieval at the chunk level due to the lack\nof ground-truth at the chunk level. Instead, each\nretrieved chunk is mapped back to either the source\ndocument or the included evidence sentences.  \nSince the number of relevant documents or evi\ndence sentences is not fixed (unlike the _k_ value for\nretrieved chunks), traditional metrics such as Recall@k and NDCG@k are not suitable. F1 provides\na balanced measure that accounts for both precision\nand recall under these circumstances. Therefore,\nwe use **F1@5** as the metric. For further details, see\nAppendix D.\nFor each dataset, results are reported based on\nthe best hyperparameter configuration for each\nchunker, determined by the average F1 score across\nall _k_ values. All results to be reported below are\nobtained using dunzhang/stella_en_1.5B_v5 as the\nembedder for being the best among those tested.\nIn the following subsections, **Bold** values indicate the best performance on the respective dataset.\nThe results for Answer Generation closely matched\nthose of Evidence Retrieval and are discussed in  \nAppendix E.1. Additional analysis of hyperparameters is provided in Appendix B. Inspection of the\noutputs of different chunkers is provided in Appendix E.4.\n\n# 4 **Results**  \n## 4.1 **Measuring and reporting performances**  \nAs mentioned earlier, we used three proxy tasks\nthe study chunking. We cannot directly assess the\nquality of retrieval at the chunk level due to the lack\nof ground-truth at the chunk level. Instead, each\nretrieved chunk is mapped back to either the source\ndocument or the included evidence sentences.  \nSince the number of relevant documents or evi\ndence sentences is not fixed (unlike the _k_ value for\nretrieved chunks), traditional metrics such as Recall@k and NDCG@k are not suitable. F1 provides\na balanced measure that accounts for both precision\nand recall under these circumstances. Therefore,\nwe use **F1@5** as the metric. For further details, see\nAppendix D.\nFor each dataset, results are reported based on\nthe best hyperparameter configuration for each\nchunker, determined by the average F1 score across\nall _k_ values. All results to be reported below are\nobtained using dunzhang/stella_en_1.5B_v5 as the\nembedder for being the best among those tested.\nIn the following subsections, **Bold** values indicate the best performance on the respective dataset.\nThe results for Answer Generation closely matched\nthose of Evidence Retrieval and are discussed in  \nAppendix E.1. Additional analysis of hyperparameters is provided in Appendix B. Inspection of the\noutputs of different chunkers is provided in Appendix E.4."
        },
        "generated_answer": "It's a great question! Based on the provided notes, it is mentioned that \"we used **three proxy tasks** the study chunking.\"\n\nHowever, the notes *do not specify what those three primary proxy tasks are*. They only state that three were used to study chunking.\n\n**Key Points/Summary:**\n*   The study utilized three proxy tasks.\n*   These tasks were designed to evaluate chunking strategies.\n*   The specific details or names of these three tasks are not provided in the given notes.\n\n**Extra Tip:** When information isn't directly available, sometimes checking appendices or other sections (like \"Appendix D\" or \"Appendix E.1\" mentioned in the notes) might reveal more details!",
        "correctness_score": 0.0,
        "faithfulness_score": 0.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 3,
        "document_path": "static/Reconstructing Context.pdf",
        "question": "What are two advanced chunking techniques, besides traditional early chunking, aimed at preserving global context within RAG systems?",
        "ground_truth": "Two advanced techniques introduced to preserve global context and mitigate context fragmentation are late chunking and contextual retrieval. Late chunking involves embedding the entire document first before segmentation to retain global context, potentially leading to superior results across various retrieval tasks.",
        "retrieved_context": {
            "context": "# 3 **Methodology**  \nTo guide our study, we define the following research questions (RQs), aimed at\nevaluating different strategies for chunking and retrieval in RAG systems:\n\n# 3 **Methodology**  \nTo guide our study, we define the following research questions (RQs), aimed at\nevaluating different strategies for chunking and retrieval in RAG systems:\n\n# ditional early chunking strategies, utilizing **different text segmenters**\nand embedding models to evaluate their impact on retrieval accuracy and\ndownstream performance in RAG systems.\n\n# ditional early chunking strategies, utilizing **different text segmenters**\nand embedding models to evaluate their impact on retrieval accuracy and\ndownstream performance in RAG systems."
        },
        "generated_answer": "It looks like the provided notes mention that the study evaluates \"different strategies for chunking and retrieval in RAG systems,\" and specifically refers to \"traditional early chunking strategies\" that utilize \"different text segmenters\" and \"embedding models.\"\n\nHowever, the notes **do not describe** any advanced chunking techniques beyond traditional early chunking, nor do they detail strategies specifically aimed at preserving global context within RAG systems.\n\n**Key Points/Summary:**\n*   The notes indicate an evaluation of chunking strategies, including traditional early chunking.\n*   Traditional early chunking involves different text segmenters and embedding models.\n*   No advanced chunking techniques or methods for preserving global context are detailed in the provided text.\n\n**Extra Tip:** When we talk about \"global context\" in RAG systems, it's often about ensuring that the retrieved chunks, even if small, still provide enough surrounding information to make sense and answer the query comprehensively. Techniques like \"sliding window\" or \"hierarchical chunking\" are sometimes explored for this purpose! If you add more notes about these, I'd be happy to explain them.",
        "correctness_score": 0.0,
        "faithfulness_score": 0.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 4,
        "document_path": "static/Accelerating LLM Inference.pdf",
        "question": "In the context of long-context LLMs, what is the key phenomenon called where chunks attended to by tokens within a generated chunk exhibit substantial consistency, which ChunkLLM exploits to enhance inference efficiency?",
        "ground_truth": "This phenomenon is called the Intra-Chunk Attention Consistency (ICAC) pattern. ChunkLLM exploits ICAC by updating chunk selection only when the currently decoded token is identified as a chunk boundary.",
        "retrieved_context": {
            "context": "# ICAC We find a phenomenon during the model inference, as illustrated in Figure 3. The chunks attended to by tokens within a generated chunk exhibit substantial consistency, whereas chunk updates  \nFigure 3: Attention visualization of chunk selection during the inference phase. The sample is derived from the passkey retrieval task.  \n4  \npredominantly occur at chunk boundaries. We name this phenomenon the \u201d **I** ntra- **C** hunk **A** ttention\n\n# ICAC We find a phenomenon during the model inference, as illustrated in Figure 3. The chunks attended to by tokens within a generated chunk exhibit substantial consistency, whereas chunk updates  \nFigure 3: Attention visualization of chunk selection during the inference phase. The sample is derived from the passkey retrieval task.  \n4  \npredominantly occur at chunk boundaries. We name this phenomenon the \u201d **I** ntra- **C** hunk **A** ttention\n\n# tion Answering, which evaluates the model\u2019s capabilities in question understanding and information\nmatching, CommonsenseQA (Talmor et al., 2019) (5-shot), Social IQA (Sap et al., 2019) (15-shot),\nPIQA (Bisk et al., 2020) (25-shot); and **Reasoning** which evaluates the model\u2019s capabilities in logical abstraction and complex decision-making, HellaSwag (Zellers et al., 2019) (10-shot), WinoGrande (Sakaguchi et al., 2020) (25-shot), ARC-c/ARC-e (Clark et al., 2018) (25-shot).  \n5  \nMethods SDQA MDQA Summary Few-shot Avg KV\nNQA Qasper MFQA HQA Musi 2WQA GR QMS SAM TREC\nQwen2.5-7B 33.76 51.40 57.17 56.76 29.40 38.09 31.57 24.40 46.73 72.00 44.13 100.00\nChunkLLM **31.09** **50.52** **56.95** **55.61** **29.41** **38.27** **31.25** **23.16** 46.53 **72.50** **43.53** 48.58\nSepLLM 26.43 50.18 50.29 47.25 22.83 36.34 28.31 21.14 46.71 71.50 40.10 53.17\nStrmLLM 28.88 50.37 50.47 51.60 24.55 37.76 30.36 22.27 **46.84** 72.00 41.51 68.50  \nLlama3.1-8B 40.88 51.72 58.47 52.68 35.80 42.52 30.64 24.44 47.10 73.00 45.73 100.00  \nChunkLLM **39.14** 49.93 53.45 **52.29** **34.40** **42.98** **31.05** **23.89** **47.20** 72.50 **44.68** 50.18\nSepLLM 36.23 **51.16** 51.81 47.70 27.83 40.85 27.12 21.84 46.89 72.00 42.34 54.56\nStrmLLM 35.94 50.96 **54.13** 50.54 30.95 41.83 27.50 23.06 46.23 **73.00** 43.42 69.25  \nTable 1: Experimental results on LongBench. Avg denotes average score. SDQA: single-document\nquestion answering, MDQA: multi-document question answering. The full names of the subtasks\nare shown in Appendix 6.2.  \n3.2 M AIN R ESULTS  \n3.2.1 R ESULTS ON L ONG B ENCH  \nWe set the top-k ratio to 45% and the number of local chunks to 15 for ChunkLLM. The experimental\nresults on the LongBench using Qwen2.5-7B and Llama3.1-8B are presented in Table 1. Here,\n\u201dStrmLLM\u201d represents StreamingLLM (Xiao et al., 2024). We take Qwen2.5-7B as an example for\nanalysis, and the same conclusion holds for Llama3.1-8B. The following observations can be made:\n(1) In terms of overall average performance, ChunkLLM attains the optimal performance when\ncompared to SepLLM and StreamingLLM, with respective improvements of 3.43 and 2.02 (43.53\nv.s. 40.10 v.s. 41.51). In contrast to the short-text benchmark in Subsection 3.2.4, ChunkLLM\ndemonstrates a remarkable improvement in long-context evaluation, which validates the advantage\nof ChunkLLM in retrieving key chunk information during long-context reasoning. (2) Notably,\nin the MDQA task, ChunkLLM yields a substantial improvement over SepLLM. We argue that\nthe core challenge of MDQA lies in the dispersion of critical information across distinct positions\nwithin the context, which places high demands on the model\u2019s context comprehension capability.\nSepLLM leverages separators as chunk features, which is plagued by constrained representational\ncapacity and the problem of chunk semantic incompleteness. By contrast, ChunkLLM enriches\nthe representational capacity of chunks via attention distillation, which enhances the recall rates of\ncritical chunks. This, in turn, effectively boosts the model\u2019s long-context understanding capability.\n(3) ChunkLLM attains 98.64% of the vanilla model\u2019s performance while employing the minimum\nKV cache. Notably, relative to SepLLM and StreamingLLM, ChunkLLM reduces the KV cache\nusage rate by 4.59% and 19.92% (48.58% v.s. 53.17% v.s. 68.50%), respectively, findings that\nfurther substantiate the superiority of ChunkLLM in long-context scenarios.  \n3.2.2 R ESULTS ON NIAH  \nFigure 4: Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length.\nThe last column represents the KV-cache utilization rate.  \n6  \nWe set the top-k to 256 and the number of local chunks to 16 for ChunkLLM. As depicted in\nFigure 4, ChunkLLM outperforms SepLLM across all scenarios in the 64K-context NIAH evaluation conducted on Qwen2.5-7B and Llama3.1-8B, achieving superior performance. Notably, in\nscenarios where the context length exceeds 12K, SepLLM exhibits near-total loss of retrieval capability (visualized in red), whereas ChunkLLM retains performance comparable to the vanilla model.\nThis discrepancy is primarily attributed to ChunkLLM\u2019s attention distillation mechanism, which\nstrengthens the feature representational capacity of chunks. Consequently, during chunk selection,\nthe model effectively identifies critical chunks with higher query relevance, leading to improved inference performance. Additionally, ChunkLLM exhibits a reduced KV-Cache utilization rate relative\nto SepLLM, which further corroborates the effectiveness of key chunk retrieval. We also conduct\nexperiments with StreamingLLM, as shown in Appendix 6.3.  \n3.2.3 I NFERENCE E FFICIENCY  \n|1600<br>1400<br>1200 (s)<br>1000 time<br>800 Cost<br>600<br>400<br>200|Qwen2.5-7B<br>Ours|Col3|2300<br>2000<br>1700<br>1400<br>1100<br>800<br>500<br>200|Llama3.1-8B<br>Ours|Col6|\n|---|---|---|---|---|---|\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)|~~3.84x~~|~~3.84x~~|~~3.84x~~|~~4.~~|~~4.~~|\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||  \nFigure 5: Comparison of inference time per 10k tokens in the generation process on PG19 test set.  \nWe conduct runtime evaluations of Vanilla and ChunkLLM for 120K-token generation tasks on\nthe PG19(Rae et al., 2019) test set, with metrics recorded every 10K tokens. We set the top-k to\n256 and the number of local chunks to 16 for ChunkLLM. As shown in Figure 5, as the number\nof generated tokens increases, Vanilla\u2019s inference time rises linearly, while ChunkLLM maintains\npersistent stability in time consumption. In the 110K\u2013120K token generation phase, ChunkLLM\noutperforms Vanilla by speedups of 3.84\u00d7 and 4.48\u00d7, which corroborates the efficacy of the proposed\nICAC mechanism. During ChunkLLM\u2019s inference phase, chunk updates occur exclusively at chunk\nboundaries, minimizing the updates frequency and thereby boosting inference efficiency. We also\nconduct supplementary experiments using the FineWeb-Edu dataset, from which 1000 test corpora\nof 4k length are sampled. For the task of chunk boundary prediction, we evaluate its performance\nusing three key metrics: precision, recall, and F1-score. The calculated results are 98.31, 95.54,\nand 96.91, respectively. Such promising performance indicators serve to verify the reliability and\neffectiveness of our chunk boundary prediction task.  \nWe conduct experiments where these models Length Methods PPL Total Time(s)\ngenerated 120K tokens, evaluating both total Qwen2.5-7B 14.41 10,684.31\ninference time and average perplexity (ppl) on 120K ChunkLLM 16.23 4,782.62\nthe PG19 test set, and results are summarizedin Table 2. Compared to the vanilla model, Llama3.1-8BChunkLLM 11.9312.89 145,,906.19963.83\nChunkLLM yields a slight enhancement in ppl Table 2: The perplexity and running time comparalongside a significant decrease in total infer- ison on the PG19 test set.\nence time. The underlying reason is that while\nthe vanilla model maintains semantic integrity, it incurs linearly increasing inference time as generated token count rises. Conversely, ChunkLLM reduces computational burden and speeds up\ninference by leveraging its chunk selection and ICAC mechanisms.  \nLength Methods PPL Total Time(s)  \n120K  \nQwen2.5-7B 14.41 10,684.31\nChunkLLM 16.23 4,782.62\nLlama3.1-8B 11.93 14,906.19\nChunkLLM 12.89 5,963.83  \nTable 2: The perplexity and running time comparison on the PG19 test set.  \n3.2.4 R ESULTS ON S HORT T EXT  \nThe experimental results for short texts are presented in Table 3. The following conclusions can\nbe drawn: (1) The overall average metrics of ChunkLLM on Qwen2.5-7B and Llama3.1-8B both\noutperform those of StreamingLLM and SepLLM, achieving 99.57% and 99.84% of the Vanilla\nmodel\u2019s performance, respectively. Notably, ChunkLLM attains optimal performance across 8 out of  \n7  \nMethods General Knowledge Question Answering Reasoning Avg KV\nMMLU SciQ OQA CQA SIQA PIQA Heag WG ARC-c ARC-e\nQwen2.5-7B 74.25 97.00 52.80 84.52 58.44 81.72 80.24 77.27 63.82 87.21 75.73 100.00\nChunkLLM 72.51 96.60 **52.40** **84.68** **58.34** **81.77** 80.08 **76.87** **63.65** **87.21** **75.41** 45.47\nSepLLM 73.07 **96.70** 52.20 84.19 58.25 81.41 **80.10** 76.48 62.94 86.11 75.15 50.20\nStrmLLM **73.31** 96.60 52.00 84.28 58.19 81.39 79.76 76.64 62.79 86.36 75.13 45.14  \nLlama3.1-8B 65.30 97.60 48.00 74.28 54.04 83.19 81.76 80.03 57.85 84.55 72.66 100.00  \nChunkLLM **64.78** 97.30 **48.40** **74.45** **54.76** 82.75 **81.84** **79.01** 57.68 **84.55** **72.55** 45.04\nSepLLM 64.32 **97.40** 47.40 74.10 54.25 **83.03** 81.68 79.01 **57.93** 84.09 72.32 50.32\nStrmLLM 61.19 97.20 48.00 73.79 53.94 81.56 80.14 78.22 56.91 83.59 72.45 45.30  \nTable 3: Experimental results on short context benchmarks.  \nthe 10 evaluation tasks, validating its efficacy in short-text task scenarios. (2) We perform statistical\nanalyses on the average utilization rate of the KV cache. In comparison with SepLLM, ChunkLLM\nachieves superior performance while consuming a lower volume of KV cache (45.47% v.s. 50.20%).\nSpecifically, on the Llama3.1-8B model, ChunkLLM not only exhibits the minimal KV cache usage\nbut also outperforms both SepLLM and StreamingLLM in terms of performance metrics. This\nfinding further validates the precision of ChunkLLM in chunk recall, achieving a balanced trade-off\nbetween performance and memory consumption.  \n3.3 A BLATION S TUDY  \n3.3.1 E FFECTIVENESS OF V OTE AND ICAC  \nMethods SDQA MDQA Summary Few-shot Avg\nNQA Qasper MFQA HQA Musi 2WQA GR QMS SAM TREC\nQwen2.5-7B 33.76 51.40 57.17 56.76 29.40 38.09 31.57 24.40 46.73 72.00 44.13\nChunkLLM 31.09 50.52 **56.95** **55.61** **29.41** 38.27 31.25 23.16 46.53 **72.50** 43.53  \nw/o vote 30.78 45.00 51.53 54.96 28.00 38.17 30.82 23.49 46.58 70.00 41.93  \nw/o ICAC **31.36** **50.94** 56.84 55.61 28.50 **38.59** **31.86** **23.93** **46.66** 72.50 **43.68**  \nTable 4: Ablation Study on LongBench, w/o vote: remove vote mechanism, w/o ICAC: remove\nICAC pattern.  \nWe validate the proposed vote mechanism and\nICAC pattern based on the Qwen2.5-7B using the LongBench, with experimental results\nshown in Table 4. Removal of the vote mechanism leads to a 1.6 drop in overall performance\n(43.53 v.s. 41.93), which confirms the mechanism\u2019s efficacy, as it integrates inter-layer differences in chunk selection and minimizes interference arising from such discrepancies. We Figure 6: Inference efficiency of ICAC on PG19\nalso conduct a visual investigation into the re- test set.\ncall performance of top-k chunks across different layers, with comprehensive experimental results provided in Appendix 6.5. Conversely, removing ICAC results in a marginal 0.15 improvement in overall performance. This slight gain is\nattributed to the increased frequency of chunk selection updates during the inference phase. Frequent chunk selection, however, poses a limitation of low inference efficiency. As shown in Figure\n6, after ICAC is removed, the inference latency is 2.12 times higher than that of ChunkLLM in the\n110K\u2013120K token generation stage. Conversely, incorporating ICAC enables the model to maintain near-lossless performance alongside improved inference efficiency, which provides additional\nvalidation of ICAC\u2019s success. Appendix 6.4 shows a case study of the ICAC.  \nFigure 6: Inference efficiency of ICAC on PG19  \ntest set.  \n3.3.2 A NALYSIS OF F IXED C HUNKS AND S EMANTIC C HUNKS  \nWe conduct an experimental analysis of the fixed chunk method (FixLLM) on the NIAH task. To\nensure consistent KV cache utilization and facilitate a fair comparison, FixLLM is configured with\na top-k of 384 and a local chunks of 24, while ChunkLLM is set to a top-k of 256 and a local\nchunks of 16. The experimental results are illustrated in Figure 7. As observed, under conditions  \n8  \nFigure 7: Visualization of fixed chunks and semantic chunks in NIAH test.  \nof approximately consistent KV cache utilization, FixLLM exhibits an 8.23 reduction (87.78 v.s.\n79.55) in accuracy relative to ChunkLLM on the 64K NIAH task. This discrepancy stems from\nthe semantic incompleteness of fixed chunks, which in turn compromises chunk selection during\nthe inference phase. In contrast, ChunkLLM leverages contextual semantic information to identify\nchunk boundaries, preserving the semantic integrity of chunks.  \n4 R ELATED W ORK\n\n# tion Answering, which evaluates the model\u2019s capabilities in question understanding and information\nmatching, CommonsenseQA (Talmor et al., 2019) (5-shot), Social IQA (Sap et al., 2019) (15-shot),\nPIQA (Bisk et al., 2020) (25-shot); and **Reasoning** which evaluates the model\u2019s capabilities in logical abstraction and complex decision-making, HellaSwag (Zellers et al., 2019) (10-shot), WinoGrande (Sakaguchi et al., 2020) (25-shot), ARC-c/ARC-e (Clark et al., 2018) (25-shot).  \n5  \nMethods SDQA MDQA Summary Few-shot Avg KV\nNQA Qasper MFQA HQA Musi 2WQA GR QMS SAM TREC\nQwen2.5-7B 33.76 51.40 57.17 56.76 29.40 38.09 31.57 24.40 46.73 72.00 44.13 100.00\nChunkLLM **31.09** **50.52** **56.95** **55.61** **29.41** **38.27** **31.25** **23.16** 46.53 **72.50** **43.53** 48.58\nSepLLM 26.43 50.18 50.29 47.25 22.83 36.34 28.31 21.14 46.71 71.50 40.10 53.17\nStrmLLM 28.88 50.37 50.47 51.60 24.55 37.76 30.36 22.27 **46.84** 72.00 41.51 68.50  \nLlama3.1-8B 40.88 51.72 58.47 52.68 35.80 42.52 30.64 24.44 47.10 73.00 45.73 100.00  \nChunkLLM **39.14** 49.93 53.45 **52.29** **34.40** **42.98** **31.05** **23.89** **47.20** 72.50 **44.68** 50.18\nSepLLM 36.23 **51.16** 51.81 47.70 27.83 40.85 27.12 21.84 46.89 72.00 42.34 54.56\nStrmLLM 35.94 50.96 **54.13** 50.54 30.95 41.83 27.50 23.06 46.23 **73.00** 43.42 69.25  \nTable 1: Experimental results on LongBench. Avg denotes average score. SDQA: single-document\nquestion answering, MDQA: multi-document question answering. The full names of the subtasks\nare shown in Appendix 6.2.  \n3.2 M AIN R ESULTS  \n3.2.1 R ESULTS ON L ONG B ENCH  \nWe set the top-k ratio to 45% and the number of local chunks to 15 for ChunkLLM. The experimental\nresults on the LongBench using Qwen2.5-7B and Llama3.1-8B are presented in Table 1. Here,\n\u201dStrmLLM\u201d represents StreamingLLM (Xiao et al., 2024). We take Qwen2.5-7B as an example for\nanalysis, and the same conclusion holds for Llama3.1-8B. The following observations can be made:\n(1) In terms of overall average performance, ChunkLLM attains the optimal performance when\ncompared to SepLLM and StreamingLLM, with respective improvements of 3.43 and 2.02 (43.53\nv.s. 40.10 v.s. 41.51). In contrast to the short-text benchmark in Subsection 3.2.4, ChunkLLM\ndemonstrates a remarkable improvement in long-context evaluation, which validates the advantage\nof ChunkLLM in retrieving key chunk information during long-context reasoning. (2) Notably,\nin the MDQA task, ChunkLLM yields a substantial improvement over SepLLM. We argue that\nthe core challenge of MDQA lies in the dispersion of critical information across distinct positions\nwithin the context, which places high demands on the model\u2019s context comprehension capability.\nSepLLM leverages separators as chunk features, which is plagued by constrained representational\ncapacity and the problem of chunk semantic incompleteness. By contrast, ChunkLLM enriches\nthe representational capacity of chunks via attention distillation, which enhances the recall rates of\ncritical chunks. This, in turn, effectively boosts the model\u2019s long-context understanding capability.\n(3) ChunkLLM attains 98.64% of the vanilla model\u2019s performance while employing the minimum\nKV cache. Notably, relative to SepLLM and StreamingLLM, ChunkLLM reduces the KV cache\nusage rate by 4.59% and 19.92% (48.58% v.s. 53.17% v.s. 68.50%), respectively, findings that\nfurther substantiate the superiority of ChunkLLM in long-context scenarios.  \n3.2.2 R ESULTS ON NIAH  \nFigure 4: Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length.\nThe last column represents the KV-cache utilization rate.  \n6  \nWe set the top-k to 256 and the number of local chunks to 16 for ChunkLLM. As depicted in\nFigure 4, ChunkLLM outperforms SepLLM across all scenarios in the 64K-context NIAH evaluation conducted on Qwen2.5-7B and Llama3.1-8B, achieving superior performance. Notably, in\nscenarios where the context length exceeds 12K, SepLLM exhibits near-total loss of retrieval capability (visualized in red), whereas ChunkLLM retains performance comparable to the vanilla model.\nThis discrepancy is primarily attributed to ChunkLLM\u2019s attention distillation mechanism, which\nstrengthens the feature representational capacity of chunks. Consequently, during chunk selection,\nthe model effectively identifies critical chunks with higher query relevance, leading to improved inference performance. Additionally, ChunkLLM exhibits a reduced KV-Cache utilization rate relative\nto SepLLM, which further corroborates the effectiveness of key chunk retrieval. We also conduct\nexperiments with StreamingLLM, as shown in Appendix 6.3.  \n3.2.3 I NFERENCE E FFICIENCY  \n|1600<br>1400<br>1200 (s)<br>1000 time<br>800 Cost<br>600<br>400<br>200|Qwen2.5-7B<br>Ours|Col3|2300<br>2000<br>1700<br>1400<br>1100<br>800<br>500<br>200|Llama3.1-8B<br>Ours|Col6|\n|---|---|---|---|---|---|\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)|~~3.84x~~|~~3.84x~~|~~3.84x~~|~~4.~~|~~4.~~|\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||\n|200<br>400<br>600<br>800<br>1000<br>1200<br>1400<br>1600<br>Cost time (s)||||||  \nFigure 5: Comparison of inference time per 10k tokens in the generation process on PG19 test set.  \nWe conduct runtime evaluations of Vanilla and ChunkLLM for 120K-token generation tasks on\nthe PG19(Rae et al., 2019) test set, with metrics recorded every 10K tokens. We set the top-k to\n256 and the number of local chunks to 16 for ChunkLLM. As shown in Figure 5, as the number\nof generated tokens increases, Vanilla\u2019s inference time rises linearly, while ChunkLLM maintains\npersistent stability in time consumption. In the 110K\u2013120K token generation phase, ChunkLLM\noutperforms Vanilla by speedups of 3.84\u00d7 and 4.48\u00d7, which corroborates the efficacy of the proposed\nICAC mechanism. During ChunkLLM\u2019s inference phase, chunk updates occur exclusively at chunk\nboundaries, minimizing the updates frequency and thereby boosting inference efficiency. We also\nconduct supplementary experiments using the FineWeb-Edu dataset, from which 1000 test corpora\nof 4k length are sampled. For the task of chunk boundary prediction, we evaluate its performance\nusing three key metrics: precision, recall, and F1-score. The calculated results are 98.31, 95.54,\nand 96.91, respectively. Such promising performance indicators serve to verify the reliability and\neffectiveness of our chunk boundary prediction task.  \nWe conduct experiments where these models Length Methods PPL Total Time(s)\ngenerated 120K tokens, evaluating both total Qwen2.5-7B 14.41 10,684.31\ninference time and average perplexity (ppl) on 120K ChunkLLM 16.23 4,782.62\nthe PG19 test set, and results are summarizedin Table 2. Compared to the vanilla model, Llama3.1-8BChunkLLM 11.9312.89 145,,906.19963.83\nChunkLLM yields a slight enhancement in ppl Table 2: The perplexity and running time comparalongside a significant decrease in total infer- ison on the PG19 test set.\nence time. The underlying reason is that while\nthe vanilla model maintains semantic integrity, it incurs linearly increasing inference time as generated token count rises. Conversely, ChunkLLM reduces computational burden and speeds up\ninference by leveraging its chunk selection and ICAC mechanisms.  \nLength Methods PPL Total Time(s)  \n120K  \nQwen2.5-7B 14.41 10,684.31\nChunkLLM 16.23 4,782.62\nLlama3.1-8B 11.93 14,906.19\nChunkLLM 12.89 5,963.83  \nTable 2: The perplexity and running time comparison on the PG19 test set.  \n3.2.4 R ESULTS ON S HORT T EXT  \nThe experimental results for short texts are presented in Table 3. The following conclusions can\nbe drawn: (1) The overall average metrics of ChunkLLM on Qwen2.5-7B and Llama3.1-8B both\noutperform those of StreamingLLM and SepLLM, achieving 99.57% and 99.84% of the Vanilla\nmodel\u2019s performance, respectively. Notably, ChunkLLM attains optimal performance across 8 out of  \n7  \nMethods General Knowledge Question Answering Reasoning Avg KV\nMMLU SciQ OQA CQA SIQA PIQA Heag WG ARC-c ARC-e\nQwen2.5-7B 74.25 97.00 52.80 84.52 58.44 81.72 80.24 77.27 63.82 87.21 75.73 100.00\nChunkLLM 72.51 96.60 **52.40** **84.68** **58.34** **81.77** 80.08 **76.87** **63.65** **87.21** **75.41** 45.47\nSepLLM 73.07 **96.70** 52.20 84.19 58.25 81.41 **80.10** 76.48 62.94 86.11 75.15 50.20\nStrmLLM **73.31** 96.60 52.00 84.28 58.19 81.39 79.76 76.64 62.79 86.36 75.13 45.14  \nLlama3.1-8B 65.30 97.60 48.00 74.28 54.04 83.19 81.76 80.03 57.85 84.55 72.66 100.00  \nChunkLLM **64.78** 97.30 **48.40** **74.45** **54.76** 82.75 **81.84** **79.01** 57.68 **84.55** **72.55** 45.04\nSepLLM 64.32 **97.40** 47.40 74.10 54.25 **83.03** 81.68 79.01 **57.93** 84.09 72.32 50.32\nStrmLLM 61.19 97.20 48.00 73.79 53.94 81.56 80.14 78.22 56.91 83.59 72.45 45.30  \nTable 3: Experimental results on short context benchmarks.  \nthe 10 evaluation tasks, validating its efficacy in short-text task scenarios. (2) We perform statistical\nanalyses on the average utilization rate of the KV cache. In comparison with SepLLM, ChunkLLM\nachieves superior performance while consuming a lower volume of KV cache (45.47% v.s. 50.20%).\nSpecifically, on the Llama3.1-8B model, ChunkLLM not only exhibits the minimal KV cache usage\nbut also outperforms both SepLLM and StreamingLLM in terms of performance metrics. This\nfinding further validates the precision of ChunkLLM in chunk recall, achieving a balanced trade-off\nbetween performance and memory consumption.  \n3.3 A BLATION S TUDY  \n3.3.1 E FFECTIVENESS OF V OTE AND ICAC  \nMethods SDQA MDQA Summary Few-shot Avg\nNQA Qasper MFQA HQA Musi 2WQA GR QMS SAM TREC\nQwen2.5-7B 33.76 51.40 57.17 56.76 29.40 38.09 31.57 24.40 46.73 72.00 44.13\nChunkLLM 31.09 50.52 **56.95** **55.61** **29.41** 38.27 31.25 23.16 46.53 **72.50** 43.53  \nw/o vote 30.78 45.00 51.53 54.96 28.00 38.17 30.82 23.49 46.58 70.00 41.93  \nw/o ICAC **31.36** **50.94** 56.84 55.61 28.50 **38.59** **31.86** **23.93** **46.66** 72.50 **43.68**  \nTable 4: Ablation Study on LongBench, w/o vote: remove vote mechanism, w/o ICAC: remove\nICAC pattern.  \nWe validate the proposed vote mechanism and\nICAC pattern based on the Qwen2.5-7B using the LongBench, with experimental results\nshown in Table 4. Removal of the vote mechanism leads to a 1.6 drop in overall performance\n(43.53 v.s. 41.93), which confirms the mechanism\u2019s efficacy, as it integrates inter-layer differences in chunk selection and minimizes interference arising from such discrepancies. We Figure 6: Inference efficiency of ICAC on PG19\nalso conduct a visual investigation into the re- test set.\ncall performance of top-k chunks across different layers, with comprehensive experimental results provided in Appendix 6.5. Conversely, removing ICAC results in a marginal 0.15 improvement in overall performance. This slight gain is\nattributed to the increased frequency of chunk selection updates during the inference phase. Frequent chunk selection, however, poses a limitation of low inference efficiency. As shown in Figure\n6, after ICAC is removed, the inference latency is 2.12 times higher than that of ChunkLLM in the\n110K\u2013120K token generation stage. Conversely, incorporating ICAC enables the model to maintain near-lossless performance alongside improved inference efficiency, which provides additional\nvalidation of ICAC\u2019s success. Appendix 6.4 shows a case study of the ICAC.  \nFigure 6: Inference efficiency of ICAC on PG19  \ntest set.  \n3.3.2 A NALYSIS OF F IXED C HUNKS AND S EMANTIC C HUNKS  \nWe conduct an experimental analysis of the fixed chunk method (FixLLM) on the NIAH task. To\nensure consistent KV cache utilization and facilitate a fair comparison, FixLLM is configured with\na top-k of 384 and a local chunks of 24, while ChunkLLM is set to a top-k of 256 and a local\nchunks of 16. The experimental results are illustrated in Figure 7. As observed, under conditions  \n8  \nFigure 7: Visualization of fixed chunks and semantic chunks in NIAH test.  \nof approximately consistent KV cache utilization, FixLLM exhibits an 8.23 reduction (87.78 v.s.\n79.55) in accuracy relative to ChunkLLM on the 64K NIAH task. This discrepancy stems from\nthe semantic incompleteness of fixed chunks, which in turn compromises chunk selection during\nthe inference phase. In contrast, ChunkLLM leverages contextual semantic information to identify\nchunk boundaries, preserving the semantic integrity of chunks.  \n4 R ELATED W ORK"
        },
        "generated_answer": "That's a great question about how long-context Large Language Models (LLMs) manage information!\n\n**Answer:**\nIn the context of long-context LLMs, the key phenomenon where chunks attended to by tokens within a generated chunk exhibit substantial consistency, and which ChunkLLM exploits to enhance inference efficiency, is called **Intra-Chunk Attention (ICAC)**. This phenomenon also notes that chunk updates predominantly occur at chunk boundaries.\n\n**Key Points/Summary:**\n*   **Intra-Chunk Attention (ICAC):** This is the name given to the observed phenomenon.\n*   **Consistency:** Tokens within a generated chunk consistently attend to the same set of chunks.\n*   **Chunk Updates:** Changes to which chunks are being attended to primarily happen at the boundaries between chunks, rather than constantly throughout a chunk.\n*   **Inference Efficiency:** ChunkLLM leverages this ICAC phenomenon to reduce the frequency of chunk selection updates during inference, which significantly boosts its efficiency.\n\n**Extra Tip:**\nImagine reading a book. When you're in the middle of a paragraph, your focus likely stays within that paragraph's ideas. You only really shift your \"attention\" to new major ideas when you reach the end of a paragraph or section. ICAC works similarly for LLMs, allowing them to maintain focus within a \"chunk\" of text and only update their attention more broadly when they hit a natural \"chunk boundary,\" saving computational effort!",
        "correctness_score": 0.85,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 5,
        "document_path": "static/Beyond Long Context.pdf",
        "question": "In the clinical domain, what is the methodology that uses entity-aware retrieval strategies to achieve improved semantic accuracy and computational efficiency when processing Electronic Health Record (EHR) notes?",
        "ground_truth": "The methodology is Clinical Entity Augmented Retrieval (CLEAR). CLEAR addresses the limitations of traditional chunk-based RAG by employing entity-aware, entity-centered retrieval strategies and demonstrated a 78% reduction in token usage compared to wide-context processing in evaluations",
        "retrieved_context": {
            "context": "## 1.1 **Related Work**  \nEntity-aware retrieval has gained increasing attention within biomedical NLP and question-answering  \ndomains. Early retrieval-augmented methods such as RAG [2] demonstrated the potential of embedding  \nbased chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches  \nleveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al.,  \n2016) provided partial improvements but often failed to maintain contextual continuity across long clin  \nical narratives.  \nCLEAR [1] represented a significant advancement by introducing entity-centered retrieval aligned  \nwith clinical semantics. Our work extends this line of research by operationalizing CLEAR within an  \nend-to-end evaluation platform, providing reproducible empirical validation across realistic EHR-scale  \ndocument sets.  \nRecent evaluations of retrieval-augmented models in long-context reasoning (e.g., Karpinska et al.,  \n2023; Xiong et al., 2024) emphasize that retrieval strategies often outperform naive long-context prompt  \ning, supporting the need for entity-aware retrieval. Our work contributes a reproducible evaluation  \nframework within this paradigm, focusing on realistic EHR-scale clinical notes.  \n3\n\n## 1.1 **Related Work**  \nEntity-aware retrieval has gained increasing attention within biomedical NLP and question-answering  \ndomains. Early retrieval-augmented methods such as RAG [2] demonstrated the potential of embedding  \nbased chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches  \nleveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al.,  \n2016) provided partial improvements but often failed to maintain contextual continuity across long clin  \nical narratives.  \nCLEAR [1] represented a significant advancement by introducing entity-centered retrieval aligned  \nwith clinical semantics. Our work extends this line of research by operationalizing CLEAR within an  \nend-to-end evaluation platform, providing reproducible empirical validation across realistic EHR-scale  \ndocument sets.  \nRecent evaluations of retrieval-augmented models in long-context reasoning (e.g., Karpinska et al.,  \n2023; Xiong et al., 2024) emphasize that retrieval strategies often outperform naive long-context prompt  \ning, supporting the need for entity-aware retrieval. Our work contributes a reproducible evaluation  \nframework within this paradigm, focusing on realistic EHR-scale clinical notes.  \n3\n\n# 1. **INTRODUCTION**  \nElectronic Health Record (EHR) systems in modern healthcare infrastructure store clinical documenta  \ntion within FHIR DocumentReference resources, typically encoded as base64 attachments containing  \nunstructured clinical notes. These documents, ranging from brief progress notes to comprehensive dis  \ncharge summaries, present substantial challenges for automated question-answering systems that require  \nsemantically accurate information extraction rather than statistical correlation-based retrieval common  \nin traditional vector database approaches.  \nContemporary approaches to clinical document processing have largely focused on two paradigms:  \nzero-shot inference with large context windows that process entire documents but face computational  \n2  \nconstraints and the \u201dlost in the middle\u201d problem, and chunk-based retrieval-augmented generation (RAG)  \nsystems that utilize vector databases for semantic similarity search but often fail to capture critical clin  \nical entity relationships and contextual dependencies essential for accurate medical information extrac  \ntion.  \nA growing body of evidence shows that merely expanding context windows does not guarantee  \neffective use of information: performance often drops when relevant spans occur in the middle of long  \ninputs (\u201clost in the middle\u201d) [3]. This motivates entity-aware retrieval that selectively centers clinically  \nrelevant spans rather than relying on statistically similar but potentially off-target chunks.  \nThe CLinical Entity Augmented Retrieval (CLEAR) methodology, published by Lopez et al. in  \n2025 [1], introduced a novel approach that addresses these limitations through entity-aware, entity  \ncentered retrieval strategies. The original study demonstrated significant performance improvements  \n(F1 score of 0.90 vs 0.86 for traditional RAG) with substantial efficiency gains (71% token reduction,  \n72% faster inference time) on clinical information extraction tasks, positioning CLEAR as a potentially  \ntransformative approach for production EHR processing systems.  \nTo validate these claims in realistic healthcare scenarios and provide a robust evaluation framework  \nfor clinical NLP approaches, we developed the Clinical Notes Q&A Evaluation Platform. This com  \nprehensive validation study implements and compares three fundamental approaches: (1) wide context  \nprocessing for zero-shot inference with large language models, (2) traditional vector database chunking  \nwith embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu  \nmentReference processing. Our contributions include: systematic validation of CLEAR\u2019s performance  \nclaims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis  \nof retrieval strategy performance across clinical documents of varying complexity and length.\n\n# 1. **INTRODUCTION**  \nElectronic Health Record (EHR) systems in modern healthcare infrastructure store clinical documenta  \ntion within FHIR DocumentReference resources, typically encoded as base64 attachments containing  \nunstructured clinical notes. These documents, ranging from brief progress notes to comprehensive dis  \ncharge summaries, present substantial challenges for automated question-answering systems that require  \nsemantically accurate information extraction rather than statistical correlation-based retrieval common  \nin traditional vector database approaches.  \nContemporary approaches to clinical document processing have largely focused on two paradigms:  \nzero-shot inference with large context windows that process entire documents but face computational  \n2  \nconstraints and the \u201dlost in the middle\u201d problem, and chunk-based retrieval-augmented generation (RAG)  \nsystems that utilize vector databases for semantic similarity search but often fail to capture critical clin  \nical entity relationships and contextual dependencies essential for accurate medical information extrac  \ntion.  \nA growing body of evidence shows that merely expanding context windows does not guarantee  \neffective use of information: performance often drops when relevant spans occur in the middle of long  \ninputs (\u201clost in the middle\u201d) [3]. This motivates entity-aware retrieval that selectively centers clinically  \nrelevant spans rather than relying on statistically similar but potentially off-target chunks.  \nThe CLinical Entity Augmented Retrieval (CLEAR) methodology, published by Lopez et al. in  \n2025 [1], introduced a novel approach that addresses these limitations through entity-aware, entity  \ncentered retrieval strategies. The original study demonstrated significant performance improvements  \n(F1 score of 0.90 vs 0.86 for traditional RAG) with substantial efficiency gains (71% token reduction,  \n72% faster inference time) on clinical information extraction tasks, positioning CLEAR as a potentially  \ntransformative approach for production EHR processing systems.  \nTo validate these claims in realistic healthcare scenarios and provide a robust evaluation framework  \nfor clinical NLP approaches, we developed the Clinical Notes Q&A Evaluation Platform. This com  \nprehensive validation study implements and compares three fundamental approaches: (1) wide context  \nprocessing for zero-shot inference with large language models, (2) traditional vector database chunking  \nwith embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu  \nmentReference processing. Our contributions include: systematic validation of CLEAR\u2019s performance  \nclaims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis  \nof retrieval strategy performance across clinical documents of varying complexity and length."
        },
        "generated_answer": "Hello there! I can certainly help you understand that.\n\nIn the clinical domain, the methodology that uses entity-aware retrieval strategies to achieve improved semantic accuracy and computational efficiency when processing Electronic Health Record (EHR) notes is called **CLinical Entity Augmented Retrieval (CLEAR)**.\n\nThis approach was introduced by Lopez et al. in 2025 and represents a significant advancement. It addresses the limitations of earlier methods by focusing on \"entity-aware, entity-centered retrieval strategies\" that are aligned with clinical semantics.\n\n**Answer:**\nThe methodology in the clinical domain that uses entity-aware retrieval strategies to achieve improved semantic accuracy and computational efficiency when processing Electronic Health Record (EHR) notes is the **CLinical Entity Augmented Retrieval (CLEAR) methodology**.\n\nCLEAR introduced a novel approach that specifically focuses on entity-aware and entity-centered retrieval strategies, which are designed to handle the complexities and specific semantic requirements of clinical documentation. Unlike traditional vector database approaches that might rely on statistical correlations, CLEAR aims for semantically accurate information extraction.\n\nThe original study on CLEAR demonstrated notable improvements:\n*   **Semantic Accuracy:** It achieved a significant performance improvement with an F1 score of 0.90, compared to 0.86 for traditional Retrieval-Augmented Generation (RAG) methods, on clinical information extraction tasks. This indicates better accuracy in understanding and extracting relevant clinical information.\n*   **Computational Efficiency:** It also provided substantial efficiency gains, including a 71% reduction in tokens processed and a 72% faster inference time. This makes it a highly efficient approach for processing large volumes of clinical data.\n\nThese benefits position CLEAR as a potentially transformative approach for production EHR processing systems, as it effectively addresses challenges like the \"lost in the middle\" problem often encountered when processing long clinical narratives with other methods.\n\n**Key Points/Summary:**\n*   **Name:** CLinical Entity Augmented Retrieval (CLEAR) methodology.\n*   **Approach:** Uses entity-aware and entity-centered retrieval strategies specifically designed for clinical semantics.\n*   **Benefits:** Achieves improved semantic accuracy (higher F1 score) and significant computational efficiency (reduced tokens and faster inference time).\n*   **Purpose:** Addresses limitations of traditional methods in processing EHR notes for accurate information extraction.\n\n**Extra Tip:**\nThink of \"entity-aware\" retrieval like a smart librarian who doesn't just look for keywords, but understands the *meaning* of clinical terms (like specific diseases, medications, or symptoms) and their relationships within a patient's record. This allows for much more precise and relevant information retrieval compared to just matching similar-sounding text chunks!",
        "correctness_score": 0.95,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 6,
        "document_path": "static/Accelerating_LLM_Inference.pdf",
        "question": "Contrast ChunkLLM and Dynamic Hierarchical Sparse Attention (DHSA) regarding their dynamic attention management mechanisms, specifically addressing how they derive chunk representations and utilize them to achieve efficiency gains while preserving performance in long-context models.",
        "ground_truth": "ChunkLLM and Dynamic Hierarchical Sparse Attention (DHSA) both propose mechanisms for efficient long-context modeling by dynamically managing attention sparsity, but they employ different architectural additions and chunk representation strategies. ChunkLLM introduces two pluggable components: the QK Adapter (Q-Adapter and K-Adapter) and the Chunk Adapter. The Chunk Adapter is a one-layer feed-forward neural network (FNN) classifier that detects if a token is a chunk boundary using contextual semantic information. The QK Adapter fulfills feature compression and generates chunk attention scores, trained using an attention distillation approach where the Kullback-Leibler (KL) divergence between chunk attention scores and full attention scores guides optimization to enhance the recall rate of key chunks. ChunkLLM leverages the Intra-Chunk Attention Consistency (ICAC) pattern, triggering chunk selection updates exclusively when the current token is identified as a chunk boundary, substantially enhancing inference efficiency. ChunkLLM maintains 98.64% of the vanilla model's performance on long-context benchmarks and achieves a maximum speedup of 4.48x when processing 120K long texts. DHSA, conversely, is a plug-in module that dynamically predicts attention sparsity during prefill and decode stages without retraining the base model. DHSA employs a **Dynamic Hierarchical Sparsity Prediction approach. It first uses a boundary prediction function to adaptively segment input sequences into variable-length chunks. Chunk representations ($q_c$ and $k_c$) are derived by aggregating token queries and keys using a **length-normalized aggregation strategy, which involves scaling the sum of embeddings by the square root of the chunk size ($\\sqrt{|C|}$) to mitigate sensitivity to variable chunk lengths. It estimates chunk-level similarity ($S_c$) and then upsamples it to obtain the token-level similarity matrix ($S_t$), applying TOPK selection to generate the sparsity mask. DHSA reports matching dense attention in accuracy, while reducing prefill latency by 20-60% and peak memory usage by 35%.",
        "retrieved_context": {
            "context": "# Sparse Attention The sparse attention mechanism constructs sparse attention matrices by confining\nattention to predefined patterns, such as local windows or fixed-stride block patterns. Beltagy et al.\n(2020a) combined dilated local window attention with task-specific global attention. MoBA (Lu\net al., 2025) proposes an innovative mixed block attention mechanism. ESA (Wang et al., 2025)\nreduces computational costs by selecting tokens most critical to the current generation for attention\ncalculation. NSA (Yuan et al., 2025) combines coarse-grained token compression and fine-grained\ntoken selection. SepLLM (Chen et al., 2024) finds that the segment information between separators\ncan be effectively compressed into the separators themselves without causing significant information\nloss.\n\n# Sparse Attention The sparse attention mechanism constructs sparse attention matrices by confining\nattention to predefined patterns, such as local windows or fixed-stride block patterns. Beltagy et al.\n(2020a) combined dilated local window attention with task-specific global attention. MoBA (Lu\net al., 2025) proposes an innovative mixed block attention mechanism. ESA (Wang et al., 2025)\nreduces computational costs by selecting tokens most critical to the current generation for attention\ncalculation. NSA (Yuan et al., 2025) combines coarse-grained token compression and fine-grained\ntoken selection. SepLLM (Chen et al., 2024) finds that the segment information between separators\ncan be effectively compressed into the separators themselves without causing significant information\nloss.\n\n# Haojie Ouyang [1] **, Jianwei Lv** [2] **, Lei Ren** [2] **, Chen Wei** [2] **, Xiaojie Wang** [1] **, Fangxiang Feng** [1]  \n1 School of Artificial Intelligence, Beijing University of Posts and Telecommunications\n2 Li Auto  \nouyanghaojie@bupt.edu.cn  \nA BSTRACT  \nTransformer-based large models excel in natural language processing and computer vision, but face severe computational inefficiencies due to the self-attention\u2019s\nquadratic complexity with input tokens. Recently, researchers have proposed a\nseries of methods based on block selection and compression to alleviate this problem, but they either have issues with semantic incompleteness or poor traininginference efficiency. To comprehensively address these challenges, we propose\nChunkLLM, a lightweight and pluggable training framework. Specifically, we\nintroduce two components: QK Adapter (Q-Adapter and K-Adapter) and Chunk\nAdapter. The former is attached to each Transformer layer, serving dual purposes\nof feature compression and chunk attention acquisition. The latter operates at the\nbottommost layer of the model, functioning to detect chunk boundaries by leveraging contextual semantic information. During the training phase, the parameters\nof the backbone remain frozen, with only the QK Adapter and Chunk Adapter\nundergoing training. Notably, we design an attention distillation method for training the QK Adapter, which enhances the recall rate of key chunks. During the\ninference phase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference. Experimental\nevaluations are conducted on a diverse set of long-text and short-text benchmark\ndatasets spanning multiple tasks. ChunkLLM not only attains comparable performance on short-text benchmarks but also maintains 98.64% of the performance\non long-context benchmarks while preserving a 48.58% key-value cache retention\nrate. Particularly, ChunkLLM attains a maximum speedup of 4.48\u00d7 in comparison\nto the vanilla Transformer in the processing of 120K long texts.  \n1 I NTRODUCTION  \nTransformer-based large models (Vaswani et al., 2017) have demonstrated exceptional performance\nacross a diverse range of tasks, including natural language processing (Srivastava et al., 2025; Zhang\net al., 2024) and computer vision (Jiang et al., 2025). However, they have also faced significant challenges in terms of computational efficiency, particularly when scaling to larger structures and large\ncontext inputs. A core issue of efficiency limitations lies in the self-attention module, whose computational complexity is a quadratic relationship with the number of input tokens. Such deficiencies\nin computational efficiency exert a profound impact on both the training complexity and inference\nlatency of large models.  \nEfficiency optimization of Transformer has emerged as a pivotal research domain, with efforts predominantly converging into three methodological paradigms. **Linear attention**, such as Mamba\n(Dao & Gu, 2024), RWKV (Peng et al., 2023b; 2024), and RetNet (Sun et al., 2023), seek to\napproximate and substitute the traditional softmax-based self-attention mechanism. However, the\nfundamental architectural disparities between linear attention and conventional attention mechanisms introduce non-trivial challenges: adapting pre-existing Transformer models to integrate linear attention often incurs prohibitive conversion costs(Mercat et al., 2024; Wang et al., 2024; Bick\net al., 2024), while alternative strategies necessitate end-to-end training of entirely new model from\nscratch(Li et al., 2025). Another optimization paradigm is **Sparse attention**, which leverages predefined structural constraints, such as sink-based attention mechanisms (Xiao et al., 2024) or sliding  \n1  \nwindow attention mechanisms (Beltagy et al., 2020b), to exploit this sparsity. While these methods may yield certain effects, they often rely heavily on specific tasks, which can limit the overall\ngeneralization ability of the model. Dynamic sparse attention mechanisms (Tang et al., 2024; Jiang\net al., 2024; Liu et al., 2024) filter out subsets of tokens during the inference phase. Although such\nmethods can reduce the computational load of long sequences, they fail to significantly lower the\nhigh training costs of long-context models, making it difficult for large language models to efficiently scale to context-processing tasks with million-level token sizes. **Chunk Selective attention**,\na special type of sparse attention, can be primarily categorized into two paradigms: fixed chunk\n(Lu et al., 2025; Yuan et al., 2025; Wang et al., 2025) and separators-based dynamic chunk (Chen\net al., 2024). Both approaches partition the input into discrete chunks: the former conducts partitioning with a fixed length, which gives rise to semantic incompleteness; the latter utilizes separators\nfor partitioning, yet ambiguities often emerge. For example, periods frequently occur in numerical\nvalues or abbreviations. Furthermore, during the inference phase, these methods necessitate chunk\nselection for each generated token, incurring additional computational overhead. It is thus evident\nthat existing efficient approaches still exhibit inherent limitations.  \nTo address the aforementioned challenges, We propose ChunkLLM, which can be directly constructed by integrating two lightweight and trainable modules into existing LLMs: **QK Adapter**\nand **Chunk Adapter** . The Chunk Adapter connects to the output of the bottommost Transformer\nlayer and used for identify if a token is the last token of a chunk. The QK Adapter is in parallel with\nQ and K matrix at each Transformer layer. It maps full attention scores to chunk attention scores,\nand trained by a distillation approach.  \nThe QK Adapter fulfills feature compression and the generation of chunk attention scores. To train\nthe QK Adapter, we propose an attention distillation approach designed to enhance the recall rate\nof key chunks. During training, LLM parameters are kept frozen, with the Kullback\u2013Leibler (KL)\ndivergence between chunk attention scores and full attention scores serving as a guidance signal for\noptimization. The Chunk Adapter determines whether a token corresponds to a chunk boundary by\nleveraging contextual semantic information. During the inference phase, we exploit the Intra-Chunk\nAttention Consistency (ICAC) pattern such that chunk selection is only updated when the current\ntoken is identified as a chunk boundary, which substantially enhances inference efficiency. Furthermore, ChunkLLM can achieve inference performance comparable to that of models optimized for\n120K context lengths, despite being trained solely on 4K context lengths, thereby substantially reducing the training overhead associated with 120K context scaling. Experimental results validate\nthat ChunkLLM yields a 4.48\u00d7 speedup relative to the vanilla Transformer when processing 120K\nlong texts.  \nOur contributions are summarized as follows:  \n- We introduce ChunkLLM that integrates two lightweight and pluggable components into\nexisting LLMs: the QK Adapter and the Chunk Adapter. The newly developed ChunkLLM only necessitates fine-tuning these lightweight components on the basis of the original model architecture. This design enables ChunkLLM to attain performance comparable\nto vanilla Transformer while utilizing a smaller KV cache, alongside achieving effective\ncontrol over computational scale.  \n- We propose an attention distillation-based training approach for the QK Adapter, which\nleverages KL divergence to drive chunk attention toward approximating full attention, effectively enhancing the recall rate of key chunks. Furthermore, we introduce a novel ICAC\npattern, which yields notable improvements in inference efficiency for long-context scenarios.  \n- Experimental evaluations show that ChunkLLM not only attains comparable performance\non short-text benchmarks but also maintains 98.64% of the performance on long-context\nbenchmarks while preserving a 48.58% key-value cache (kvcache) retention rate, relative\nto the vanilla Transformer. Particularly, ChunkLLM attains a maximum speedup of 4.48\u00d7\nin comparison to the vanilla Transformer in the processing of 120K long texts.  \n2  \n2 M ETHOD  \nThe framework of ChunkLLM is shown in Figure 1. ChunkLLM can be built on any existing\ntransformer-based LLMs. Two extra lightweight and pluggable modules are designed to support\nchunk-related capability. One is Chunk Adapter, which is used to identify chunk boundaries. The\nother is the Q Adapter and K Adapter, which is tailored for efficient feature compression and chunk\nselection. This section elaborates on the details of the two modules.  \n2.1 C HUNK A DAPTER  \nThe Chunk Adapter is a one-layer forward\nneural network (FNN) classifier for chunk\nboundary prediction. Its input is the output\nof the first layer of the LLM, and the output is if or not the token is a chunk boundary, as depicted in the Figure 1.  \nFor an input X = _{x_ 1 _, x_ 2 _, ..., x_ _n\u2212_ 1 _, x_ _n_ _}_\nwith _n_ tokens and their corresponding labels Y = _{y_ 1 _, y_ 2 _, ..., y_ _n\u2212_ 1 _, y_ _n_ _}_, _y_ _i_ _\u2208_\n_{_ 0 _,_ 1 _}_, where 1 indicates that the token\n_x_ _i_ is a chunk boundary, 0 indicates that it\ndoes not, **H** _[l]_ _i_ [1] [be the output of] _[ x]_ _[i]_ [ at first]\nlayer. The FNN based Chunk Adapter is\ngiven as in equation 1  \n\u02c6 1 _,_ Sigmoid(FFN( **H** _[l]_ _i_ [1] [))] _[ > \u03b1,]_\n_y_ _i_ =\n\ufffd0 _,_ _otherwise_\n(1)  \noutput  \nInput  \nFigure 1: The framework of ChunkLLM.  \nFor training the chunk adapter, we employ\nthe binary cross-entropy loss (BCE) (equation 2) as the objective function. Detailed\ninformation on the training dataset will be given in the experiment part.  \n_L_ _CBP_ = _\u2212_ [1]  \n_n_  \n_n_  \n\u02c6  \n\ufffd[ _y_ _i_ _\u00b7 log_ (\u02c6 _y_ _i_ ) + (1 _\u2212_ _y_ _i_ ) _\u00b7 log_ (1 _\u2212_ _y_ _i_ )] (2)  \n_i_ =1  \n2.2 QK A DAPTER  \nAt each layer of the LLM, we incorporate a Q-Adapter and a K-Adapter which used to compress the\nattention and select the chunks.  \nFor each layer, let **Q** and **K** be the attention matrix respectively, _c_ be the chunk number of the input,\n_Index_ ~~_c_~~ = _{i_ 1 _, i_ 2 _, ...i_ _c_ _}_ is the index set of chunk boundary tokens. Let **K** [\u02c6] be the K matrix of these\ntokens. We then calculate chunk attention scores as follows:\n\n# Haojie Ouyang [1] **, Jianwei Lv** [2] **, Lei Ren** [2] **, Chen Wei** [2] **, Xiaojie Wang** [1] **, Fangxiang Feng** [1]  \n1 School of Artificial Intelligence, Beijing University of Posts and Telecommunications\n2 Li Auto  \nouyanghaojie@bupt.edu.cn  \nA BSTRACT  \nTransformer-based large models excel in natural language processing and computer vision, but face severe computational inefficiencies due to the self-attention\u2019s\nquadratic complexity with input tokens. Recently, researchers have proposed a\nseries of methods based on block selection and compression to alleviate this problem, but they either have issues with semantic incompleteness or poor traininginference efficiency. To comprehensively address these challenges, we propose\nChunkLLM, a lightweight and pluggable training framework. Specifically, we\nintroduce two components: QK Adapter (Q-Adapter and K-Adapter) and Chunk\nAdapter. The former is attached to each Transformer layer, serving dual purposes\nof feature compression and chunk attention acquisition. The latter operates at the\nbottommost layer of the model, functioning to detect chunk boundaries by leveraging contextual semantic information. During the training phase, the parameters\nof the backbone remain frozen, with only the QK Adapter and Chunk Adapter\nundergoing training. Notably, we design an attention distillation method for training the QK Adapter, which enhances the recall rate of key chunks. During the\ninference phase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference. Experimental\nevaluations are conducted on a diverse set of long-text and short-text benchmark\ndatasets spanning multiple tasks. ChunkLLM not only attains comparable performance on short-text benchmarks but also maintains 98.64% of the performance\non long-context benchmarks while preserving a 48.58% key-value cache retention\nrate. Particularly, ChunkLLM attains a maximum speedup of 4.48\u00d7 in comparison\nto the vanilla Transformer in the processing of 120K long texts.  \n1 I NTRODUCTION  \nTransformer-based large models (Vaswani et al., 2017) have demonstrated exceptional performance\nacross a diverse range of tasks, including natural language processing (Srivastava et al., 2025; Zhang\net al., 2024) and computer vision (Jiang et al., 2025). However, they have also faced significant challenges in terms of computational efficiency, particularly when scaling to larger structures and large\ncontext inputs. A core issue of efficiency limitations lies in the self-attention module, whose computational complexity is a quadratic relationship with the number of input tokens. Such deficiencies\nin computational efficiency exert a profound impact on both the training complexity and inference\nlatency of large models.  \nEfficiency optimization of Transformer has emerged as a pivotal research domain, with efforts predominantly converging into three methodological paradigms. **Linear attention**, such as Mamba\n(Dao & Gu, 2024), RWKV (Peng et al., 2023b; 2024), and RetNet (Sun et al., 2023), seek to\napproximate and substitute the traditional softmax-based self-attention mechanism. However, the\nfundamental architectural disparities between linear attention and conventional attention mechanisms introduce non-trivial challenges: adapting pre-existing Transformer models to integrate linear attention often incurs prohibitive conversion costs(Mercat et al., 2024; Wang et al., 2024; Bick\net al., 2024), while alternative strategies necessitate end-to-end training of entirely new model from\nscratch(Li et al., 2025). Another optimization paradigm is **Sparse attention**, which leverages predefined structural constraints, such as sink-based attention mechanisms (Xiao et al., 2024) or sliding  \n1  \nwindow attention mechanisms (Beltagy et al., 2020b), to exploit this sparsity. While these methods may yield certain effects, they often rely heavily on specific tasks, which can limit the overall\ngeneralization ability of the model. Dynamic sparse attention mechanisms (Tang et al., 2024; Jiang\net al., 2024; Liu et al., 2024) filter out subsets of tokens during the inference phase. Although such\nmethods can reduce the computational load of long sequences, they fail to significantly lower the\nhigh training costs of long-context models, making it difficult for large language models to efficiently scale to context-processing tasks with million-level token sizes. **Chunk Selective attention**,\na special type of sparse attention, can be primarily categorized into two paradigms: fixed chunk\n(Lu et al., 2025; Yuan et al., 2025; Wang et al., 2025) and separators-based dynamic chunk (Chen\net al., 2024). Both approaches partition the input into discrete chunks: the former conducts partitioning with a fixed length, which gives rise to semantic incompleteness; the latter utilizes separators\nfor partitioning, yet ambiguities often emerge. For example, periods frequently occur in numerical\nvalues or abbreviations. Furthermore, during the inference phase, these methods necessitate chunk\nselection for each generated token, incurring additional computational overhead. It is thus evident\nthat existing efficient approaches still exhibit inherent limitations.  \nTo address the aforementioned challenges, We propose ChunkLLM, which can be directly constructed by integrating two lightweight and trainable modules into existing LLMs: **QK Adapter**\nand **Chunk Adapter** . The Chunk Adapter connects to the output of the bottommost Transformer\nlayer and used for identify if a token is the last token of a chunk. The QK Adapter is in parallel with\nQ and K matrix at each Transformer layer. It maps full attention scores to chunk attention scores,\nand trained by a distillation approach.  \nThe QK Adapter fulfills feature compression and the generation of chunk attention scores. To train\nthe QK Adapter, we propose an attention distillation approach designed to enhance the recall rate\nof key chunks. During training, LLM parameters are kept frozen, with the Kullback\u2013Leibler (KL)\ndivergence between chunk attention scores and full attention scores serving as a guidance signal for\noptimization. The Chunk Adapter determines whether a token corresponds to a chunk boundary by\nleveraging contextual semantic information. During the inference phase, we exploit the Intra-Chunk\nAttention Consistency (ICAC) pattern such that chunk selection is only updated when the current\ntoken is identified as a chunk boundary, which substantially enhances inference efficiency. Furthermore, ChunkLLM can achieve inference performance comparable to that of models optimized for\n120K context lengths, despite being trained solely on 4K context lengths, thereby substantially reducing the training overhead associated with 120K context scaling. Experimental results validate\nthat ChunkLLM yields a 4.48\u00d7 speedup relative to the vanilla Transformer when processing 120K\nlong texts.  \nOur contributions are summarized as follows:  \n- We introduce ChunkLLM that integrates two lightweight and pluggable components into\nexisting LLMs: the QK Adapter and the Chunk Adapter. The newly developed ChunkLLM only necessitates fine-tuning these lightweight components on the basis of the original model architecture. This design enables ChunkLLM to attain performance comparable\nto vanilla Transformer while utilizing a smaller KV cache, alongside achieving effective\ncontrol over computational scale.  \n- We propose an attention distillation-based training approach for the QK Adapter, which\nleverages KL divergence to drive chunk attention toward approximating full attention, effectively enhancing the recall rate of key chunks. Furthermore, we introduce a novel ICAC\npattern, which yields notable improvements in inference efficiency for long-context scenarios.  \n- Experimental evaluations show that ChunkLLM not only attains comparable performance\non short-text benchmarks but also maintains 98.64% of the performance on long-context\nbenchmarks while preserving a 48.58% key-value cache (kvcache) retention rate, relative\nto the vanilla Transformer. Particularly, ChunkLLM attains a maximum speedup of 4.48\u00d7\nin comparison to the vanilla Transformer in the processing of 120K long texts.  \n2  \n2 M ETHOD  \nThe framework of ChunkLLM is shown in Figure 1. ChunkLLM can be built on any existing\ntransformer-based LLMs. Two extra lightweight and pluggable modules are designed to support\nchunk-related capability. One is Chunk Adapter, which is used to identify chunk boundaries. The\nother is the Q Adapter and K Adapter, which is tailored for efficient feature compression and chunk\nselection. This section elaborates on the details of the two modules.  \n2.1 C HUNK A DAPTER  \nThe Chunk Adapter is a one-layer forward\nneural network (FNN) classifier for chunk\nboundary prediction. Its input is the output\nof the first layer of the LLM, and the output is if or not the token is a chunk boundary, as depicted in the Figure 1.  \nFor an input X = _{x_ 1 _, x_ 2 _, ..., x_ _n\u2212_ 1 _, x_ _n_ _}_\nwith _n_ tokens and their corresponding labels Y = _{y_ 1 _, y_ 2 _, ..., y_ _n\u2212_ 1 _, y_ _n_ _}_, _y_ _i_ _\u2208_\n_{_ 0 _,_ 1 _}_, where 1 indicates that the token\n_x_ _i_ is a chunk boundary, 0 indicates that it\ndoes not, **H** _[l]_ _i_ [1] [be the output of] _[ x]_ _[i]_ [ at first]\nlayer. The FNN based Chunk Adapter is\ngiven as in equation 1  \n\u02c6 1 _,_ Sigmoid(FFN( **H** _[l]_ _i_ [1] [))] _[ > \u03b1,]_\n_y_ _i_ =\n\ufffd0 _,_ _otherwise_\n(1)  \noutput  \nInput  \nFigure 1: The framework of ChunkLLM.  \nFor training the chunk adapter, we employ\nthe binary cross-entropy loss (BCE) (equation 2) as the objective function. Detailed\ninformation on the training dataset will be given in the experiment part.  \n_L_ _CBP_ = _\u2212_ [1]  \n_n_  \n_n_  \n\u02c6  \n\ufffd[ _y_ _i_ _\u00b7 log_ (\u02c6 _y_ _i_ ) + (1 _\u2212_ _y_ _i_ ) _\u00b7 log_ (1 _\u2212_ _y_ _i_ )] (2)  \n_i_ =1  \n2.2 QK A DAPTER  \nAt each layer of the LLM, we incorporate a Q-Adapter and a K-Adapter which used to compress the\nattention and select the chunks.  \nFor each layer, let **Q** and **K** be the attention matrix respectively, _c_ be the chunk number of the input,\n_Index_ ~~_c_~~ = _{i_ 1 _, i_ 2 _, ...i_ _c_ _}_ is the index set of chunk boundary tokens. Let **K** [\u02c6] be the K matrix of these\ntokens. We then calculate chunk attention scores as follows:"
        },
        "generated_answer": "Based on the provided notes, I can explain ChunkLLM's dynamic attention management mechanisms. However, the notes do not contain information about \"Dynamic Hierarchical Sparse Attention (DHSA),\" so I cannot contrast ChunkLLM with DHSA.\n\nHere's how ChunkLLM manages dynamic attention to achieve efficiency while preserving performance:\n\n**ChunkLLM's Dynamic Attention Management Mechanism**\n\nChunkLLM is designed to address the computational inefficiencies of the self-attention mechanism in large Transformer models, which has a quadratic complexity with respect to the input token count. It introduces two lightweight, pluggable modules: the **Chunk Adapter** and the **QK Adapter**.\n\n1.  **Deriving Chunk Representations:**\n    *   **Chunk Adapter:** This module acts as a one-layer Feedforward Neural Network (FNN) classifier. Its role is to dynamically identify \"chunk boundaries\" within the input text. It takes the output of the *first layer* of the Large Language Model (LLM) as input and predicts whether each token is a boundary (labeled 1) or not (labeled 0). This prediction leverages contextual semantic information to determine where one logical \"chunk\" of text ends and another begins.\n    *   **QK Adapter:** This adapter is incorporated at each Transformer layer, working in parallel with the standard Query (Q) and Key (K) matrices. It performs two main functions: feature compression and chunk attention acquisition. Once the Chunk Adapter identifies chunk boundary tokens, the QK Adapter uses these specific tokens to calculate \"chunk attention scores.\" This means it focuses attention on the key information contained within these identified chunk boundaries, effectively creating a compressed representation of the chunks.\n\n2.  **Utilizing Chunk Representations for Efficiency and Performance:**\n    *   **Training Phase:** During training, the parameters of the main LLM backbone remain frozen. Only the Chunk Adapter and QK Adapter are trained. An \"attention distillation\" method is used for the QK Adapter, where the Kullback\u2013Leibler (KL) divergence between the chunk attention scores (from the QK Adapter) and the full attention scores (from the original Transformer) serves as a guide. This distillation process helps the QK Adapter learn to approximate the full attention, thereby enhancing the recall rate of \"key chunks\" \u2013 ensuring important information isn't lost despite compression.\n    *   **Inference Phase:** To significantly enhance inference efficiency, ChunkLLM exploits an \"Intra-Chunk Attention Consistency (ICAC)\" pattern. This means that chunk selection (and thus the update of attention) is *only triggered* when the current token is detected as a chunk boundary by the Chunk Adapter. This dynamic, on-demand update strategy avoids recalculating attention for every single token, drastically reducing computational overhead.\n    *   **Efficiency Gains:** The approach leads to substantial speedups, with ChunkLLM achieving a maximum speedup of 4.48\u00d7 when processing 120K long texts compared to a vanilla Transformer. It also maintains a high performance (98.64% of the vanilla Transformer) on long-context benchmarks while significantly reducing the Key-Value (KV) cache retention rate by 48.58%. This is particularly impressive as ChunkLLM can achieve this performance even when trained on shorter 4K context lengths, reducing training overhead for extremely long contexts.\n\n**Regarding DHSA:**\nThe provided notes do not contain any information about \"Dynamic Hierarchical Sparse Attention (DHSA).\" Therefore, I cannot provide details on its mechanisms or contrast it with ChunkLLM based on this context. You might need to provide additional materials that describe DHSA to get a comparison.\n\n---\n\n**Key Points/Summary:**\n*   **Chunk Adapter:** Identifies chunk boundaries using a FNN classifier based on contextual semantic information.\n*   **QK Adapter:** Compresses features and calculates chunk attention scores by focusing on identified chunk boundary tokens.\n*   **Attention Distillation:** QK Adapter is trained using KL divergence to approximate full attention, ensuring key information is retained.\n*   **Dynamic Inference:** Chunk selection and attention updates are only triggered at identified chunk boundaries, using an Intra-Chunk Attention Consistency (ICAC) pattern, leading to significant speedups and reduced KV cache usage.\n\n**Extra Tip:**\nThink of ChunkLLM like a smart reader who doesn't need to reread every single word to understand a long document. Instead, it learns to identify the most important sentences or paragraphs (chunk boundaries and key chunks) and focuses its processing power on those, only re-evaluating the overall context when a new \"important point\" is reached. This allows it to understand very long texts quickly without missing crucial information.",
        "correctness_score": 0.1,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    },
    {
        "question_id": 7,
        "document_path": "multiple files",
        "question": "Explain the observed trade-offs between computational efficiency and semantic integrity across various chunking and retrieval strategies\u2014including Fixed-size/Semantic Chunking, Late Chunking, Contextual Retrieval, and Clinical Entity Augmented Retrieval (CLEAR)\u2014and identify which approach demonstrated superior scalability advantages in high-complexity clinical documents.",
        "ground_truth": "The sources reveal significant trade-offs among various chunking and retrieval strategies concerning computational cost and the preservation of semantic integrity. 1. Fixed-size vs. Semantic Chunking (RAG Baseline): Traditional fixed-size chunking is computationally simple and efficient. However, its simplicity risks fragmenting semantically related content, leading to suboptimal retrieval. Semantic chunking, which aims for semantically coherent segments, involves additional computational costs that the sources found were often not justified by consistent performance gains on standard document structures. Overall, fixed-size chunking was suggested as a more efficient and reliable choice for practical RAG applications on non-synthetic datasets. 2. Late Chunking vs. Contextual Retrieval: Late Chunking defers segmentation until after the entire document is embedded, preserving full contextual information for efficiency. Late Chunking offers higher efficiency but may sacrifice relevance and completeness. In contrast, Contextual Retrieval enhances chunks by prompting an LLM to generate additional context for each chunk, improving contextual integrity. This context preservation, particularly when combined with Rank Fusion (ContextualRankFusion), yields better overall results in retrieval evaluation than Late Chunking but incurs greater computational resources, potentially requiring up to 20GB of VRAM for chunk contextualization in long documents. 3. Clinical Entity Augmented Retrieval (CLEAR): This entity-aware method achieves a balance by selectively centering clinically relevant spans around identified entities, overcoming the positional bias ('lost in the middle' problem) associated with processing entire long documents. CLEAR achieved a 78.4% token savings compared to Wide Context processing while maintaining the highest average semantic similarity (0.878), demonstrating an optimal balance between accuracy and computational cost. The approach that demonstrated superior scalability advantages in high-complexity documents was CLEAR. In evaluations involving large clinical notes (exceeding 65,000 tokens), CLEAR achieved a 75% win rate, confirming that its entity-aware retrieval advantages grow as document complexity and document size increase, making it highly suitable for large EHR document processing",
        "retrieved_context": {
            "context": "## 1.1 **Related Work**  \nEntity-aware retrieval has gained increasing attention within biomedical NLP and question-answering  \ndomains. Early retrieval-augmented methods such as RAG [2] demonstrated the potential of embedding  \nbased chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches  \nleveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al.,  \n2016) provided partial improvements but often failed to maintain contextual continuity across long clin  \nical narratives.  \nCLEAR [1] represented a significant advancement by introducing entity-centered retrieval aligned  \nwith clinical semantics. Our work extends this line of research by operationalizing CLEAR within an  \nend-to-end evaluation platform, providing reproducible empirical validation across realistic EHR-scale  \ndocument sets.  \nRecent evaluations of retrieval-augmented models in long-context reasoning (e.g., Karpinska et al.,  \n2023; Xiong et al., 2024) emphasize that retrieval strategies often outperform naive long-context prompt  \ning, supporting the need for entity-aware retrieval. Our work contributes a reproducible evaluation  \nframework within this paradigm, focusing on realistic EHR-scale clinical notes.  \n3\n\n# 1. **INTRODUCTION**  \nElectronic Health Record (EHR) systems in modern healthcare infrastructure store clinical documenta  \ntion within FHIR DocumentReference resources, typically encoded as base64 attachments containing  \nunstructured clinical notes. These documents, ranging from brief progress notes to comprehensive dis  \ncharge summaries, present substantial challenges for automated question-answering systems that require  \nsemantically accurate information extraction rather than statistical correlation-based retrieval common  \nin traditional vector database approaches.  \nContemporary approaches to clinical document processing have largely focused on two paradigms:  \nzero-shot inference with large context windows that process entire documents but face computational  \n2  \nconstraints and the \u201dlost in the middle\u201d problem, and chunk-based retrieval-augmented generation (RAG)  \nsystems that utilize vector databases for semantic similarity search but often fail to capture critical clin  \nical entity relationships and contextual dependencies essential for accurate medical information extrac  \ntion.  \nA growing body of evidence shows that merely expanding context windows does not guarantee  \neffective use of information: performance often drops when relevant spans occur in the middle of long  \ninputs (\u201clost in the middle\u201d) [3]. This motivates entity-aware retrieval that selectively centers clinically  \nrelevant spans rather than relying on statistically similar but potentially off-target chunks.  \nThe CLinical Entity Augmented Retrieval (CLEAR) methodology, published by Lopez et al. in  \n2025 [1], introduced a novel approach that addresses these limitations through entity-aware, entity  \ncentered retrieval strategies. The original study demonstrated significant performance improvements  \n(F1 score of 0.90 vs 0.86 for traditional RAG) with substantial efficiency gains (71% token reduction,  \n72% faster inference time) on clinical information extraction tasks, positioning CLEAR as a potentially  \ntransformative approach for production EHR processing systems.  \nTo validate these claims in realistic healthcare scenarios and provide a robust evaluation framework  \nfor clinical NLP approaches, we developed the Clinical Notes Q&A Evaluation Platform. This com  \nprehensive validation study implements and compares three fundamental approaches: (1) wide context  \nprocessing for zero-shot inference with large language models, (2) traditional vector database chunking  \nwith embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu  \nmentReference processing. Our contributions include: systematic validation of CLEAR\u2019s performance  \nclaims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis  \nof retrieval strategy performance across clinical documents of varying complexity and length.\n\n# Retrieval-Augmented Generation (RAG): Semantic chunking with embedding-based retrieval us  \ning top-k chunk selection. This approach prioritizes efficiency with minimal token usage (average 544  \ntokens per query) but may miss critical clinical relationships.\n\n# 5. **CONCLUSION**  \nThis study developed and deployed a comprehensive Clinical Notes Q&A Evaluation Platform and found  \nresults consistent with the benefits reported for Clinical Entity Augmented Retrieval (CLEAR) in prior  \nwork. In our synthetic EHR QA setting, entity-aware retrieval achieved stronger semantic similarity  \nthan wide-context processing at markedly lower token budgets, echoing the efficiency\u2013quality trade-offs  \nhighlighted by CLEAR [1].  \nThe validation results strongly confirm the original research findings, particularly demonstrating  \nscalability advantages on large clinical documents characteristic of comprehensive EHR DocumentRe  \nference content. The 75% win rate on documents exceeding 65,000 tokens validates that entity-aware  \nretrieval becomes increasingly advantageous as document complexity increases, confirming the method  \nology\u2019s suitability for enterprise healthcare environments.  \n10  \nFigure 4: Interactive interface showing different analysis approaches and their corresponding performance scores, enabling experimentation to improve reasoning accuracy.  \nThe Clinical Notes Q&A Evaluation Platform represents a significant contribution to clinical NLP  \nresearch by providing a systematic framework for evaluating retrieval strategies in realistic EHR pro  \ncessing scenarios. The platform\u2019s validation of CLEAR methodology demonstrates its viability as a  \nproduction-ready approach for clinical information extraction systems requiring optimal balance be  \ntween semantic accuracy and computational efficiency.  \nThe demonstrated effectiveness of CLEAR through systematic platform-based validation provides  \nevidence-based guidance for healthcare organizations implementing clinical question-answering sys  \ntems. The evaluation platform framework enables continued research and development in clinical  \nentity-aware retrieval methodologies while supporting reproducible evaluation of future clinical NLP  \ninnovations in production-relevant contexts."
        },
        "generated_answer": "Hello there! I can help you understand the trade-offs between computational efficiency and semantic integrity for different retrieval strategies based on your notes.\n\nYour notes discuss three primary approaches:\n\n1.  **Wide Context Processing (Zero-shot inference with large context windows)**\n2.  **Traditional Vector Database Chunking (Chunk-based Retrieval-Augmented Generation - RAG)**, which aligns with what you might call \"Fixed-size/Semantic Chunking.\"\n3.  **Clinical Entity Augmented Retrieval (CLEAR)**\n\nThe notes do not explicitly detail \"Late Chunking\" or \"Contextual Retrieval\" as distinct strategies with their specific trade-offs, beyond mentioning earlier \"UMLS concepts and ontology-based retrieval\" that had partial improvements but failed to maintain contextual continuity.\n\nHere's an explanation of the strategies covered in your notes:\n\n### Trade-offs Between Computational Efficiency and Semantic Integrity\n\n1.  **Wide Context Processing (Zero-shot Inference with Large Context Windows)**\n    *   **Semantic Integrity:** This approach aims for high semantic integrity by processing entire documents within a large context window. In theory, having the full document available should allow for comprehensive understanding.\n    *   **Computational Efficiency:** It faces significant computational constraints due to the large amount of data being processed. Furthermore, it suffers from the \"lost in the middle\" problem, where performance drops if relevant information is located in the middle of a very long input, thereby degrading its effective semantic integrity despite having the full context.\n    *   **Trade-off:** High potential for semantic integrity (if the \"lost in the middle\" problem is avoided) but at a high computational cost.\n\n2.  **Traditional Vector Database Chunking (Chunk-based RAG / \"Fixed-size/Semantic Chunking\")**\n    *   **Semantic Integrity:** This method uses embedding-based chunk retrieval to find statistically similar pieces of information. However, it often struggles to capture critical clinical entity relationships and contextual dependencies that are crucial for accurate medical information extraction. It relies on statistical correlation rather than deep semantic understanding.\n    *   **Computational Efficiency:** It prioritizes efficiency and minimal token usage (e.g., an average of 544 tokens per query), making it computationally lighter than wide context processing.\n    *   **Trade-off:** High computational efficiency but often at the expense of semantic integrity, as it may miss crucial clinical relationships and contextual nuances.\n\n3.  **Clinical Entity Augmented Retrieval (CLEAR)**\n    *   **Semantic Integrity:** CLEAR introduces entity-aware, entity-centered retrieval strategies, which are specifically designed to align with clinical semantics. It selectively centers clinically relevant spans, significantly improving semantic accuracy and maintaining contextual continuity compared to traditional RAG. It achieves stronger semantic similarity than wide-context processing.\n    *   **Computational Efficiency:** CLEAR demonstrates substantial efficiency gains, including a 71% token reduction and 72% faster inference time compared to traditional RAG. It also operates at markedly lower token budgets than wide-context processing.\n    *   **Trade-off:** CLEAR is presented as an approach that achieves an optimal balance, offering both superior semantic accuracy and high computational efficiency.\n\n### Superior Scalability Advantages in High-Complexity Clinical Documents\n\n**Clinical Entity Augmented Retrieval (CLEAR)** demonstrated superior scalability advantages in high-complexity clinical documents.\n\nThe validation results strongly confirmed that entity-aware retrieval becomes increasingly advantageous as document complexity increases. Specifically, CLEAR achieved a **75% win rate on documents exceeding 65,000 tokens**, validating its suitability for enterprise healthcare environments with large and complex EHR DocumentReference content.\n\n---\n\n**Key Points/Summary:**\n\n*   **Wide Context Processing:** High semantic potential, but computationally expensive and prone to the \"lost in the middle\" problem.\n*   **Traditional RAG/Chunking:** Computationally efficient, but often lacks critical semantic understanding and misses clinical relationships.\n*   **CLEAR:** Offers an optimal balance, providing both stronger semantic accuracy and high computational efficiency.\n*   **Scalability:** CLEAR (entity-aware retrieval) showed superior scalability, becoming more advantageous with increasing document complexity, especially for documents over 65,000 tokens.\n\n**Extra Tip:**\nThink of the \"lost in the middle\" problem like trying to find a specific sentence in a very long book by only glancing at the middle pages. Even if the information is there, a general search might miss it if it's not prominently positioned or highlighted. Entity-aware retrieval, like CLEAR, is like having a smart index that directly points you to the most relevant sections based on specific medical concepts, ensuring you don't miss crucial details even in vast documents.",
        "correctness_score": 0.55,
        "faithfulness_score": 1.0,
        "model_tested": "Layout"
    }
]