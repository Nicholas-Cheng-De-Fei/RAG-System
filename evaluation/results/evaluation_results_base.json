[
    {
        "question_id": 1,
        "document_path": "static/Computational cost of semantic chunking.pdf",
        "question": "What is the defining characteristic of 'semantic chunking' in Retrieval-Augmented Generation (RAG) systems?",
        "ground_truth": "Semantic chunking is a strategy that aims to improve retrieval performance by dividing documents into semantically coherent segments. This approach segments documents based on semantic similarity or detecting semantic distance thresholds between consecutive sentences to maintain coherence",
        "retrieved_context": {
            "context": "need for more efficient chunking strategies in RAG systems. 1 Introduction In Retrieval-Augmented Generation (RAG) sys- tems, cutting documents into smaller units called chunks has a crucial effect on the quality of both retrieval and generation tasks (Chen et al., 2023; Wadhwa et al., 2024; Shi et al., 2023; Yu et al., 2023). By retrieving the most relevant chunks for a given query and feeding them into a generative language model, these systems aim to produce ac- curate and contextually appropriate\n\nneed for more efficient chunking strategies in RAG systems. 1 Introduction In Retrieval-Augmented Generation (RAG) sys- tems, cutting documents into smaller units called chunks has a crucial effect on the quality of both retrieval and generation tasks (Chen et al., 2023; Wadhwa et al., 2024; Shi et al., 2023; Yu et al., 2023). By retrieving the most relevant chunks for a given query and feeding them into a generative language model, these systems aim to produce ac- curate and contextually appropriate\n\nbenefits of semantic chunk- ing, suggesting that its advantages are highly task- dependent and often insufficient to justify the added computational costs. This study lays the ground- work for future exploration of more efficient and adaptive chunking strategies in RAG systems. In general, our contributions are:  We present a novel, large-scale evaluation framework comparing semantic and fixed-size chunking across diverse tasks.  We demonstrate that while semantic chunk- ing shows some benefits in certain\n\nbenefits of semantic chunk- ing, suggesting that its advantages are highly task- dependent and often insufficient to justify the added computational costs. This study lays the ground- work for future exploration of more efficient and adaptive chunking strategies in RAG systems. In general, our contributions are:  We present a novel, large-scale evaluation framework comparing semantic and fixed-size chunking across diverse tasks.  We demonstrate that while semantic chunk- ing shows some benefits in certain\n\nIs Semantic Chunking Worth the Computational Cost? Renyi Qu Vectara, Inc. renyi@vectara.com Forrest Bao Vectara, Inc. forrest@vectara.com Ruixuan Tu University of WisconsinMadison turx2003@gmail.com Abstract Recent advances in Retrieval-Augmented Gen- eration (RAG) systems have popularized se- mantic chunking, which aims to improve re- trieval performance by dividing documents into semantically coherent segments. Despite its growing adoption, the actual benefits over sim- pler fixed-size chunking, where\n\nIs Semantic Chunking Worth the Computational Cost? Renyi Qu Vectara, Inc. renyi@vectara.com Forrest Bao Vectara, Inc. forrest@vectara.com Ruixuan Tu University of WisconsinMadison turx2003@gmail.com Abstract Recent advances in Retrieval-Augmented Gen- eration (RAG) systems have popularized se- mantic chunking, which aims to improve re- trieval performance by dividing documents into semantically coherent segments. Despite its growing adoption, the actual benefits over sim- pler fixed-size chunking, where\n\nRecently, there has been a surge of interest in semantic chunking, where documents are seg- mented based on semantic similarity, with some in- dustry applications suggesting promising improve- ments in performance (LangChain, 2024; LlamaIn- dex, 2024; McCormick, 2024). However, there is no systematic evidence that semantic chunking yields a performance gain in downstream tasks, and if there is, the gain is significant enough to justify the computational overhead than fixed-size chunking. Such a systematic\n\nRecently, there has been a surge of interest in semantic chunking, where documents are seg- mented based on semantic similarity, with some in- dustry applications suggesting promising improve- ments in performance (LangChain, 2024; LlamaIn- dex, 2024; McCormick, 2024). However, there is no systematic evidence that semantic chunking yields a performance gain in downstream tasks, and if there is, the gain is significant enough to justify the computational overhead than fixed-size chunking. Such a systematic\n\nclusions. Dataset Fixed-size Breakpoint Clustering ExpertQA 0.65 0.65 0.65 DelucionQA 0.76 0.76 0.76 TechQA 0.68 0.68 0.68 ConditionalQA 0.42 0.43 0.43 Qasper 0.49 0.49 0.50 Table 3: BERTScore for Answer Generation. 5 Conclusion In this paper, we evaluated semantic and fixed-size chunking strategies in RAG systems across docu- ment retrieval, evidence retrieval, and answer gen- eration. Semantic chunking occasionally improved performance, particularly on stitched datasets with high topic diversity.\n\nclusions. Dataset Fixed-size Breakpoint Clustering ExpertQA 0.65 0.65 0.65 DelucionQA 0.76 0.76 0.76 TechQA 0.68 0.68 0.68 ConditionalQA 0.42 0.43 0.43 Qasper 0.49 0.49 0.50 Table 3: BERTScore for Answer Generation. 5 Conclusion In this paper, we evaluated semantic and fixed-size chunking strategies in RAG systems across docu- ment retrieval, evidence retrieval, and answer gen- eration. Semantic chunking occasionally improved performance, particularly on stitched datasets with high topic diversity."
        },
        "generated_answer": "**Answer:**\nIn Retrieval-Augmented Generation (RAG) systems, the defining characteristic of 'semantic chunking' is that it divides documents into **semantically coherent segments** or segments them based on **semantic similarity**. This approach aims to improve retrieval performance by grouping related information together.\n\n**Key Points/Summary:**\n*   **Semantic Coherence:** Documents are broken into chunks where the content within each chunk is semantically related.\n*   **Semantic Similarity:** Segmentation is performed based on the meaning or similarity of the text.\n*   **Improved Retrieval:** The goal is to enhance the accuracy of retrieving relevant information.\n\n**Extra Tip:**\nThink of it like organizing a book by chapters and sub-sections, where each section focuses on a specific topic. Semantic chunking tries to achieve a similar logical grouping of information, but automatically, based on the text's meaning.",
        "correctness_score": 0.9,
        "faithfulness_score": 1.0,
        "model_tested": "base"
    },
    {
        "question_id": 2,
        "document_path": "static/Computational cost of semantic chunking.pdf",
        "question": "What three primary proxy tasks were designed to indirectly evaluate the quality of chunking strategies in the study presented in the sources?",
        "ground_truth": "The study systematically evaluated the effectiveness of chunking strategies using three proxy tasks: document retrieval, evidence retrieval, and retrieval-based answer generation",
        "retrieved_context": {
            "context": "answers. 4 Results 4.1 Measuring and reporting performances As mentioned earlier, we used three proxy tasks the study chunking. We cannot directly assess the quality of retrieval at the chunk level due to the lack of ground-truth at the chunk level. Instead, each retrieved chunk is mapped back to either the source document or the included evidence sentences. Since the number of relevant documents or evi- dence sentences is not fixed (unlike the k value for retrieved chunks), traditional metrics such as Re-\n\nanswers. 4 Results 4.1 Measuring and reporting performances As mentioned earlier, we used three proxy tasks the study chunking. We cannot directly assess the quality of retrieval at the chunk level due to the lack of ground-truth at the chunk level. Instead, each retrieved chunk is mapped back to either the source document or the included evidence sentences. Since the number of relevant documents or evi- dence sentences is not fixed (unlike the k value for retrieved chunks), traditional metrics such as Re-\n\nevaluation is not trivial due to the lack of data that can be directly used to com- pare chunking strategies. Therefore, we design an indirect evaluation using three proxy tasks: (1) doc- ument retrieval, measuring the ability to identify relevant documents; (2) evidence retrieval, mea- suring the ability to locate ground-truth evidence; and (3) answer generation, testing the quality of answers produced by a generative model using re- trieved chunks. Our findings challenge prevailing assumptions about the\n\nevaluation is not trivial due to the lack of data that can be directly used to com- pare chunking strategies. Therefore, we design an indirect evaluation using three proxy tasks: (1) doc- ument retrieval, measuring the ability to identify relevant documents; (2) evidence retrieval, mea- suring the ability to locate ground-truth evidence; and (3) answer generation, testing the quality of answers produced by a generative model using re- trieved chunks. Our findings challenge prevailing assumptions about the\n\nembeddings is necessary before definitively concluding the limitations of semantic chunking. Lack of Chunk Quality Measures As noted in Section 4, while the output chunks differed be- tween methods, retrieval and generation perfor- mances were similar across chunkers. In addition to the influence of embedding models, the absence of direct chunk quality metrics likely contributed to this issue. Having ground-truth query-chunk rel- evance scores would provide more accurate evalua- tions than relying solely\n\nembeddings is necessary before definitively concluding the limitations of semantic chunking. Lack of Chunk Quality Measures As noted in Section 4, while the output chunks differed be- tween methods, retrieval and generation perfor- mances were similar across chunkers. In addition to the influence of embedding models, the absence of direct chunk quality metrics likely contributed to this issue. Having ground-truth query-chunk rel- evance scores would provide more accurate evalua- tions than relying solely\n\nTable 2, Fixed-size Chunker per- formed best on 3 out of 5 datasets, indicating a slight edge in capturing core evidence sentences. However, the performance differences between the Fixed-size Chunker and the two semantic chunkers were minimal, suggesting no clear advantage for any specific chunking strategy. See Appendix B for more details. Further inspection revealed that despite varia- tions in chunking methods, the top-k retrieved chunks frequently contained the same evidence sen- tences, explaining the\n\nTable 2, Fixed-size Chunker per- formed best on 3 out of 5 datasets, indicating a slight edge in capturing core evidence sentences. However, the performance differences between the Fixed-size Chunker and the two semantic chunkers were minimal, suggesting no clear advantage for any specific chunking strategy. See Appendix B for more details. Further inspection revealed that despite varia- tions in chunking methods, the top-k retrieved chunks frequently contained the same evidence sen- tences, explaining the\n\nthresholds are generally higher than those from percentiles or interquartile ranges. Fixed-size Chunker Figure 5 shows results for the Fixed-size Chunker. The trends mirror those seen in other chunkers. Adding one overlapping sentence between chunks does not notably improve performance, indicating that a single overlapping sentence is insufficient to significantly boost con- textual coherence. E.4 Chunk Inspection We examined the output chunks to (1) confirm that different chunkers were functioning as\n\nthresholds are generally higher than those from percentiles or interquartile ranges. Fixed-size Chunker Figure 5 shows results for the Fixed-size Chunker. The trends mirror those seen in other chunkers. Adding one overlapping sentence between chunks does not notably improve performance, indicating that a single overlapping sentence is insufficient to significantly boost con- textual coherence. E.4 Chunk Inspection We examined the output chunks to (1) confirm that different chunkers were functioning as"
        },
        "generated_answer": "Hello! I can certainly help you with that.\n\nThe study designed an indirect evaluation using three primary proxy tasks to assess the quality of chunking strategies. These tasks were necessary because directly comparing chunking strategies is difficult due to a lack of suitable data.\n\nHere are the three proxy tasks:\n\n1.  **Document Retrieval:** This task measures how well a system can identify relevant source documents.\n2.  **Evidence Retrieval:** This task evaluates the ability to pinpoint ground-truth evidence within the documents.\n3.  **Answer Generation:** This task tests the quality of answers produced by a generative model, using the chunks that were retrieved.\n\n**Key Points/Summary:**\n*   The evaluation of chunking strategies was indirect.\n*   Three proxy tasks were used: document retrieval, evidence retrieval, and answer generation.\n*   These tasks aimed to measure the identification of relevant documents, location of ground-truth evidence, and quality of generated answers.\n\n**Extra Tip:**\nThink of \"proxy tasks\" as stand-in experiments. Since they couldn't directly measure chunk quality (like having a perfect answer key for chunks), they measured related outcomes (like finding documents, finding evidence, and generating good answers) that would indicate how effective the chunking was.",
        "correctness_score": 1.0,
        "faithfulness_score": 1.0,
        "model_tested": "base"
    },
    {
        "question_id": 3,
        "document_path": "static/Reconstructing Context.pdf",
        "question": "What are two advanced chunking techniques, besides traditional early chunking, aimed at preserving global context within RAG systems?",
        "ground_truth": "Two advanced techniques introduced to preserve global context and mitigate context fragmentation are late chunking and contextual retrieval. Late chunking involves embedding the entire document first before segmentation to retain global context, potentially leading to superior results across various retrieval tasks.",
        "retrieved_context": {
            "context": "to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coher- ence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice rel- evance and\n\nto preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coher- ence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice rel- evance and\n\n4 J. Singh and C. Merola to align with the early and late chunking strategies under evaluation. This ad- justment allows us to explore how different embedding techniques influence the retrieval quality and, subsequently, the overall performance of the RAG system. Additionally, we test dynamic segmenting models to further refine the chunk- ing process, providing an adaptive mechanism that adjusts chunk sizes based on content characteristics. By evaluating the impact of these dynamic segmenting\n\n4 J. Singh and C. Merola to align with the early and late chunking strategies under evaluation. This ad- justment allows us to explore how different embedding techniques influence the retrieval quality and, subsequently, the overall performance of the RAG system. Additionally, we test dynamic segmenting models to further refine the chunk- ing process, providing an adaptive mechanism that adjusts chunk sizes based on content characteristics. By evaluating the impact of these dynamic segmenting\n\nRAG. A standard RAG workflow involves four main stages: document segmentation, chunk embedding, indexing, and retrieval. During segmentation, documents are divided into manageable chunks. These chunks are then trans- formed into vector representations using encoder models, often normalized to 1 2\n\nRAG. A standard RAG workflow involves four main stages: document segmentation, chunk embedding, indexing, and retrieval. During segmentation, documents are divided into manageable chunks. These chunks are then trans- formed into vector representations using encoder models, often normalized to 1 2\n\nmodels,weaimtoimprovetheoverallretrievalefficiencyandresponsegeneration within the RAG framework. Early Chunking. Documents are segmented into text chunks, and each chunk is processed by the embedding model. The model generates token-level embed- dings for each chunk, which are subsequently aggregated using mean pooling to produce a single embedding per chunk. Late Chunking. Late chunking defers the chunking process. As shown in Figure 3.1, instead of segmenting the document initially, the entire document\n\nmodels,weaimtoimprovetheoverallretrievalefficiencyandresponsegeneration within the RAG framework. Early Chunking. Documents are segmented into text chunks, and each chunk is processed by the embedding model. The model generates token-level embed- dings for each chunk, which are subsequently aggregated using mean pooling to produce a single embedding per chunk. Late Chunking. Late chunking defers the chunking process. As shown in Figure 3.1, instead of segmenting the document initially, the entire document\n\nofcontextual retrieval versus tra- ditional early chunkingstrategies, utilizingdifferent text segmenters and embedding models to evaluate their impact on retrieval accuracy and downstream performance in RAG systems. 3.1 RQ#1: Early or Late Chunking? In this workflow, the main architectural modification compared to the standard RAG lies in the document embedding process Figure 3.1. Specifically, we experi- ment with various embedding models to encode document chunks, tailoring them 3 chunking/\n\nofcontextual retrieval versus tra- ditional early chunkingstrategies, utilizingdifferent text segmenters and embedding models to evaluate their impact on retrieval accuracy and downstream performance in RAG systems. 3.1 RQ#1: Early or Late Chunking? In this workflow, the main architectural modification compared to the standard RAG lies in the document embedding process Figure 3.1. Specifically, we experi- ment with various embedding models to encode document chunks, tailoring them 3 chunking/"
        },
        "generated_answer": "Based on your notes, here are two advanced techniques, besides traditional early chunking, that are aimed at preserving global context within RAG systems:\n\n**Answer:**\n\n1.  **Late Chunking:** This technique defers the chunking process. Instead of segmenting the document at the very beginning, the entire document is initially considered. While the notes indicate it offers higher efficiency, it may sacrifice some relevance. It is, however, explored as a method to preserve global context.\n2.  **Contextual Retrieval:** This approach is specifically highlighted for its ability to preserve semantic coherence more effectively. It is evaluated alongside late chunking as a strategy for optimizing RAG systems and maintaining global context, though it requires greater computational resources compared to other methods.\n\n**Key Points/Summary:**\n\n*   **Late Chunking:** Delays the segmentation of documents, initially processing the entire document to help preserve overall context.\n*   **Contextual Retrieval:** Focuses on maintaining the semantic coherence of information, which is crucial for preserving global context.\n*   Both techniques are studied for their potential to optimize RAG systems by addressing the challenge of global context preservation.\n\n**Extra Tip:**\nThink of \"global context\" as understanding the entire story or main idea of a document, rather than just isolated sentences or paragraphs. These advanced techniques try to ensure that when information is retrieved, it still makes sense in the broader scope of the original document.",
        "correctness_score": 0.95,
        "faithfulness_score": 1.0,
        "model_tested": "base"
    },
    {
        "question_id": 4,
        "document_path": "static/Accelerating LLM Inference.pdf",
        "question": "In the context of long-context LLMs, what is the key phenomenon called where chunks attended to by tokens within a generated chunk exhibit substantial consistency, which ChunkLLM exploits to enhance inference efficiency?",
        "ground_truth": "This phenomenon is called the Intra-Chunk Attention Consistency (ICAC) pattern. ChunkLLM exploits ICAC by updating chunk selection only when the currently decoded token is identified as a chunk boundary.",
        "retrieved_context": {
            "context": "exploit the Intra-Chunk Attention Consistency (ICAC) pattern such that chunk selection is only updated when the current token is identified as a chunk boundary, which substantially enhances inference efficiency. Further- more, ChunkLLM can achieve inference performance comparable to that of models optimized for 120K context lengths, despite being trained solely on 4K context lengths, thereby substantially re- ducing the training overhead associated with 120K context scaling. Experimental results validate\n\nexploit the Intra-Chunk Attention Consistency (ICAC) pattern such that chunk selection is only updated when the current token is identified as a chunk boundary, which substantially enhances inference efficiency. Further- more, ChunkLLM can achieve inference performance comparable to that of models optimized for 120K context lengths, despite being trained solely on 4K context lengths, thereby substantially re- ducing the training overhead associated with 120K context scaling. Experimental results validate\n\nintroduce a novel Intra-chunk Attention Consistency Pattern, which yields notable improvements in inference efficiency for long- context scenarios. Experimental results show that ChunkLLM attains a maximum speedup of 4.48 in comparison to the vanilla Transformer in the processing of 120K long texts. 9\n\nintroduce a novel Intra-chunk Attention Consistency Pattern, which yields notable improvements in inference efficiency for long- context scenarios. Experimental results show that ChunkLLM attains a maximum speedup of 4.48 in comparison to the vanilla Transformer in the processing of 120K long texts. 9\n\nexclusively when the current token is detected as a chunk boundary, thereby accelerating model inference. Experimental evaluations are conducted on a diverse set of long-text and short-text benchmark datasets spanning multiple tasks. ChunkLLM not only attains comparable perfor- mance on short-text benchmarks but also maintains 98.64% of the performance on long-context benchmarks while preserving a 48.58% key-value cache retention rate. Particularly, ChunkLLM attains a maximum speedup of 4.48 in comparison\n\nexclusively when the current token is detected as a chunk boundary, thereby accelerating model inference. Experimental evaluations are conducted on a diverse set of long-text and short-text benchmark datasets spanning multiple tasks. ChunkLLM not only attains comparable perfor- mance on short-text benchmarks but also maintains 98.64% of the performance on long-context benchmarks while preserving a 48.58% key-value cache retention rate. Particularly, ChunkLLM attains a maximum speedup of 4.48 in comparison\n\nv.s. 40.10 v.s. 41.51). In contrast to the short-text benchmark in Subsection 3.2.4, ChunkLLM demonstrates a remarkable improvement in long-context evaluation, which validates the advantage of ChunkLLM in retrieving key chunk information during long-context reasoning. (2) Notably, in the MDQA task, ChunkLLM yields a substantial improvement over SepLLM. We argue that the core challenge of MDQA lies in the dispersion of critical information across distinct positions within the context, which places high\n\nv.s. 40.10 v.s. 41.51). In contrast to the short-text benchmark in Subsection 3.2.4, ChunkLLM demonstrates a remarkable improvement in long-context evaluation, which validates the advantage of ChunkLLM in retrieving key chunk information during long-context reasoning. (2) Notably, in the MDQA task, ChunkLLM yields a substantial improvement over SepLLM. We argue that the core challenge of MDQA lies in the dispersion of critical information across distinct positions within the context, which places high\n\nperplexity (ppl) on the PG19 test set, and results are summarized in Table 2. Compared to the vanilla model, ChunkLLM yields a slight enhancement in ppl alongside a significant decrease in total infer- ence time. The underlying reason is that while the vanilla model maintains semantic integrity, it incurs linearly increasing inference time as gen- erated token count rises. Conversely, ChunkLLM reduces computational burden and speeds up inference by leveraging its chunk selection and ICAC mechanisms. 3.2.4\n\nperplexity (ppl) on the PG19 test set, and results are summarized in Table 2. Compared to the vanilla model, ChunkLLM yields a slight enhancement in ppl alongside a significant decrease in total infer- ence time. The underlying reason is that while the vanilla model maintains semantic integrity, it incurs linearly increasing inference time as gen- erated token count rises. Conversely, ChunkLLM reduces computational burden and speeds up inference by leveraging its chunk selection and ICAC mechanisms. 3.2.4"
        },
        "generated_answer": "Hello there! I can help you with that.\n\nIn the context of long-context Large Language Models (LLMs), the key phenomenon where chunks attended to by tokens within a generated chunk exhibit substantial consistency is called the **Intra-Chunk Attention Consistency (ICAC) pattern**.\n\nChunkLLM exploits this specific pattern to significantly enhance its inference efficiency. It does so by only updating its chunk selection when the current token is identified as a chunk boundary. This strategic approach helps ChunkLLM reduce computational burden and speed up inference, especially in long-context scenarios.\n\n**Key Points/Summary:**\n*   The phenomenon is named **Intra-Chunk Attention Consistency (ICAC) pattern**.\n*   It describes the substantial consistency in attention patterns within chunks.\n*   ChunkLLM leverages ICAC to improve inference efficiency.\n*   Chunk selection is updated only at chunk boundaries, which optimizes performance.\n\n**Extra Tip:**\nThink of it like reading a book. Instead of constantly shifting your attention to every single word across the entire book, you might focus on understanding one paragraph (a \"chunk\") at a time. The \"Intra-Chunk Attention Consistency\" means that while you're focused on that paragraph, your attention largely stays *within* that paragraph, and you only decide to move to a new paragraph (update your \"chunk selection\") once you've reached the end of the current one. This makes reading much faster and more efficient!",
        "correctness_score": 1.0,
        "faithfulness_score": 1.0,
        "model_tested": "base"
    },
    {
        "question_id": 5,
        "document_path": "static/Beyond Long Context.pdf",
        "question": "In the clinical domain, what is the methodology that uses entity-aware retrieval strategies to achieve improved semantic accuracy and computational efficiency when processing Electronic Health Record (EHR) notes?",
        "ground_truth": "The methodology is Clinical Entity Augmented Retrieval (CLEAR). CLEAR addresses the limitations of traditional chunk-based RAG by employing entity-aware, entity-centered retrieval strategies and demonstrated a 78% reduction in token usage compared to wide-context processing in evaluations",
        "retrieved_context": {
            "context": "strategies often outperform naive long-context prompt- ing, supporting the need for entity-aware retrieval. Our work contributes a reproducible evaluation framework within this paradigm, focusing on realistic EHR-scale clinical notes.\n\nstrategies often outperform naive long-context prompt- ing, supporting the need for entity-aware retrieval. Our work contributes a reproducible evaluation framework within this paradigm, focusing on realistic EHR-scale clinical notes.\n\nfor semantic clinical retrieval in EHR settings where computational efficiency and semantic accuracy are critical, and provides a reusable framework for evaluating clinical NLP approaches in production environments. 1. INTRODUCTION Electronic Health Record (EHR) systems in modern healthcare infrastructure store clinical documenta- tion within FHIR DocumentReference resources, typically encoded as base64 attachments containing unstructured clinical notes. These documents, ranging from brief progress notes\n\nfor semantic clinical retrieval in EHR settings where computational efficiency and semantic accuracy are critical, and provides a reusable framework for evaluating clinical NLP approaches in production environments. 1. INTRODUCTION Electronic Health Record (EHR) systems in modern healthcare infrastructure store clinical documenta- tion within FHIR DocumentReference resources, typically encoded as base64 attachments containing unstructured clinical notes. These documents, ranging from brief progress notes\n\ncomplexity and length. 1.1 Related Work Entity-aware retrieval has gained increasing attention within biomedical NLP and question-answering domains. Early retrieval-augmented methods such as RAG demonstrated the potential of embedding- based chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches leveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al., 2016) provided partial improvements but often failed to maintain contextual\n\ncomplexity and length. 1.1 Related Work Entity-aware retrieval has gained increasing attention within biomedical NLP and question-answering domains. Early retrieval-augmented methods such as RAG demonstrated the potential of embedding- based chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches leveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al., 2016) provided partial improvements but often failed to maintain contextual\n\nstatistical correlation through vector database chunking often fail to capture the nuanced clinical relationships required for accurate medical information extraction. The Clinical Entity Augmented Retrieval (CLEAR) methodology, introduced by Lopez et al. (2025) , addresses these limitations through entity- aware retrieval strategies and reports improved performance (F1 0.90 vs. 0.86 for embedding RAG; >70% fewer tokens and faster inference). Objective:To develop a comprehensive evaluation platform for\n\nstatistical correlation through vector database chunking often fail to capture the nuanced clinical relationships required for accurate medical information extraction. The Clinical Entity Augmented Retrieval (CLEAR) methodology, introduced by Lopez et al. (2025) , addresses these limitations through entity- aware retrieval strategies and reports improved performance (F1 0.90 vs. 0.86 for embedding RAG; >70% fewer tokens and faster inference). Objective:To develop a comprehensive evaluation platform for\n\nfundamental approaches: (1) wide context processing for zero-shot inference with large language models, (2) traditional vector database chunking with embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu- mentReference processing. Our contributions include: systematic validation of CLEARs performance claims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis of retrieval strategy performance across clinical documents of varying\n\nfundamental approaches: (1) wide context processing for zero-shot inference with large language models, (2) traditional vector database chunking with embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu- mentReference processing. Our contributions include: systematic validation of CLEARs performance claims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis of retrieval strategy performance across clinical documents of varying"
        },
        "generated_answer": "Hello there! I can help you with that.\n\n**Answer:**\nIn the clinical domain, the methodology that uses entity-aware retrieval strategies to achieve improved semantic accuracy and computational efficiency when processing Electronic Health Record (EHR) notes is called **Clinical Entity Augmented Retrieval (CLEAR)**.\n\nThis methodology, introduced by Lopez et al. (2025), specifically addresses the limitations of other retrieval methods, such as traditional vector database chunking, which often struggle to capture the nuanced clinical relationships required for accurate medical information extraction. CLEAR is adapted for EHR DocumentReference processing and is designed for semantic clinical retrieval in EHR settings where both computational efficiency and semantic accuracy are critical.\n\nAccording to the notes, CLEAR has demonstrated improved performance, reporting an F1 score of 0.90 compared to 0.86 for embedding RAG, and achieving more than 70% fewer tokens and faster inference.\n\n**Key Points/Summary:**\n*   **Name:** Clinical Entity Augmented Retrieval (CLEAR) methodology.\n*   **Purpose:** Addresses limitations in medical information extraction by using entity-aware retrieval strategies.\n*   **Application:** Designed for semantic clinical retrieval within EHR settings.\n*   **Benefits:** Achieves improved semantic accuracy and computational efficiency (higher F1 score, fewer tokens, faster inference).\n\n**Extra Tip:**\nThink of \"entity-aware\" as a smart librarian who not only knows where books are but also understands *what* the books are about and *how* specific concepts (entities like diseases, medications, or symptoms) relate to each other. This is much more effective than a librarian who just knows where chunks of text are located without understanding their deeper clinical meaning!",
        "correctness_score": 0.95,
        "faithfulness_score": 1.0,
        "model_tested": "base"
    },
    {
        "question_id": 6,
        "document_path": "static/Accelerating_LLM_Inference.pdf",
        "question": "Contrast ChunkLLM and Dynamic Hierarchical Sparse Attention (DHSA) regarding their dynamic attention management mechanisms, specifically addressing how they derive chunk representations and utilize them to achieve efficiency gains while preserving performance in long-context models.",
        "ground_truth": "ChunkLLM and Dynamic Hierarchical Sparse Attention (DHSA) both propose mechanisms for efficient long-context modeling by dynamically managing attention sparsity, but they employ different architectural additions and chunk representation strategies. ChunkLLM introduces two pluggable components: the QK Adapter (Q-Adapter and K-Adapter) and the Chunk Adapter. The Chunk Adapter is a one-layer feed-forward neural network (FNN) classifier that detects if a token is a chunk boundary using contextual semantic information. The QK Adapter fulfills feature compression and generates chunk attention scores, trained using an attention distillation approach where the Kullback-Leibler (KL) divergence between chunk attention scores and full attention scores guides optimization to enhance the recall rate of key chunks. ChunkLLM leverages the Intra-Chunk Attention Consistency (ICAC) pattern, triggering chunk selection updates exclusively when the current token is identified as a chunk boundary, substantially enhancing inference efficiency. ChunkLLM maintains 98.64% of the vanilla model's performance on long-context benchmarks and achieves a maximum speedup of 4.48x when processing 120K long texts. DHSA, conversely, is a plug-in module that dynamically predicts attention sparsity during prefill and decode stages without retraining the base model. DHSA employs a **Dynamic Hierarchical Sparsity Prediction approach. It first uses a boundary prediction function to adaptively segment input sequences into variable-length chunks. Chunk representations ($q_c$ and $k_c$) are derived by aggregating token queries and keys using a **length-normalized aggregation strategy, which involves scaling the sum of embeddings by the square root of the chunk size ($\\sqrt{|C|}$) to mitigate sensitivity to variable chunk lengths. It estimates chunk-level similarity ($S_c$) and then upsamples it to obtain the token-level similarity matrix ($S_t$), applying TOPK selection to generate the sparsity mask. DHSA reports matching dense attention in accuracy, while reducing prefill latency by 20-60% and peak memory usage by 35%.",
        "retrieved_context": {
            "context": "costs of long-context models, making it difficult for large language models to effi- ciently scale to context-processing tasks with million-level token sizes.Chunk Selective attention, a special type of sparse attention, can be primarily categorized into two paradigms: fixed chunk (Lu et al., 2025; Yuan et al., 2025; Wang et al., 2025) and separators-based dynamic chunk (Chen et al., 2024). Both approaches partition the input into discrete chunks: the former conducts parti- tioning with a fixed length,\n\ncosts of long-context models, making it difficult for large language models to effi- ciently scale to context-processing tasks with million-level token sizes.Chunk Selective attention, a special type of sparse attention, can be primarily categorized into two paradigms: fixed chunk (Lu et al., 2025; Yuan et al., 2025; Wang et al., 2025) and separators-based dynamic chunk (Chen et al., 2024). Both approaches partition the input into discrete chunks: the former conducts parti- tioning with a fixed length,\n\ndemands on the models context comprehension capability. SepLLM leverages separators as chunk features, which is plagued by constrained representational capacity and the problem of chunk semantic incompleteness. By contrast, ChunkLLM enriches the representational capacity of chunks via attention distillation, which enhances the recall rates of critical chunks. This, in turn, effectively boosts the models long-context understanding capability. (3) ChunkLLM attains 98.64% of the vanilla models performance\n\ndemands on the models context comprehension capability. SepLLM leverages separators as chunk features, which is plagued by constrained representational capacity and the problem of chunk semantic incompleteness. By contrast, ChunkLLM enriches the representational capacity of chunks via attention distillation, which enhances the recall rates of critical chunks. This, in turn, effectively boosts the models long-context understanding capability. (3) ChunkLLM attains 98.64% of the vanilla models performance\n\nChunkLLMs attention distillation mechanism, which strengthens the feature representational capacity of chunks. Consequently, during chunk selection, the model effectively identifies critical chunks with higher query relevance, leading to improved in- ference performance. Additionally, ChunkLLM exhibits a reduced KV-Cache utilization rate relative to SepLLM, which further corroborates the effectiveness of key chunk retrieval. We also conduct experiments with StreamingLLM, as shown in Appendix 6.3. 3.2.3\n\nChunkLLMs attention distillation mechanism, which strengthens the feature representational capacity of chunks. Consequently, during chunk selection, the model effectively identifies critical chunks with higher query relevance, leading to improved in- ference performance. Additionally, ChunkLLM exhibits a reduced KV-Cache utilization rate relative to SepLLM, which further corroborates the effectiveness of key chunk retrieval. We also conduct experiments with StreamingLLM, as shown in Appendix 6.3. 3.2.3\n\nintroduce a novel Intra-chunk Attention Consistency Pattern, which yields notable improvements in inference efficiency for long- context scenarios. Experimental results show that ChunkLLM attains a maximum speedup of 4.48 in comparison to the vanilla Transformer in the processing of 120K long texts. 9\n\nintroduce a novel Intra-chunk Attention Consistency Pattern, which yields notable improvements in inference efficiency for long- context scenarios. Experimental results show that ChunkLLM attains a maximum speedup of 4.48 in comparison to the vanilla Transformer in the processing of 120K long texts. 9\n\nexploit the Intra-Chunk Attention Consistency (ICAC) pattern such that chunk selection is only updated when the current token is identified as a chunk boundary, which substantially enhances inference efficiency. Further- more, ChunkLLM can achieve inference performance comparable to that of models optimized for 120K context lengths, despite being trained solely on 4K context lengths, thereby substantially re- ducing the training overhead associated with 120K context scaling. Experimental results validate\n\nexploit the Intra-Chunk Attention Consistency (ICAC) pattern such that chunk selection is only updated when the current token is identified as a chunk boundary, which substantially enhances inference efficiency. Further- more, ChunkLLM can achieve inference performance comparable to that of models optimized for 120K context lengths, despite being trained solely on 4K context lengths, thereby substantially re- ducing the training overhead associated with 120K context scaling. Experimental results validate"
        },
        "generated_answer": "It's a great question to understand the nuances of different attention mechanisms! Based on the provided notes, I can explain ChunkLLM's dynamic attention management mechanisms, specifically how it derives and utilizes chunk representations for efficiency and performance.\n\nHowever, the provided notes do not contain any information about **Dynamic Hierarchical Sparse Attention (DHSA)**. Therefore, I can only describe ChunkLLM's approach and cannot offer a direct contrast with DHSA from this material.\n\n---\n\n### ChunkLLM's Dynamic Attention Management\n\nChunkLLM is designed to enhance the long-context understanding capability of large language models while improving inference efficiency. It tackles the challenge of scaling to million-level token sizes, which is often difficult due to the high costs associated with long-context models.\n\n1.  **Deriving Chunk Representations:**\n    ChunkLLM significantly enriches the representational capacity of chunks through an **attention distillation mechanism**. This mechanism strengthens the feature representation of individual chunks, making them more informative.\n\n2.  **Utilizing Chunk Representations for Efficiency and Performance:**\n    *   **Enhanced Chunk Selection:** By having richer chunk representations, ChunkLLM can more effectively identify **critical chunks** that have higher relevance to the query during the chunk selection process. This improved selection leads to better inference performance.\n    *   **Reduced KV-Cache Utilization:** Compared to models like SepLLM, ChunkLLM demonstrates a reduced KV-Cache utilization rate. This efficiency gain is a direct result of its effective key chunk retrieval, meaning it doesn't need to store as much information for irrelevant parts of the context.\n    *   **Intra-Chunk Attention Consistency (ICAC) Pattern:** ChunkLLM introduces a novel **Intra-chunk Attention Consistency Pattern**. This pattern ensures that chunk selection is only updated when the current token is identified as a chunk boundary. This selective updating substantially enhances inference efficiency, particularly in long-context scenarios.\n    *   **Speedup and Performance Preservation:** The model achieves a maximum speedup of 4.48 times compared to a vanilla Transformer when processing 120K long texts. Remarkably, ChunkLLM can achieve inference performance comparable to models optimized for 120K context lengths, even though it was trained solely on 4K context lengths. This significantly reduces the training overhead that would typically be associated with scaling to longer contexts. Overall, ChunkLLM attains 98.64% of the vanilla model's performance while offering these efficiency benefits.\n\n---\n\n**Key Points/Summary:**\n*   **Chunk Representation:** ChunkLLM uses an **attention distillation mechanism** to enrich and strengthen the representational capacity of individual chunks.\n*   **Efficiency Gains:** It improves efficiency through better **critical chunk identification**, reduced **KV-Cache utilization**, and an **Intra-Chunk Attention Consistency (ICAC) pattern** that updates chunk selection only at boundaries.\n*   **Performance Preservation:** ChunkLLM achieves performance comparable to models trained on much longer contexts (120K) despite being trained on shorter ones (4K), attaining 98.64% of vanilla model performance and significant speedups.\n\n**Extra Tip:**\nThink of \"attention distillation\" like concentrating the most important information from a larger context into a smaller, more potent summary for each chunk. This makes it easier and faster for the model to pick out the truly relevant parts when answering a query.",
        "correctness_score": 0.1,
        "faithfulness_score": 0.0,
        "model_tested": "base"
    },
    {
        "question_id": 7,
        "document_path": "static/Beyond Long Context.pdf",
        "question": "Explain the observed trade-offs between computational efficiency and semantic integrity across various chunking and retrieval strategies\u2014including Fixed-size/Semantic Chunking, Late Chunking, Contextual Retrieval, and Clinical Entity Augmented Retrieval (CLEAR)\u2014and identify which approach demonstrated superior scalability advantages in high-complexity clinical documents.",
        "ground_truth": "The sources reveal significant trade-offs among various chunking and retrieval strategies concerning computational cost and the preservation of semantic integrity. 1. Fixed-size vs. Semantic Chunking (RAG Baseline): Traditional fixed-size chunking is computationally simple and efficient. However, its simplicity risks fragmenting semantically related content, leading to suboptimal retrieval. Semantic chunking, which aims for semantically coherent segments, involves additional computational costs that the sources found were often not justified by consistent performance gains on standard document structures. Overall, fixed-size chunking was suggested as a more efficient and reliable choice for practical RAG applications on non-synthetic datasets. 2. Late Chunking vs. Contextual Retrieval: Late Chunking defers segmentation until after the entire document is embedded, preserving full contextual information for efficiency. Late Chunking offers higher efficiency but may sacrifice relevance and completeness. In contrast, Contextual Retrieval enhances chunks by prompting an LLM to generate additional context for each chunk, improving contextual integrity. This context preservation, particularly when combined with Rank Fusion (ContextualRankFusion), yields better overall results in retrieval evaluation than Late Chunking but incurs greater computational resources, potentially requiring up to 20GB of VRAM for chunk contextualization in long documents. 3. Clinical Entity Augmented Retrieval (CLEAR): This entity-aware method achieves a balance by selectively centering clinically relevant spans around identified entities, overcoming the positional bias ('lost in the middle' problem) associated with processing entire long documents. CLEAR achieved a 78.4% token savings compared to Wide Context processing while maintaining the highest average semantic similarity (0.878), demonstrating an optimal balance between accuracy and computational cost. The approach that demonstrated superior scalability advantages in high-complexity documents was CLEAR. In evaluations involving large clinical notes (exceeding 65,000 tokens), CLEAR achieved a 75% win rate, confirming that its entity-aware retrieval advantages grow as document complexity and document size increase, making it highly suitable for large EHR document processing",
        "retrieved_context": {
            "context": "4 Retrieval-Augmented Generation (RAG):Semantic chunking with embedding-based retrieval us- ing top-k chunk selection. This approach prioritizes efficiency with minimal token usage (average 544 tokens per query) but may miss critical clinical relationships. 2.4 Evaluation Framework Evaluation was conducted on a dataset of 12 clinical notes ranging from 10,000 to 65,000 tokens, repre- senting diverse clinical scenarios. Each note was accompanied by clinical questions requiring informa- tion extraction and\n\ncomplexity and length. 1.1 Related Work Entity-aware retrieval has gained increasing attention within biomedical NLP and question-answering domains. Early retrieval-augmented methods such as RAG demonstrated the potential of embedding- based chunk retrieval but lacked domain-specific entity modeling. In the clinical domain, approaches leveraging UMLS concepts and ontology-based retrieval (e.g., Neumann et al., 2019; Johnson et al., 2016) provided partial improvements but often failed to maintain contextual\n\nlong inputs (lost in the middle) . This motivates entity-aware retrieval that selectively centers clinically relevant spans rather than relying on statistically similar but potentially off-target chunks. The CLinical Entity Augmented Retrieval (CLEAR) methodology, published by Lopez et al. in 2025 , introduced a novel approach that addresses these limitations through entity-aware, entity- centered retrieval strategies. The original study demonstrated significant performance improvements (F1 score of 0.90\n\nfundamental approaches: (1) wide context processing for zero-shot inference with large language models, (2) traditional vector database chunking with embedding-based retrieval, and (3) entity-aware CLEAR methodology adapted for EHR Docu- mentReference processing. Our contributions include: systematic validation of CLEARs performance claims, development of a reusable evaluation platform for clinical NLP research, and empirical analysis of retrieval strategy performance across clinical documents of varying\n\nstatistical correlation through vector database chunking often fail to capture the nuanced clinical relationships required for accurate medical information extraction. The Clinical Entity Augmented Retrieval (CLEAR) methodology, introduced by Lopez et al. (2025) , addresses these limitations through entity- aware retrieval strategies and reports improved performance (F1 0.90 vs. 0.86 for embedding RAG; >70% fewer tokens and faster inference). Objective:To develop a comprehensive evaluation platform for\n\ndocuments are split into consecutive, fixed-size segments, re- main unclear. This study systematically evalu- ates the effectiveness of semantic chunking us- ing three common retrieval-related tasks: docu- ment retrieval, evidence retrieval, and retrieval- based answer generation. The results show that the computational costs associated with seman- tic chunking are not justified by consistent per- formance gains. These findings challenge the previous assumptions about semantic chunk- ing and highlight the\n\ncontinuity across long clin- ical narratives. CLEAR represented a significant advancement by introducing entity-centered retrieval aligned with clinical semantics. Our work extends this line of research by operationalizing CLEAR within an end-to-end evaluation platform, providing reproducible empirical validation across realistic EHR-scale document sets. Recent evaluations of retrieval-augmented models in long-context reasoning (e.g., Karpinska et al., 2023; Xiong et al., 2024) emphasize that retrieval\n\nanswers. How- ever, the effectiveness of chunking strategies re- mains a significant challenge in optimizing retrieval quality and computational efficiency (Lewis et al., 2020; Finardi et al., 2024). Known as fixed-size chunking, the traditional way to chunk is to cut documents into chunks of a fixed length such as 200 tokens (Gao et al., 2023). While computationally simple, this approach can fragment semantically related content across multi- ple chunks, leading to suboptimal retrieval perfor- mance.\n\nIs Semantic Chunking Worth the Computational Cost? Renyi Qu Vectara, Inc. renyi@vectara.com Forrest Bao Vectara, Inc. forrest@vectara.com Ruixuan Tu University of WisconsinMadison turx2003@gmail.com Abstract Recent advances in Retrieval-Augmented Gen- eration (RAG) systems have popularized se- mantic chunking, which aims to improve re- trieval performance by dividing documents into semantically coherent segments. Despite its growing adoption, the actual benefits over sim- pler fixed-size chunking, where\n\nwide-context processing at markedly lower token budgets, echoing the efficiencyquality trade-offs highlighted by CLEAR . The validation results strongly confirm the original research findings, particularly demonstrating scalability advantages on large clinical documents characteristic of comprehensive EHR DocumentRe- ference content. The 75% win rate on documents exceeding 65,000 tokens validates that entity-aware retrieval becomes increasingly advantageous as document complexity increases, confirming the"
        },
        "generated_answer": "It's a great question about how different strategies balance getting the right information with computational costs! Let's break down the trade-offs described in your notes.\n\nThe notes discuss a balance between **computational efficiency** (how fast and resource-intensive a method is, often measured by token usage) and **semantic integrity** (how well a method preserves the meaning and relationships within the text, especially critical clinical relationships).\n\nHere's an explanation of the strategies and their trade-offs:\n\n1.  **Fixed-size Chunking:**\n    *   **Description:** This is a traditional and computationally simple method where documents are cut into chunks of a fixed length (e.g., 200 tokens).\n    *   **Computational Efficiency:** High. It's \"computationally simple\" to implement.\n    *   **Semantic Integrity:** Low. The major drawback is that it \"can fragment semantically related content across multiple chunks,\" leading to \"suboptimal retrieval performance\" because important contextual information might be split up.\n    *   **Trade-off:** Prioritizes simplicity and speed over maintaining the complete meaning of related information.\n\n2.  **Semantic Chunking (within Retrieval-Augmented Generation - RAG):**\n    *   **Description:** This approach aims to improve retrieval by dividing documents into \"semantically coherent segments\" using embedding-based retrieval with top-k chunk selection.\n    *   **Computational Efficiency:** Moderate. It \"prioritizes efficiency with minimal token usage (average 544 tokens per query).\" However, the notes also state that \"the computational costs associated with semantic chunking are not justified by consistent performance gains,\" suggesting that while it aims for efficiency, the benefits might not always outweigh the cost compared to simpler methods, or it might not be as efficient as ideal for the gains it provides.\n    *   **Semantic Integrity:** Moderate. While it tries to create coherent segments, it \"may miss critical clinical relationships\" and \"lacked domain-specific entity modeling,\" often failing \"to capture the nuanced clinical relationships required for accurate medical information extraction.\"\n    *   **Trade-off:** Attempts to balance efficiency with semantic coherence but struggles with the complex, domain-specific nuances found in clinical texts.\n\n3.  **Clinical Entity Augmented Retrieval (CLEAR):**\n    *   **Description:** Introduced by Lopez et al. (2025), CLEAR is a novel methodology that addresses the limitations of earlier RAG approaches through \"entity-aware, entity-centered retrieval strategies.\" It \"selectively centers clinically relevant spans.\"\n    *   **Computational Efficiency:** High. It boasts \"markedly lower token budgets,\" \"70% fewer tokens and faster inference\" compared to embedding RAG.\n    *   **Semantic Integrity:** High. CLEAR is \"aligned with clinical semantics\" and directly addresses the problem of missing critical relationships by focusing on entities. It demonstrated \"significant performance improvements\" (an F1 score of 0.90 compared to 0.86 for embedding RAG).\n    *   **Trade-off:** Successfully achieves both high computational efficiency and strong semantic integrity by being \"entity-aware.\"\n\n**Regarding Late Chunking and Contextual Retrieval:**\nYour notes do not provide specific definitions or detailed trade-offs for \"Late Chunking\" or \"Contextual Retrieval\" as distinct strategies. While \"wide context processing for zero-shot inference with large language models\" is mentioned as one fundamental approach, its specific trade-offs in terms of efficiency and semantic integrity are not elaborated upon in the provided text.\n\n**Superior Scalability Advantage:**\nThe **Clinical Entity Augmented Retrieval (CLEAR)** methodology demonstrated superior scalability advantages in high-complexity clinical documents. The notes state that CLEAR shows \"scalability advantages on large clinical documents characteristic of comprehensive EHR DocumentReference content,\" with a \"75% win rate on documents exceeding 65,000 tokens,\" validating that \"entity-aware retrieval becomes increasingly advantageous as document complexity increases.\"\n\n---\n\n**Key Points/Summary:**\n\n*   **Fixed-size Chunking:** High efficiency, but low semantic integrity due to fragmentation.\n*   **Semantic Chunking (RAG):** Aims for efficiency and coherence but often misses critical clinical relationships and lacks domain-specific entity modeling.\n*   **CLEAR:** Achieves both high efficiency (fewer tokens, faster inference) and high semantic integrity by focusing on \"entity-aware, entity-centered\" retrieval.\n*   **Scalability:** CLEAR demonstrated superior scalability, especially with large and complex clinical documents (over 65,000 tokens).\n\n**Extra Tip:**\nThink of it like this: Fixed-size chunking is like cutting a book into pages without regard for sentences or paragraphs \u2013 easy but you lose context. Semantic chunking tries to cut by paragraph but might still miss key connections between paragraphs. CLEAR is like having a smart editor who understands the subject matter and highlights only the most important, interconnected sentences and phrases related to a specific topic, making it both efficient to read and highly accurate!",
        "correctness_score": 0.85,
        "faithfulness_score": 1.0,
        "model_tested": "base"
    }
]